{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) / 255.0, label)).shuffle(1024).batch(32)\n",
    "model = tf.keras.applications.MobileNetV2(weights=None, classes=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for images, labels in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        labels_pred = model(images)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"loss %f\" % loss.numpy())\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = np.array([[\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 2, 1, 0],\n",
    "    [0, 0, 2, 2, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 2, 1, 0],\n",
    "    [0, 0, 2, 1, 1, 0, 0],\n",
    "    [0, 2, 1, 1, 2, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0]\n",
    "]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.expand_dims(image, axis=-1)\n",
    "W = np.array([[\n",
    "    [ 0, 0, -1], \n",
    "    [ 0, 1, 0 ], \n",
    "    [-2, 0, 2 ]\n",
    "]], dtype=np.float32)\n",
    "b = np.array([1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=1,\n",
    "        kernel_size=[3,3],\n",
    "        kernel_initializer=tf.constant_initializer(W),\n",
    "        bias_initializer=tf.constant_initializer(b)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this operation returns a tensor of the same type with all dimensions of size 1 removed\n",
    "output = model(image)\n",
    "print(tf.squeeze(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        #print(self.char_indices)\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "        #print(self.text)\n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "            #print(seq)\n",
    "            #print(next_char)\n",
    "        return np.array(seq), np.array(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        #print(\"state\", state)\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "        logits = self.dense(output)\n",
    "        if from_logits:\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 3.953150\n",
      "batch 1: loss 3.936770\n",
      "batch 2: loss 3.924057\n",
      "batch 3: loss 3.889787\n",
      "batch 4: loss 3.885556\n",
      "batch 5: loss 3.831541\n",
      "batch 6: loss 3.705526\n",
      "batch 7: loss 3.404818\n",
      "batch 8: loss 3.639490\n",
      "batch 9: loss 2.982469\n",
      "batch 10: loss 3.023395\n",
      "batch 11: loss 2.954731\n",
      "batch 12: loss 3.150671\n",
      "batch 13: loss 3.294437\n",
      "batch 14: loss 3.187977\n",
      "batch 15: loss 3.001268\n",
      "batch 16: loss 3.155298\n",
      "batch 17: loss 3.202326\n",
      "batch 18: loss 3.066057\n",
      "batch 19: loss 2.820950\n",
      "batch 20: loss 3.187222\n",
      "batch 21: loss 3.047943\n",
      "batch 22: loss 3.073235\n",
      "batch 23: loss 3.024383\n",
      "batch 24: loss 3.172982\n",
      "batch 25: loss 3.311368\n",
      "batch 26: loss 3.131812\n",
      "batch 27: loss 3.131819\n",
      "batch 28: loss 3.104218\n",
      "batch 29: loss 2.991270\n",
      "batch 30: loss 3.062727\n",
      "batch 31: loss 2.886787\n",
      "batch 32: loss 2.819685\n",
      "batch 33: loss 3.159575\n",
      "batch 34: loss 2.968798\n",
      "batch 35: loss 2.966655\n",
      "batch 36: loss 3.238174\n",
      "batch 37: loss 3.368665\n",
      "batch 38: loss 2.889974\n",
      "batch 39: loss 3.345523\n",
      "batch 40: loss 3.032537\n",
      "batch 41: loss 3.147779\n",
      "batch 42: loss 3.157084\n",
      "batch 43: loss 2.895236\n",
      "batch 44: loss 2.900584\n",
      "batch 45: loss 3.026832\n",
      "batch 46: loss 3.033587\n",
      "batch 47: loss 3.152212\n",
      "batch 48: loss 2.995073\n",
      "batch 49: loss 3.230997\n",
      "batch 50: loss 3.215626\n",
      "batch 51: loss 3.131702\n",
      "batch 52: loss 3.089408\n",
      "batch 53: loss 2.837049\n",
      "batch 54: loss 2.990840\n",
      "batch 55: loss 3.121514\n",
      "batch 56: loss 3.034213\n",
      "batch 57: loss 2.980591\n",
      "batch 58: loss 3.343285\n",
      "batch 59: loss 2.920830\n",
      "batch 60: loss 3.264612\n",
      "batch 61: loss 3.214328\n",
      "batch 62: loss 3.194437\n",
      "batch 63: loss 2.937346\n",
      "batch 64: loss 2.907159\n",
      "batch 65: loss 3.064053\n",
      "batch 66: loss 3.189419\n",
      "batch 67: loss 3.096684\n",
      "batch 68: loss 2.924441\n",
      "batch 69: loss 3.060661\n",
      "batch 70: loss 2.947951\n",
      "batch 71: loss 3.014718\n",
      "batch 72: loss 2.948186\n",
      "batch 73: loss 3.000663\n",
      "batch 74: loss 3.231882\n",
      "batch 75: loss 3.034258\n",
      "batch 76: loss 3.092070\n",
      "batch 77: loss 2.951451\n",
      "batch 78: loss 3.177822\n",
      "batch 79: loss 3.122213\n",
      "batch 80: loss 3.031050\n",
      "batch 81: loss 2.890352\n",
      "batch 82: loss 3.045019\n",
      "batch 83: loss 3.146950\n",
      "batch 84: loss 3.071665\n",
      "batch 85: loss 2.926746\n",
      "batch 86: loss 3.396875\n",
      "batch 87: loss 2.828848\n",
      "batch 88: loss 3.026343\n",
      "batch 89: loss 2.940792\n",
      "batch 90: loss 3.012469\n",
      "batch 91: loss 2.883704\n",
      "batch 92: loss 2.958092\n",
      "batch 93: loss 2.975912\n",
      "batch 94: loss 2.852917\n",
      "batch 95: loss 2.994509\n",
      "batch 96: loss 3.026026\n",
      "batch 97: loss 2.987799\n",
      "batch 98: loss 3.008958\n",
      "batch 99: loss 2.972250\n",
      "batch 100: loss 3.118131\n",
      "batch 101: loss 2.888850\n",
      "batch 102: loss 3.186429\n",
      "batch 103: loss 3.194859\n",
      "batch 104: loss 3.022045\n",
      "batch 105: loss 3.003679\n",
      "batch 106: loss 3.007341\n",
      "batch 107: loss 3.039515\n",
      "batch 108: loss 2.710765\n",
      "batch 109: loss 3.180103\n",
      "batch 110: loss 3.157552\n",
      "batch 111: loss 2.981651\n",
      "batch 112: loss 3.359714\n",
      "batch 113: loss 2.779303\n",
      "batch 114: loss 3.169461\n",
      "batch 115: loss 3.129189\n",
      "batch 116: loss 3.061883\n",
      "batch 117: loss 3.121282\n",
      "batch 118: loss 2.811627\n",
      "batch 119: loss 2.873832\n",
      "batch 120: loss 3.227224\n",
      "batch 121: loss 2.913229\n",
      "batch 122: loss 2.958189\n",
      "batch 123: loss 3.241219\n",
      "batch 124: loss 2.987372\n",
      "batch 125: loss 2.957014\n",
      "batch 126: loss 3.192613\n",
      "batch 127: loss 2.940152\n",
      "batch 128: loss 2.900638\n",
      "batch 129: loss 3.022750\n",
      "batch 130: loss 2.900804\n",
      "batch 131: loss 2.774569\n",
      "batch 132: loss 3.176624\n",
      "batch 133: loss 3.099036\n",
      "batch 134: loss 3.274293\n",
      "batch 135: loss 2.934641\n",
      "batch 136: loss 3.053842\n",
      "batch 137: loss 2.976619\n",
      "batch 138: loss 2.903277\n",
      "batch 139: loss 2.892795\n",
      "batch 140: loss 3.231521\n",
      "batch 141: loss 2.987391\n",
      "batch 142: loss 3.017798\n",
      "batch 143: loss 2.744666\n",
      "batch 144: loss 3.003899\n",
      "batch 145: loss 2.774713\n",
      "batch 146: loss 3.008877\n",
      "batch 147: loss 2.917568\n",
      "batch 148: loss 2.981246\n",
      "batch 149: loss 2.985142\n",
      "batch 150: loss 2.926114\n",
      "batch 151: loss 2.644502\n",
      "batch 152: loss 2.879995\n",
      "batch 153: loss 3.101315\n",
      "batch 154: loss 3.152746\n",
      "batch 155: loss 2.849883\n",
      "batch 156: loss 3.017900\n",
      "batch 157: loss 2.679348\n",
      "batch 158: loss 2.989082\n",
      "batch 159: loss 2.763103\n",
      "batch 160: loss 2.856305\n",
      "batch 161: loss 2.775703\n",
      "batch 162: loss 2.806345\n",
      "batch 163: loss 2.814583\n",
      "batch 164: loss 2.716648\n",
      "batch 165: loss 3.087001\n",
      "batch 166: loss 2.973997\n",
      "batch 167: loss 2.748801\n",
      "batch 168: loss 3.041432\n",
      "batch 169: loss 2.800007\n",
      "batch 170: loss 3.041764\n",
      "batch 171: loss 2.816415\n",
      "batch 172: loss 3.191598\n",
      "batch 173: loss 2.953417\n",
      "batch 174: loss 2.993459\n",
      "batch 175: loss 2.876868\n",
      "batch 176: loss 3.019717\n",
      "batch 177: loss 2.851241\n",
      "batch 178: loss 3.018170\n",
      "batch 179: loss 3.042263\n",
      "batch 180: loss 2.999832\n",
      "batch 181: loss 2.889159\n",
      "batch 182: loss 2.783611\n",
      "batch 183: loss 2.694210\n",
      "batch 184: loss 3.003760\n",
      "batch 185: loss 3.052833\n",
      "batch 186: loss 3.045319\n",
      "batch 187: loss 2.986292\n",
      "batch 188: loss 2.905896\n",
      "batch 189: loss 3.072214\n",
      "batch 190: loss 2.877644\n",
      "batch 191: loss 3.090666\n",
      "batch 192: loss 2.965931\n",
      "batch 193: loss 2.819479\n",
      "batch 194: loss 2.871615\n",
      "batch 195: loss 2.993264\n",
      "batch 196: loss 2.965695\n",
      "batch 197: loss 3.177597\n",
      "batch 198: loss 2.782542\n",
      "batch 199: loss 2.976684\n",
      "batch 200: loss 2.717345\n",
      "batch 201: loss 2.790707\n",
      "batch 202: loss 3.031376\n",
      "batch 203: loss 2.796512\n",
      "batch 204: loss 2.916251\n",
      "batch 205: loss 2.894545\n",
      "batch 206: loss 3.138300\n",
      "batch 207: loss 2.650276\n",
      "batch 208: loss 2.739743\n",
      "batch 209: loss 3.238768\n",
      "batch 210: loss 2.797983\n",
      "batch 211: loss 3.003510\n",
      "batch 212: loss 2.987098\n",
      "batch 213: loss 3.010955\n",
      "batch 214: loss 3.017917\n",
      "batch 215: loss 3.057611\n",
      "batch 216: loss 3.097941\n",
      "batch 217: loss 2.926784\n",
      "batch 218: loss 2.788393\n",
      "batch 219: loss 2.960783\n",
      "batch 220: loss 3.045762\n",
      "batch 221: loss 3.108476\n",
      "batch 222: loss 2.601806\n",
      "batch 223: loss 3.177819\n",
      "batch 224: loss 2.829286\n",
      "batch 225: loss 2.942371\n",
      "batch 226: loss 3.189157\n",
      "batch 227: loss 3.055246\n",
      "batch 228: loss 2.940315\n",
      "batch 229: loss 2.765388\n",
      "batch 230: loss 2.999690\n",
      "batch 231: loss 3.138931\n",
      "batch 232: loss 3.153424\n",
      "batch 233: loss 2.918657\n",
      "batch 234: loss 3.105752\n",
      "batch 235: loss 3.048086\n",
      "batch 236: loss 2.991655\n",
      "batch 237: loss 2.864006\n",
      "batch 238: loss 2.701250\n",
      "batch 239: loss 3.006467\n",
      "batch 240: loss 2.812780\n",
      "batch 241: loss 2.798945\n",
      "batch 242: loss 3.032658\n",
      "batch 243: loss 2.886766\n",
      "batch 244: loss 2.989440\n",
      "batch 245: loss 2.786647\n",
      "batch 246: loss 3.237356\n",
      "batch 247: loss 3.004606\n",
      "batch 248: loss 3.088903\n",
      "batch 249: loss 2.791201\n",
      "batch 250: loss 2.868653\n",
      "batch 251: loss 2.879282\n",
      "batch 252: loss 2.592952\n",
      "batch 253: loss 2.896279\n",
      "batch 254: loss 2.804227\n",
      "batch 255: loss 2.777663\n",
      "batch 256: loss 2.807151\n",
      "batch 257: loss 2.876687\n",
      "batch 258: loss 2.830785\n",
      "batch 259: loss 3.042124\n",
      "batch 260: loss 2.943327\n",
      "batch 261: loss 2.980580\n",
      "batch 262: loss 2.891168\n",
      "batch 263: loss 2.941804\n",
      "batch 264: loss 2.936097\n",
      "batch 265: loss 3.190279\n",
      "batch 266: loss 2.966924\n",
      "batch 267: loss 2.939301\n",
      "batch 268: loss 2.714864\n",
      "batch 269: loss 2.907833\n",
      "batch 270: loss 2.792693\n",
      "batch 271: loss 2.877481\n",
      "batch 272: loss 2.812486\n",
      "batch 273: loss 2.973461\n",
      "batch 274: loss 3.085428\n",
      "batch 275: loss 2.887037\n",
      "batch 276: loss 2.917688\n",
      "batch 277: loss 2.735343\n",
      "batch 278: loss 3.128479\n",
      "batch 279: loss 2.872524\n",
      "batch 280: loss 2.769811\n",
      "batch 281: loss 2.849878\n",
      "batch 282: loss 2.987654\n",
      "batch 283: loss 2.676869\n",
      "batch 284: loss 2.861838\n",
      "batch 285: loss 2.700143\n",
      "batch 286: loss 2.741835\n",
      "batch 287: loss 2.823956\n",
      "batch 288: loss 2.634103\n",
      "batch 289: loss 2.925523\n",
      "batch 290: loss 2.709064\n",
      "batch 291: loss 2.973353\n",
      "batch 292: loss 2.682328\n",
      "batch 293: loss 2.846395\n",
      "batch 294: loss 3.042181\n",
      "batch 295: loss 2.919553\n",
      "batch 296: loss 2.549017\n",
      "batch 297: loss 2.844920\n",
      "batch 298: loss 2.824587\n",
      "batch 299: loss 2.898732\n",
      "batch 300: loss 2.806934\n",
      "batch 301: loss 2.857715\n",
      "batch 302: loss 2.622199\n",
      "batch 303: loss 3.051306\n",
      "batch 304: loss 2.835744\n",
      "batch 305: loss 2.835196\n",
      "batch 306: loss 2.935942\n",
      "batch 307: loss 3.095878\n",
      "batch 308: loss 2.642442\n",
      "batch 309: loss 2.958280\n",
      "batch 310: loss 2.883114\n",
      "batch 311: loss 2.688844\n",
      "batch 312: loss 2.792821\n",
      "batch 313: loss 2.787597\n",
      "batch 314: loss 2.627351\n",
      "batch 315: loss 2.827426\n",
      "batch 316: loss 2.911127\n",
      "batch 317: loss 2.798641\n",
      "batch 318: loss 2.877145\n",
      "batch 319: loss 2.805876\n",
      "batch 320: loss 2.673192\n",
      "batch 321: loss 2.751003\n",
      "batch 322: loss 2.695408\n",
      "batch 323: loss 2.671365\n",
      "batch 324: loss 2.654531\n",
      "batch 325: loss 2.786832\n",
      "batch 326: loss 2.856840\n",
      "batch 327: loss 2.640500\n",
      "batch 328: loss 3.029878\n",
      "batch 329: loss 2.684440\n",
      "batch 330: loss 2.894634\n",
      "batch 331: loss 2.741797\n",
      "batch 332: loss 2.681294\n",
      "batch 333: loss 2.622337\n",
      "batch 334: loss 2.730793\n",
      "batch 335: loss 2.528646\n",
      "batch 336: loss 2.898172\n",
      "batch 337: loss 2.510067\n",
      "batch 338: loss 2.622629\n",
      "batch 339: loss 2.533384\n",
      "batch 340: loss 2.793118\n",
      "batch 341: loss 2.929499\n",
      "batch 342: loss 2.826171\n",
      "batch 343: loss 2.447560\n",
      "batch 344: loss 2.824384\n",
      "batch 345: loss 2.553884\n",
      "batch 346: loss 2.699872\n",
      "batch 347: loss 2.720944\n",
      "batch 348: loss 2.614000\n",
      "batch 349: loss 2.748519\n",
      "batch 350: loss 2.911434\n",
      "batch 351: loss 2.750158\n",
      "batch 352: loss 2.480554\n",
      "batch 353: loss 2.699187\n",
      "batch 354: loss 2.955413\n",
      "batch 355: loss 3.043464\n",
      "batch 356: loss 2.748460\n",
      "batch 357: loss 2.627692\n",
      "batch 358: loss 2.792106\n",
      "batch 359: loss 2.610482\n",
      "batch 360: loss 2.749376\n",
      "batch 361: loss 2.597687\n",
      "batch 362: loss 2.817377\n",
      "batch 363: loss 2.599487\n",
      "batch 364: loss 2.593308\n",
      "batch 365: loss 2.615514\n",
      "batch 366: loss 2.810897\n",
      "batch 367: loss 2.885683\n",
      "batch 368: loss 3.344677\n",
      "batch 369: loss 3.042933\n",
      "batch 370: loss 2.840316\n",
      "batch 371: loss 2.723490\n",
      "batch 372: loss 2.726310\n",
      "batch 373: loss 2.699923\n",
      "batch 374: loss 3.036120\n",
      "batch 375: loss 2.654092\n",
      "batch 376: loss 2.513553\n",
      "batch 377: loss 2.670508\n",
      "batch 378: loss 2.603045\n",
      "batch 379: loss 2.823011\n",
      "batch 380: loss 2.626538\n",
      "batch 381: loss 2.844371\n",
      "batch 382: loss 2.831616\n",
      "batch 383: loss 2.789518\n",
      "batch 384: loss 2.590320\n",
      "batch 385: loss 2.391434\n",
      "batch 386: loss 2.847626\n",
      "batch 387: loss 2.603724\n",
      "batch 388: loss 2.835238\n",
      "batch 389: loss 2.912202\n",
      "batch 390: loss 2.855197\n",
      "batch 391: loss 2.545511\n",
      "batch 392: loss 2.653153\n",
      "batch 393: loss 2.764856\n",
      "batch 394: loss 2.934927\n",
      "batch 395: loss 2.658932\n",
      "batch 396: loss 2.609554\n",
      "batch 397: loss 2.722919\n",
      "batch 398: loss 2.622054\n",
      "batch 399: loss 2.598946\n",
      "batch 400: loss 2.620532\n",
      "batch 401: loss 2.385894\n",
      "batch 402: loss 2.922605\n",
      "batch 403: loss 2.266268\n",
      "batch 404: loss 3.076716\n",
      "batch 405: loss 2.707124\n",
      "batch 406: loss 2.626387\n",
      "batch 407: loss 2.644882\n",
      "batch 408: loss 2.645693\n",
      "batch 409: loss 2.604598\n",
      "batch 410: loss 2.807982\n",
      "batch 411: loss 2.716586\n",
      "batch 412: loss 2.343323\n",
      "batch 413: loss 2.892278\n",
      "batch 414: loss 2.879137\n",
      "batch 415: loss 2.570581\n",
      "batch 416: loss 2.675092\n",
      "batch 417: loss 2.602402\n",
      "batch 418: loss 2.503894\n",
      "batch 419: loss 2.620194\n",
      "batch 420: loss 2.371092\n",
      "batch 421: loss 2.751474\n",
      "batch 422: loss 2.363655\n",
      "batch 423: loss 2.613065\n",
      "batch 424: loss 2.523188\n",
      "batch 425: loss 2.645368\n",
      "batch 426: loss 2.502998\n",
      "batch 427: loss 3.126854\n",
      "batch 428: loss 2.659803\n",
      "batch 429: loss 2.759091\n",
      "batch 430: loss 2.708159\n",
      "batch 431: loss 2.510641\n",
      "batch 432: loss 2.420871\n",
      "batch 433: loss 2.820096\n",
      "batch 434: loss 2.813326\n",
      "batch 435: loss 2.893632\n",
      "batch 436: loss 2.569003\n",
      "batch 437: loss 2.771309\n",
      "batch 438: loss 2.795681\n",
      "batch 439: loss 2.960304\n",
      "batch 440: loss 2.738203\n",
      "batch 441: loss 2.718017\n",
      "batch 442: loss 2.551430\n",
      "batch 443: loss 2.522234\n",
      "batch 444: loss 2.700164\n",
      "batch 445: loss 2.902160\n",
      "batch 446: loss 2.413506\n",
      "batch 447: loss 2.590958\n",
      "batch 448: loss 2.723822\n",
      "batch 449: loss 2.341232\n",
      "batch 450: loss 2.544941\n",
      "batch 451: loss 2.648618\n",
      "batch 452: loss 2.411532\n",
      "batch 453: loss 2.999855\n",
      "batch 454: loss 2.665030\n",
      "batch 455: loss 2.646886\n",
      "batch 456: loss 2.634117\n",
      "batch 457: loss 2.997547\n",
      "batch 458: loss 2.690302\n",
      "batch 459: loss 2.962790\n",
      "batch 460: loss 2.677325\n",
      "batch 461: loss 2.948315\n",
      "batch 462: loss 2.560951\n",
      "batch 463: loss 2.446081\n",
      "batch 464: loss 2.887125\n",
      "batch 465: loss 2.811991\n",
      "batch 466: loss 2.691578\n",
      "batch 467: loss 2.782488\n",
      "batch 468: loss 2.532860\n",
      "batch 469: loss 2.495639\n",
      "batch 470: loss 2.724701\n",
      "batch 471: loss 2.592645\n",
      "batch 472: loss 2.766119\n",
      "batch 473: loss 2.372027\n",
      "batch 474: loss 2.728459\n",
      "batch 475: loss 2.529094\n",
      "batch 476: loss 2.574911\n",
      "batch 477: loss 2.905952\n",
      "batch 478: loss 2.488419\n",
      "batch 479: loss 2.708581\n",
      "batch 480: loss 2.732630\n",
      "batch 481: loss 2.494989\n",
      "batch 482: loss 2.875396\n",
      "batch 483: loss 2.560229\n",
      "batch 484: loss 2.517477\n",
      "batch 485: loss 2.262338\n",
      "batch 486: loss 2.713655\n",
      "batch 487: loss 2.509345\n",
      "batch 488: loss 2.913898\n",
      "batch 489: loss 2.643171\n",
      "batch 490: loss 2.710724\n",
      "batch 491: loss 2.425599\n",
      "batch 492: loss 2.868101\n",
      "batch 493: loss 2.640395\n",
      "batch 494: loss 2.630370\n",
      "batch 495: loss 2.783398\n",
      "batch 496: loss 2.372563\n",
      "batch 497: loss 2.742199\n",
      "batch 498: loss 2.723020\n",
      "batch 499: loss 2.466784\n",
      "batch 500: loss 2.517914\n",
      "batch 501: loss 2.475954\n",
      "batch 502: loss 2.575611\n",
      "batch 503: loss 2.527149\n",
      "batch 504: loss 2.612968\n",
      "batch 505: loss 2.768272\n",
      "batch 506: loss 2.568673\n",
      "batch 507: loss 2.742882\n",
      "batch 508: loss 2.646276\n",
      "batch 509: loss 2.693691\n",
      "batch 510: loss 2.701158\n",
      "batch 511: loss 2.822131\n",
      "batch 512: loss 2.643938\n",
      "batch 513: loss 2.490390\n",
      "batch 514: loss 2.754120\n",
      "batch 515: loss 2.816136\n",
      "batch 516: loss 2.443332\n",
      "batch 517: loss 2.721743\n",
      "batch 518: loss 2.571226\n",
      "batch 519: loss 2.775008\n",
      "batch 520: loss 2.175176\n",
      "batch 521: loss 2.415040\n",
      "batch 522: loss 2.648105\n",
      "batch 523: loss 2.660882\n",
      "batch 524: loss 2.434419\n",
      "batch 525: loss 2.309458\n",
      "batch 526: loss 2.497453\n",
      "batch 527: loss 2.326883\n",
      "batch 528: loss 2.384750\n",
      "batch 529: loss 2.646732\n",
      "batch 530: loss 2.596382\n",
      "batch 531: loss 2.604950\n",
      "batch 532: loss 2.431612\n",
      "batch 533: loss 2.686367\n",
      "batch 534: loss 2.690994\n",
      "batch 535: loss 2.137299\n",
      "batch 536: loss 2.770458\n",
      "batch 537: loss 2.357434\n",
      "batch 538: loss 2.619824\n",
      "batch 539: loss 2.513348\n",
      "batch 540: loss 2.514407\n",
      "batch 541: loss 2.458507\n",
      "batch 542: loss 2.584765\n",
      "batch 543: loss 2.569399\n",
      "batch 544: loss 2.674791\n",
      "batch 545: loss 2.619064\n",
      "batch 546: loss 2.374886\n",
      "batch 547: loss 2.471372\n",
      "batch 548: loss 2.626538\n",
      "batch 549: loss 2.666172\n",
      "batch 550: loss 2.658442\n",
      "batch 551: loss 2.539738\n",
      "batch 552: loss 2.419007\n",
      "batch 553: loss 2.842984\n",
      "batch 554: loss 2.565765\n",
      "batch 555: loss 2.786508\n",
      "batch 556: loss 2.615681\n",
      "batch 557: loss 2.838656\n",
      "batch 558: loss 2.598973\n",
      "batch 559: loss 2.648621\n",
      "batch 560: loss 2.523723\n",
      "batch 561: loss 2.373801\n",
      "batch 562: loss 2.632666\n",
      "batch 563: loss 2.507570\n",
      "batch 564: loss 2.620127\n",
      "batch 565: loss 2.683827\n",
      "batch 566: loss 2.619790\n",
      "batch 567: loss 2.284349\n",
      "batch 568: loss 2.548355\n",
      "batch 569: loss 2.847183\n",
      "batch 570: loss 2.377049\n",
      "batch 571: loss 2.695130\n",
      "batch 572: loss 2.203851\n",
      "batch 573: loss 2.489259\n",
      "batch 574: loss 2.519073\n",
      "batch 575: loss 2.307852\n",
      "batch 576: loss 2.636825\n",
      "batch 577: loss 2.465862\n",
      "batch 578: loss 2.458600\n",
      "batch 579: loss 2.648310\n",
      "batch 580: loss 2.720083\n",
      "batch 581: loss 2.664444\n",
      "batch 582: loss 2.670699\n",
      "batch 583: loss 2.479282\n",
      "batch 584: loss 2.369248\n",
      "batch 585: loss 2.567100\n",
      "batch 586: loss 2.204403\n",
      "batch 587: loss 2.555388\n",
      "batch 588: loss 2.858626\n",
      "batch 589: loss 2.439262\n",
      "batch 590: loss 2.593593\n",
      "batch 591: loss 2.553821\n",
      "batch 592: loss 2.345057\n",
      "batch 593: loss 2.373963\n",
      "batch 594: loss 2.398976\n",
      "batch 595: loss 2.658143\n",
      "batch 596: loss 2.715270\n",
      "batch 597: loss 2.563352\n",
      "batch 598: loss 2.641112\n",
      "batch 599: loss 2.705088\n",
      "batch 600: loss 2.413452\n",
      "batch 601: loss 2.425325\n",
      "batch 602: loss 2.778185\n",
      "batch 603: loss 2.474084\n",
      "batch 604: loss 2.669972\n",
      "batch 605: loss 2.756974\n",
      "batch 606: loss 2.497436\n",
      "batch 607: loss 2.764593\n",
      "batch 608: loss 2.234127\n",
      "batch 609: loss 2.376689\n",
      "batch 610: loss 2.581635\n",
      "batch 611: loss 2.708375\n",
      "batch 612: loss 2.376231\n",
      "batch 613: loss 2.147855\n",
      "batch 614: loss 2.572191\n",
      "batch 615: loss 2.661908\n",
      "batch 616: loss 2.781435\n",
      "batch 617: loss 2.339777\n",
      "batch 618: loss 2.417389\n",
      "batch 619: loss 2.539884\n",
      "batch 620: loss 2.368600\n",
      "batch 621: loss 2.338722\n",
      "batch 622: loss 2.659447\n",
      "batch 623: loss 2.399711\n",
      "batch 624: loss 2.675286\n",
      "batch 625: loss 2.784731\n",
      "batch 626: loss 2.632364\n",
      "batch 627: loss 2.620270\n",
      "batch 628: loss 2.792387\n",
      "batch 629: loss 2.346337\n",
      "batch 630: loss 2.203298\n",
      "batch 631: loss 2.482428\n",
      "batch 632: loss 2.221523\n",
      "batch 633: loss 2.331823\n",
      "batch 634: loss 2.408833\n",
      "batch 635: loss 2.180205\n",
      "batch 636: loss 2.505959\n",
      "batch 637: loss 2.514872\n",
      "batch 638: loss 2.842104\n",
      "batch 639: loss 2.291476\n",
      "batch 640: loss 2.212063\n",
      "batch 641: loss 2.380007\n",
      "batch 642: loss 2.721066\n",
      "batch 643: loss 2.506230\n",
      "batch 644: loss 2.570391\n",
      "batch 645: loss 2.474445\n",
      "batch 646: loss 2.000305\n",
      "batch 647: loss 2.551296\n",
      "batch 648: loss 2.389657\n",
      "batch 649: loss 2.248405\n",
      "batch 650: loss 2.418776\n",
      "batch 651: loss 2.294614\n",
      "batch 652: loss 2.769316\n",
      "batch 653: loss 2.611274\n",
      "batch 654: loss 2.578668\n",
      "batch 655: loss 2.630036\n",
      "batch 656: loss 2.544527\n",
      "batch 657: loss 2.163702\n",
      "batch 658: loss 2.489328\n",
      "batch 659: loss 2.535427\n",
      "batch 660: loss 2.338680\n",
      "batch 661: loss 2.471887\n",
      "batch 662: loss 2.584163\n",
      "batch 663: loss 2.277399\n",
      "batch 664: loss 2.299906\n",
      "batch 665: loss 2.807951\n",
      "batch 666: loss 2.742403\n",
      "batch 667: loss 2.535813\n",
      "batch 668: loss 2.766754\n",
      "batch 669: loss 2.722062\n",
      "batch 670: loss 2.534586\n",
      "batch 671: loss 2.419673\n",
      "batch 672: loss 2.272574\n",
      "batch 673: loss 2.308662\n",
      "batch 674: loss 2.357178\n",
      "batch 675: loss 2.926422\n",
      "batch 676: loss 2.769575\n",
      "batch 677: loss 2.749590\n",
      "batch 678: loss 2.643659\n",
      "batch 679: loss 2.224290\n",
      "batch 680: loss 2.638013\n",
      "batch 681: loss 2.374041\n",
      "batch 682: loss 2.274312\n",
      "batch 683: loss 2.471635\n",
      "batch 684: loss 2.500009\n",
      "batch 685: loss 2.369124\n",
      "batch 686: loss 2.306641\n",
      "batch 687: loss 2.258245\n",
      "batch 688: loss 2.281245\n",
      "batch 689: loss 2.480188\n",
      "batch 690: loss 2.513188\n",
      "batch 691: loss 2.617432\n",
      "batch 692: loss 2.802817\n",
      "batch 693: loss 2.377628\n",
      "batch 694: loss 2.668219\n",
      "batch 695: loss 2.440964\n",
      "batch 696: loss 2.358402\n",
      "batch 697: loss 2.625736\n",
      "batch 698: loss 2.533449\n",
      "batch 699: loss 2.693430\n",
      "batch 700: loss 2.843336\n",
      "batch 701: loss 2.523757\n",
      "batch 702: loss 2.666364\n",
      "batch 703: loss 2.494000\n",
      "batch 704: loss 2.461331\n",
      "batch 705: loss 2.790316\n",
      "batch 706: loss 2.535103\n",
      "batch 707: loss 2.621731\n",
      "batch 708: loss 2.603702\n",
      "batch 709: loss 2.618034\n",
      "batch 710: loss 2.468801\n",
      "batch 711: loss 2.486846\n",
      "batch 712: loss 2.264759\n",
      "batch 713: loss 1.968557\n",
      "batch 714: loss 2.386857\n",
      "batch 715: loss 2.160831\n",
      "batch 716: loss 2.232082\n",
      "batch 717: loss 2.472485\n",
      "batch 718: loss 2.451989\n",
      "batch 719: loss 2.359149\n",
      "batch 720: loss 2.458865\n",
      "batch 721: loss 2.432078\n",
      "batch 722: loss 2.382959\n",
      "batch 723: loss 2.906910\n",
      "batch 724: loss 2.465509\n",
      "batch 725: loss 2.348744\n",
      "batch 726: loss 2.520513\n",
      "batch 727: loss 2.313166\n",
      "batch 728: loss 2.102803\n",
      "batch 729: loss 2.625520\n",
      "batch 730: loss 2.257100\n",
      "batch 731: loss 2.369678\n",
      "batch 732: loss 2.532781\n",
      "batch 733: loss 2.295390\n",
      "batch 734: loss 2.293558\n",
      "batch 735: loss 2.573155\n",
      "batch 736: loss 2.453897\n",
      "batch 737: loss 2.744141\n",
      "batch 738: loss 2.571330\n",
      "batch 739: loss 2.264402\n",
      "batch 740: loss 2.554342\n",
      "batch 741: loss 2.501547\n",
      "batch 742: loss 2.470402\n",
      "batch 743: loss 2.335741\n",
      "batch 744: loss 2.164432\n",
      "batch 745: loss 2.420587\n",
      "batch 746: loss 2.762970\n",
      "batch 747: loss 2.659175\n",
      "batch 748: loss 2.546737\n",
      "batch 749: loss 2.509871\n",
      "batch 750: loss 2.545154\n",
      "batch 751: loss 2.238329\n",
      "batch 752: loss 2.293828\n",
      "batch 753: loss 2.329993\n",
      "batch 754: loss 2.648716\n",
      "batch 755: loss 2.475944\n",
      "batch 756: loss 2.190108\n",
      "batch 757: loss 2.417052\n",
      "batch 758: loss 2.295858\n",
      "batch 759: loss 2.389097\n",
      "batch 760: loss 2.793628\n",
      "batch 761: loss 2.247193\n",
      "batch 762: loss 2.316872\n",
      "batch 763: loss 2.359337\n",
      "batch 764: loss 2.441424\n",
      "batch 765: loss 3.237412\n",
      "batch 766: loss 2.601362\n",
      "batch 767: loss 2.455507\n",
      "batch 768: loss 2.622868\n",
      "batch 769: loss 2.323689\n",
      "batch 770: loss 2.364719\n",
      "batch 771: loss 2.307287\n",
      "batch 772: loss 2.305589\n",
      "batch 773: loss 2.595064\n",
      "batch 774: loss 2.067855\n",
      "batch 775: loss 2.388296\n",
      "batch 776: loss 2.335155\n",
      "batch 777: loss 2.368012\n",
      "batch 778: loss 2.293929\n",
      "batch 779: loss 2.303752\n",
      "batch 780: loss 2.422471\n",
      "batch 781: loss 2.407323\n",
      "batch 782: loss 2.267064\n",
      "batch 783: loss 2.592789\n",
      "batch 784: loss 2.283255\n",
      "batch 785: loss 2.575130\n",
      "batch 786: loss 2.322017\n",
      "batch 787: loss 2.697038\n",
      "batch 788: loss 2.444866\n",
      "batch 789: loss 2.494496\n",
      "batch 790: loss 2.643709\n",
      "batch 791: loss 2.628208\n",
      "batch 792: loss 2.374819\n",
      "batch 793: loss 2.457934\n",
      "batch 794: loss 2.183134\n",
      "batch 795: loss 2.703851\n",
      "batch 796: loss 2.349356\n",
      "batch 797: loss 2.722379\n",
      "batch 798: loss 2.390795\n",
      "batch 799: loss 2.148044\n",
      "batch 800: loss 2.487071\n",
      "batch 801: loss 2.243031\n",
      "batch 802: loss 2.589228\n",
      "batch 803: loss 2.388044\n",
      "batch 804: loss 2.557308\n",
      "batch 805: loss 2.329877\n",
      "batch 806: loss 2.296678\n",
      "batch 807: loss 2.345737\n",
      "batch 808: loss 2.487630\n",
      "batch 809: loss 2.560359\n",
      "batch 810: loss 2.313772\n",
      "batch 811: loss 2.664769\n",
      "batch 812: loss 2.197011\n",
      "batch 813: loss 2.615686\n",
      "batch 814: loss 2.472124\n",
      "batch 815: loss 2.300843\n",
      "batch 816: loss 2.555587\n",
      "batch 817: loss 2.411732\n",
      "batch 818: loss 2.326924\n",
      "batch 819: loss 2.358834\n",
      "batch 820: loss 2.418530\n",
      "batch 821: loss 2.372155\n",
      "batch 822: loss 2.434441\n",
      "batch 823: loss 2.512531\n",
      "batch 824: loss 2.195375\n",
      "batch 825: loss 2.189609\n",
      "batch 826: loss 2.641649\n",
      "batch 827: loss 2.455351\n",
      "batch 828: loss 2.371143\n",
      "batch 829: loss 2.465264\n",
      "batch 830: loss 2.573370\n",
      "batch 831: loss 2.290231\n",
      "batch 832: loss 2.537543\n",
      "batch 833: loss 2.219634\n",
      "batch 834: loss 2.614323\n",
      "batch 835: loss 2.389597\n",
      "batch 836: loss 2.626036\n",
      "batch 837: loss 2.167185\n",
      "batch 838: loss 2.290613\n",
      "batch 839: loss 2.119186\n",
      "batch 840: loss 2.298950\n",
      "batch 841: loss 2.493558\n",
      "batch 842: loss 2.304377\n",
      "batch 843: loss 2.577565\n",
      "batch 844: loss 2.590970\n",
      "batch 845: loss 2.388750\n",
      "batch 846: loss 2.576110\n",
      "batch 847: loss 2.366349\n",
      "batch 848: loss 2.378471\n",
      "batch 849: loss 2.261986\n",
      "batch 850: loss 2.191601\n",
      "batch 851: loss 2.294593\n",
      "batch 852: loss 2.169318\n",
      "batch 853: loss 2.322852\n",
      "batch 854: loss 2.871914\n",
      "batch 855: loss 2.306140\n",
      "batch 856: loss 2.537901\n",
      "batch 857: loss 2.218837\n",
      "batch 858: loss 2.435215\n",
      "batch 859: loss 2.032248\n",
      "batch 860: loss 2.288214\n",
      "batch 861: loss 2.474060\n",
      "batch 862: loss 2.549583\n",
      "batch 863: loss 2.439839\n",
      "batch 864: loss 2.417693\n",
      "batch 865: loss 2.336127\n",
      "batch 866: loss 2.351768\n",
      "batch 867: loss 2.598298\n",
      "batch 868: loss 2.608516\n",
      "batch 869: loss 2.340640\n",
      "batch 870: loss 2.456536\n",
      "batch 871: loss 2.267143\n",
      "batch 872: loss 2.443302\n",
      "batch 873: loss 2.389872\n",
      "batch 874: loss 2.391832\n",
      "batch 875: loss 2.411342\n",
      "batch 876: loss 2.526394\n",
      "batch 877: loss 2.531098\n",
      "batch 878: loss 2.332484\n",
      "batch 879: loss 2.651325\n",
      "batch 880: loss 2.428905\n",
      "batch 881: loss 2.464414\n",
      "batch 882: loss 2.741322\n",
      "batch 883: loss 2.526181\n",
      "batch 884: loss 2.274301\n",
      "batch 885: loss 2.437877\n",
      "batch 886: loss 2.450057\n",
      "batch 887: loss 2.178253\n",
      "batch 888: loss 2.327547\n",
      "batch 889: loss 2.430488\n",
      "batch 890: loss 2.129383\n",
      "batch 891: loss 2.453003\n",
      "batch 892: loss 2.465710\n",
      "batch 893: loss 3.015669\n",
      "batch 894: loss 2.333957\n",
      "batch 895: loss 2.152900\n",
      "batch 896: loss 2.446568\n",
      "batch 897: loss 2.398258\n",
      "batch 898: loss 2.260463\n",
      "batch 899: loss 2.203115\n",
      "batch 900: loss 2.272198\n",
      "batch 901: loss 2.459372\n",
      "batch 902: loss 2.242096\n",
      "batch 903: loss 2.342574\n",
      "batch 904: loss 2.785148\n",
      "batch 905: loss 2.289537\n",
      "batch 906: loss 2.704766\n",
      "batch 907: loss 2.536320\n",
      "batch 908: loss 2.451272\n",
      "batch 909: loss 2.645158\n",
      "batch 910: loss 2.670168\n",
      "batch 911: loss 2.480132\n",
      "batch 912: loss 2.146406\n",
      "batch 913: loss 2.344285\n",
      "batch 914: loss 2.247268\n",
      "batch 915: loss 2.679202\n",
      "batch 916: loss 2.445155\n",
      "batch 917: loss 2.516263\n",
      "batch 918: loss 2.284623\n",
      "batch 919: loss 2.214763\n",
      "batch 920: loss 2.099086\n",
      "batch 921: loss 2.265637\n",
      "batch 922: loss 2.346017\n",
      "batch 923: loss 2.522297\n",
      "batch 924: loss 2.476319\n",
      "batch 925: loss 2.952866\n",
      "batch 926: loss 2.185241\n",
      "batch 927: loss 2.734456\n",
      "batch 928: loss 2.331612\n",
      "batch 929: loss 2.286591\n",
      "batch 930: loss 2.254774\n",
      "batch 931: loss 2.581600\n",
      "batch 932: loss 2.564009\n",
      "batch 933: loss 2.161876\n",
      "batch 934: loss 2.328531\n",
      "batch 935: loss 2.183707\n",
      "batch 936: loss 2.358379\n",
      "batch 937: loss 2.513778\n",
      "batch 938: loss 2.199407\n",
      "batch 939: loss 2.353901\n",
      "batch 940: loss 2.430857\n",
      "batch 941: loss 2.656729\n",
      "batch 942: loss 2.550787\n",
      "batch 943: loss 2.223984\n",
      "batch 944: loss 2.160434\n",
      "batch 945: loss 2.756887\n",
      "batch 946: loss 2.548794\n",
      "batch 947: loss 2.342513\n",
      "batch 948: loss 2.488408\n",
      "batch 949: loss 2.517952\n",
      "batch 950: loss 2.509014\n",
      "batch 951: loss 2.108111\n",
      "batch 952: loss 2.692203\n",
      "batch 953: loss 2.611445\n",
      "batch 954: loss 2.478441\n",
      "batch 955: loss 2.332509\n",
      "batch 956: loss 2.184204\n",
      "batch 957: loss 2.600978\n",
      "batch 958: loss 2.366556\n",
      "batch 959: loss 2.265312\n",
      "batch 960: loss 2.258255\n",
      "batch 961: loss 2.345212\n",
      "batch 962: loss 2.498850\n",
      "batch 963: loss 2.289316\n",
      "batch 964: loss 2.226395\n",
      "batch 965: loss 2.163217\n",
      "batch 966: loss 2.313037\n",
      "batch 967: loss 2.193388\n",
      "batch 968: loss 2.310715\n",
      "batch 969: loss 2.470154\n",
      "batch 970: loss 2.615551\n",
      "batch 971: loss 2.545126\n",
      "batch 972: loss 2.477101\n",
      "batch 973: loss 2.488910\n",
      "batch 974: loss 2.675697\n",
      "batch 975: loss 2.402975\n",
      "batch 976: loss 2.299732\n",
      "batch 977: loss 2.458445\n",
      "batch 978: loss 2.035491\n",
      "batch 979: loss 2.684208\n",
      "batch 980: loss 2.154316\n",
      "batch 981: loss 2.420798\n",
      "batch 982: loss 2.555540\n",
      "batch 983: loss 2.511358\n",
      "batch 984: loss 2.801541\n",
      "batch 985: loss 2.524422\n",
      "batch 986: loss 2.572505\n",
      "batch 987: loss 2.573101\n",
      "batch 988: loss 2.077209\n",
      "batch 989: loss 2.390723\n",
      "batch 990: loss 2.217374\n",
      "batch 991: loss 2.070539\n",
      "batch 992: loss 2.005051\n",
      "batch 993: loss 2.000777\n",
      "batch 994: loss 2.566203\n",
      "batch 995: loss 2.351523\n",
      "batch 996: loss 2.444911\n",
      "batch 997: loss 2.110287\n",
      "batch 998: loss 2.302054\n",
      "batch 999: loss 2.065493\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, inputs, temperature=1.):\n",
    "    print(\"tf.shape\", tf.shape(inputs))\n",
    "    batch_size, _ = tf.shape(inputs)\n",
    "    logits = self(inputs, from_logits=True)\n",
    "    prob = tf.nn.softmax(logits / temperature).numpy()\n",
    "    return np.array([np.random.choice(self.num_chars, p=prob[i, :]) for i in range(batch_size.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ [[45  1 34 44  1 41 30 43 33 26 41 44  1 26 44  1 44 46 27 45 37 30  7  0\n",
      "  27 30 37 26 45 30 29  7  1 29 34 47 30 43 44 30]]\n",
      "diversity 0.200000:\n",
      "tf.shape tf.Tensor([], shape=(0,), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aefdb054110f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diversity %f:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-40854e35508d>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputs, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "model.predict = predict\n",
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "print(\"X_\", X_)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
