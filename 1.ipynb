{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_float = tf.random.uniform(shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector = tf.zeros(shape=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([[1., 2.], [3., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = tf.constant([[5., 6.], [7., 8.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector = tf.zeros(shape=(2), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.add(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)\n",
    "print([y, y_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y, y_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([[1.], [2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = 0.5 * tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])\n",
    "print([L.numpy(), w_grad.numpy(), b_grad.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 1e-3\n",
    "for e in range(num_epoch):\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = (y_pred - y).dot(X), (y_pred - y).sum()\n",
    "    a, b = a -learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "for e in range(num_epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, input):\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.zeros_initializer(),\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "    \n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)\n",
    "        self.test_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)\n",
    "        self.train_label = self.train_label.astype(np.int32)\n",
    "        self.test_label = self.test_label.astype(np.int32)\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.259120\n",
      "batch 1: loss 2.276939\n",
      "batch 2: loss 2.285206\n",
      "batch 3: loss 2.223507\n",
      "batch 4: loss 2.086220\n",
      "batch 5: loss 1.997189\n",
      "batch 6: loss 1.883638\n",
      "batch 7: loss 1.787586\n",
      "batch 8: loss 1.735435\n",
      "batch 9: loss 1.780396\n",
      "batch 10: loss 1.715183\n",
      "batch 11: loss 1.628739\n",
      "batch 12: loss 1.653841\n",
      "batch 13: loss 1.345497\n",
      "batch 14: loss 1.388500\n",
      "batch 15: loss 1.501500\n",
      "batch 16: loss 1.404137\n",
      "batch 17: loss 1.441607\n",
      "batch 18: loss 1.368984\n",
      "batch 19: loss 1.192204\n",
      "batch 20: loss 0.987467\n",
      "batch 21: loss 1.151904\n",
      "batch 22: loss 1.252292\n",
      "batch 23: loss 1.104752\n",
      "batch 24: loss 0.945899\n",
      "batch 25: loss 1.102151\n",
      "batch 26: loss 0.944426\n",
      "batch 27: loss 0.826030\n",
      "batch 28: loss 1.199069\n",
      "batch 29: loss 0.952218\n",
      "batch 30: loss 0.925899\n",
      "batch 31: loss 0.788253\n",
      "batch 32: loss 0.644461\n",
      "batch 33: loss 0.916108\n",
      "batch 34: loss 0.746471\n",
      "batch 35: loss 0.703679\n",
      "batch 36: loss 0.840046\n",
      "batch 37: loss 0.968001\n",
      "batch 38: loss 0.818307\n",
      "batch 39: loss 0.609879\n",
      "batch 40: loss 0.917076\n",
      "batch 41: loss 0.612886\n",
      "batch 42: loss 0.778091\n",
      "batch 43: loss 0.613595\n",
      "batch 44: loss 0.635015\n",
      "batch 45: loss 0.897119\n",
      "batch 46: loss 0.592831\n",
      "batch 47: loss 0.466122\n",
      "batch 48: loss 0.625950\n",
      "batch 49: loss 0.599245\n",
      "batch 50: loss 0.543274\n",
      "batch 51: loss 0.636565\n",
      "batch 52: loss 0.668069\n",
      "batch 53: loss 0.476229\n",
      "batch 54: loss 0.497015\n",
      "batch 55: loss 0.634456\n",
      "batch 56: loss 0.673145\n",
      "batch 57: loss 0.840216\n",
      "batch 58: loss 0.449018\n",
      "batch 59: loss 0.552336\n",
      "batch 60: loss 0.443629\n",
      "batch 61: loss 0.589626\n",
      "batch 62: loss 0.532102\n",
      "batch 63: loss 0.350925\n",
      "batch 64: loss 0.365160\n",
      "batch 65: loss 0.579372\n",
      "batch 66: loss 0.717132\n",
      "batch 67: loss 0.397213\n",
      "batch 68: loss 0.573525\n",
      "batch 69: loss 0.417977\n",
      "batch 70: loss 0.369926\n",
      "batch 71: loss 0.453843\n",
      "batch 72: loss 0.417020\n",
      "batch 73: loss 0.472115\n",
      "batch 74: loss 0.496516\n",
      "batch 75: loss 0.494115\n",
      "batch 76: loss 0.465039\n",
      "batch 77: loss 0.369830\n",
      "batch 78: loss 0.411194\n",
      "batch 79: loss 0.377477\n",
      "batch 80: loss 0.546789\n",
      "batch 81: loss 0.525908\n",
      "batch 82: loss 0.386896\n",
      "batch 83: loss 0.403928\n",
      "batch 84: loss 0.470092\n",
      "batch 85: loss 0.672374\n",
      "batch 86: loss 0.372452\n",
      "batch 87: loss 0.577198\n",
      "batch 88: loss 0.519171\n",
      "batch 89: loss 0.441543\n",
      "batch 90: loss 0.465342\n",
      "batch 91: loss 0.545714\n",
      "batch 92: loss 0.440699\n",
      "batch 93: loss 0.359570\n",
      "batch 94: loss 0.377658\n",
      "batch 95: loss 0.317182\n",
      "batch 96: loss 0.304356\n",
      "batch 97: loss 0.404541\n",
      "batch 98: loss 0.438184\n",
      "batch 99: loss 0.484284\n",
      "batch 100: loss 0.443931\n",
      "batch 101: loss 0.400331\n",
      "batch 102: loss 0.564026\n",
      "batch 103: loss 0.560569\n",
      "batch 104: loss 0.378324\n",
      "batch 105: loss 0.421286\n",
      "batch 106: loss 0.315229\n",
      "batch 107: loss 0.557679\n",
      "batch 108: loss 0.439481\n",
      "batch 109: loss 0.531018\n",
      "batch 110: loss 0.413444\n",
      "batch 111: loss 0.368157\n",
      "batch 112: loss 0.436518\n",
      "batch 113: loss 0.545162\n",
      "batch 114: loss 0.636025\n",
      "batch 115: loss 0.639754\n",
      "batch 116: loss 0.455971\n",
      "batch 117: loss 0.508535\n",
      "batch 118: loss 0.437856\n",
      "batch 119: loss 0.381399\n",
      "batch 120: loss 0.428355\n",
      "batch 121: loss 0.393883\n",
      "batch 122: loss 0.301659\n",
      "batch 123: loss 0.473451\n",
      "batch 124: loss 0.385792\n",
      "batch 125: loss 0.425628\n",
      "batch 126: loss 0.461629\n",
      "batch 127: loss 0.301897\n",
      "batch 128: loss 0.444694\n",
      "batch 129: loss 0.412972\n",
      "batch 130: loss 0.267986\n",
      "batch 131: loss 0.422924\n",
      "batch 132: loss 0.403729\n",
      "batch 133: loss 0.400247\n",
      "batch 134: loss 0.344878\n",
      "batch 135: loss 0.513458\n",
      "batch 136: loss 0.275987\n",
      "batch 137: loss 0.196010\n",
      "batch 138: loss 0.351950\n",
      "batch 139: loss 0.541249\n",
      "batch 140: loss 0.506950\n",
      "batch 141: loss 0.421098\n",
      "batch 142: loss 0.280827\n",
      "batch 143: loss 0.261528\n",
      "batch 144: loss 0.302055\n",
      "batch 145: loss 0.337089\n",
      "batch 146: loss 0.604774\n",
      "batch 147: loss 0.236322\n",
      "batch 148: loss 0.340198\n",
      "batch 149: loss 0.233798\n",
      "batch 150: loss 0.216689\n",
      "batch 151: loss 0.381114\n",
      "batch 152: loss 0.350808\n",
      "batch 153: loss 0.339841\n",
      "batch 154: loss 0.362541\n",
      "batch 155: loss 0.420778\n",
      "batch 156: loss 0.343089\n",
      "batch 157: loss 0.481564\n",
      "batch 158: loss 0.246186\n",
      "batch 159: loss 0.298677\n",
      "batch 160: loss 0.691751\n",
      "batch 161: loss 0.303589\n",
      "batch 162: loss 0.467029\n",
      "batch 163: loss 0.315036\n",
      "batch 164: loss 0.523504\n",
      "batch 165: loss 0.377008\n",
      "batch 166: loss 0.531452\n",
      "batch 167: loss 0.308664\n",
      "batch 168: loss 0.430181\n",
      "batch 169: loss 0.355258\n",
      "batch 170: loss 0.384665\n",
      "batch 171: loss 0.143707\n",
      "batch 172: loss 0.258483\n",
      "batch 173: loss 0.526919\n",
      "batch 174: loss 0.301123\n",
      "batch 175: loss 0.287457\n",
      "batch 176: loss 0.252490\n",
      "batch 177: loss 0.206385\n",
      "batch 178: loss 0.376505\n",
      "batch 179: loss 0.431929\n",
      "batch 180: loss 0.267926\n",
      "batch 181: loss 0.408781\n",
      "batch 182: loss 0.231022\n",
      "batch 183: loss 0.227414\n",
      "batch 184: loss 0.349278\n",
      "batch 185: loss 0.403128\n",
      "batch 186: loss 0.189564\n",
      "batch 187: loss 0.242877\n",
      "batch 188: loss 0.357242\n",
      "batch 189: loss 0.380257\n",
      "batch 190: loss 0.358365\n",
      "batch 191: loss 0.374129\n",
      "batch 192: loss 0.308951\n",
      "batch 193: loss 0.266052\n",
      "batch 194: loss 0.348658\n",
      "batch 195: loss 0.268011\n",
      "batch 196: loss 0.271654\n",
      "batch 197: loss 0.337014\n",
      "batch 198: loss 0.311439\n",
      "batch 199: loss 0.411826\n",
      "batch 200: loss 0.485488\n",
      "batch 201: loss 0.260970\n",
      "batch 202: loss 0.226044\n",
      "batch 203: loss 0.335168\n",
      "batch 204: loss 0.125758\n",
      "batch 205: loss 0.309519\n",
      "batch 206: loss 0.341266\n",
      "batch 207: loss 0.201745\n",
      "batch 208: loss 0.236254\n",
      "batch 209: loss 0.331446\n",
      "batch 210: loss 0.353517\n",
      "batch 211: loss 0.410684\n",
      "batch 212: loss 0.402051\n",
      "batch 213: loss 0.320902\n",
      "batch 214: loss 0.317000\n",
      "batch 215: loss 0.286667\n",
      "batch 216: loss 0.225744\n",
      "batch 217: loss 0.292053\n",
      "batch 218: loss 0.358639\n",
      "batch 219: loss 0.283731\n",
      "batch 220: loss 0.308128\n",
      "batch 221: loss 0.273820\n",
      "batch 222: loss 0.234713\n",
      "batch 223: loss 0.357397\n",
      "batch 224: loss 0.403542\n",
      "batch 225: loss 0.330244\n",
      "batch 226: loss 0.504592\n",
      "batch 227: loss 0.272031\n",
      "batch 228: loss 0.167476\n",
      "batch 229: loss 0.400617\n",
      "batch 230: loss 0.442365\n",
      "batch 231: loss 0.353498\n",
      "batch 232: loss 0.403167\n",
      "batch 233: loss 0.288494\n",
      "batch 234: loss 0.309705\n",
      "batch 235: loss 0.365931\n",
      "batch 236: loss 0.243144\n",
      "batch 237: loss 0.481223\n",
      "batch 238: loss 0.262986\n",
      "batch 239: loss 0.257575\n",
      "batch 240: loss 0.267555\n",
      "batch 241: loss 0.380691\n",
      "batch 242: loss 0.269303\n",
      "batch 243: loss 0.138366\n",
      "batch 244: loss 0.136273\n",
      "batch 245: loss 0.324482\n",
      "batch 246: loss 0.415493\n",
      "batch 247: loss 0.368434\n",
      "batch 248: loss 0.374482\n",
      "batch 249: loss 0.627494\n",
      "batch 250: loss 0.394357\n",
      "batch 251: loss 0.216060\n",
      "batch 252: loss 0.454378\n",
      "batch 253: loss 0.513759\n",
      "batch 254: loss 0.422701\n",
      "batch 255: loss 0.278403\n",
      "batch 256: loss 0.250734\n",
      "batch 257: loss 0.417703\n",
      "batch 258: loss 0.328365\n",
      "batch 259: loss 0.242402\n",
      "batch 260: loss 0.294185\n",
      "batch 261: loss 0.269858\n",
      "batch 262: loss 0.233679\n",
      "batch 263: loss 0.175792\n",
      "batch 264: loss 0.269992\n",
      "batch 265: loss 0.320782\n",
      "batch 266: loss 0.410746\n",
      "batch 267: loss 0.273934\n",
      "batch 268: loss 0.267537\n",
      "batch 269: loss 0.298734\n",
      "batch 270: loss 0.391281\n",
      "batch 271: loss 0.436667\n",
      "batch 272: loss 0.312106\n",
      "batch 273: loss 0.247584\n",
      "batch 274: loss 0.256944\n",
      "batch 275: loss 0.420695\n",
      "batch 276: loss 0.369344\n",
      "batch 277: loss 0.177155\n",
      "batch 278: loss 0.242536\n",
      "batch 279: loss 0.296491\n",
      "batch 280: loss 0.345174\n",
      "batch 281: loss 0.279484\n",
      "batch 282: loss 0.187055\n",
      "batch 283: loss 0.285668\n",
      "batch 284: loss 0.299029\n",
      "batch 285: loss 0.221157\n",
      "batch 286: loss 0.121131\n",
      "batch 287: loss 0.151003\n",
      "batch 288: loss 0.378948\n",
      "batch 289: loss 0.284092\n",
      "batch 290: loss 0.178405\n",
      "batch 291: loss 0.183910\n",
      "batch 292: loss 0.242475\n",
      "batch 293: loss 0.188438\n",
      "batch 294: loss 0.307628\n",
      "batch 295: loss 0.255729\n",
      "batch 296: loss 0.321283\n",
      "batch 297: loss 0.184513\n",
      "batch 298: loss 0.260040\n",
      "batch 299: loss 0.291063\n",
      "batch 300: loss 0.351188\n",
      "batch 301: loss 0.223551\n",
      "batch 302: loss 0.521706\n",
      "batch 303: loss 0.248074\n",
      "batch 304: loss 0.170061\n",
      "batch 305: loss 0.275128\n",
      "batch 306: loss 0.211343\n",
      "batch 307: loss 0.262686\n",
      "batch 308: loss 0.268251\n",
      "batch 309: loss 0.215958\n",
      "batch 310: loss 0.463210\n",
      "batch 311: loss 0.404674\n",
      "batch 312: loss 0.526366\n",
      "batch 313: loss 0.120884\n",
      "batch 314: loss 0.348986\n",
      "batch 315: loss 0.304789\n",
      "batch 316: loss 0.232753\n",
      "batch 317: loss 0.292107\n",
      "batch 318: loss 0.358014\n",
      "batch 319: loss 0.327394\n",
      "batch 320: loss 0.340581\n",
      "batch 321: loss 0.256830\n",
      "batch 322: loss 0.306210\n",
      "batch 323: loss 0.152877\n",
      "batch 324: loss 0.213122\n",
      "batch 325: loss 0.316726\n",
      "batch 326: loss 0.178703\n",
      "batch 327: loss 0.216913\n",
      "batch 328: loss 0.444655\n",
      "batch 329: loss 0.320333\n",
      "batch 330: loss 0.418334\n",
      "batch 331: loss 0.111567\n",
      "batch 332: loss 0.369484\n",
      "batch 333: loss 0.393399\n",
      "batch 334: loss 0.184055\n",
      "batch 335: loss 0.297833\n",
      "batch 336: loss 0.488274\n",
      "batch 337: loss 0.134329\n",
      "batch 338: loss 0.150539\n",
      "batch 339: loss 0.345834\n",
      "batch 340: loss 0.563501\n",
      "batch 341: loss 0.202680\n",
      "batch 342: loss 0.284757\n",
      "batch 343: loss 0.242758\n",
      "batch 344: loss 0.212626\n",
      "batch 345: loss 0.470711\n",
      "batch 346: loss 0.165376\n",
      "batch 347: loss 0.210964\n",
      "batch 348: loss 0.195515\n",
      "batch 349: loss 0.369604\n",
      "batch 350: loss 0.402276\n",
      "batch 351: loss 0.202837\n",
      "batch 352: loss 0.205555\n",
      "batch 353: loss 0.274217\n",
      "batch 354: loss 0.155996\n",
      "batch 355: loss 0.350746\n",
      "batch 356: loss 0.249516\n",
      "batch 357: loss 0.355643\n",
      "batch 358: loss 0.218935\n",
      "batch 359: loss 0.371541\n",
      "batch 360: loss 0.444889\n",
      "batch 361: loss 0.206351\n",
      "batch 362: loss 0.303211\n",
      "batch 363: loss 0.413389\n",
      "batch 364: loss 0.156941\n",
      "batch 365: loss 0.216172\n",
      "batch 366: loss 0.229237\n",
      "batch 367: loss 0.345549\n",
      "batch 368: loss 0.691607\n",
      "batch 369: loss 0.228538\n",
      "batch 370: loss 0.250333\n",
      "batch 371: loss 0.152597\n",
      "batch 372: loss 0.221040\n",
      "batch 373: loss 0.592198\n",
      "batch 374: loss 0.202625\n",
      "batch 375: loss 0.446284\n",
      "batch 376: loss 0.295010\n",
      "batch 377: loss 0.273411\n",
      "batch 378: loss 0.301122\n",
      "batch 379: loss 0.209678\n",
      "batch 380: loss 0.371733\n",
      "batch 381: loss 0.333577\n",
      "batch 382: loss 0.212991\n",
      "batch 383: loss 0.308932\n",
      "batch 384: loss 0.202113\n",
      "batch 385: loss 0.394011\n",
      "batch 386: loss 0.484978\n",
      "batch 387: loss 0.265219\n",
      "batch 388: loss 0.159559\n",
      "batch 389: loss 0.181657\n",
      "batch 390: loss 0.296451\n",
      "batch 391: loss 0.163889\n",
      "batch 392: loss 0.211504\n",
      "batch 393: loss 0.203268\n",
      "batch 394: loss 0.214099\n",
      "batch 395: loss 0.217937\n",
      "batch 396: loss 0.416478\n",
      "batch 397: loss 0.191128\n",
      "batch 398: loss 0.291134\n",
      "batch 399: loss 0.569053\n",
      "batch 400: loss 0.199059\n",
      "batch 401: loss 0.234848\n",
      "batch 402: loss 0.167897\n",
      "batch 403: loss 0.143515\n",
      "batch 404: loss 0.222414\n",
      "batch 405: loss 0.347489\n",
      "batch 406: loss 0.452184\n",
      "batch 407: loss 0.451185\n",
      "batch 408: loss 0.180878\n",
      "batch 409: loss 0.180051\n",
      "batch 410: loss 0.149336\n",
      "batch 411: loss 0.299705\n",
      "batch 412: loss 0.222013\n",
      "batch 413: loss 0.327153\n",
      "batch 414: loss 0.290815\n",
      "batch 415: loss 0.393351\n",
      "batch 416: loss 0.371142\n",
      "batch 417: loss 0.523399\n",
      "batch 418: loss 0.432553\n",
      "batch 419: loss 0.334928\n",
      "batch 420: loss 0.354163\n",
      "batch 421: loss 0.283490\n",
      "batch 422: loss 0.294908\n",
      "batch 423: loss 0.389317\n",
      "batch 424: loss 0.185719\n",
      "batch 425: loss 0.255594\n",
      "batch 426: loss 0.241594\n",
      "batch 427: loss 0.162815\n",
      "batch 428: loss 0.308271\n",
      "batch 429: loss 0.373323\n",
      "batch 430: loss 0.266958\n",
      "batch 431: loss 0.432145\n",
      "batch 432: loss 0.202690\n",
      "batch 433: loss 0.256664\n",
      "batch 434: loss 0.463369\n",
      "batch 435: loss 0.238136\n",
      "batch 436: loss 0.338265\n",
      "batch 437: loss 0.229350\n",
      "batch 438: loss 0.134458\n",
      "batch 439: loss 0.338043\n",
      "batch 440: loss 0.156224\n",
      "batch 441: loss 0.124809\n",
      "batch 442: loss 0.199911\n",
      "batch 443: loss 0.158986\n",
      "batch 444: loss 0.240565\n",
      "batch 445: loss 0.225436\n",
      "batch 446: loss 0.297823\n",
      "batch 447: loss 0.345143\n",
      "batch 448: loss 0.231777\n",
      "batch 449: loss 0.125321\n",
      "batch 450: loss 0.160304\n",
      "batch 451: loss 0.161666\n",
      "batch 452: loss 0.328933\n",
      "batch 453: loss 0.196282\n",
      "batch 454: loss 0.213917\n",
      "batch 455: loss 0.204631\n",
      "batch 456: loss 0.593518\n",
      "batch 457: loss 0.331515\n",
      "batch 458: loss 0.442405\n",
      "batch 459: loss 0.206152\n",
      "batch 460: loss 0.348459\n",
      "batch 461: loss 0.136481\n",
      "batch 462: loss 0.174810\n",
      "batch 463: loss 0.427786\n",
      "batch 464: loss 0.279492\n",
      "batch 465: loss 0.269827\n",
      "batch 466: loss 0.211258\n",
      "batch 467: loss 0.287384\n",
      "batch 468: loss 0.446285\n",
      "batch 469: loss 0.246867\n",
      "batch 470: loss 0.305937\n",
      "batch 471: loss 0.163269\n",
      "batch 472: loss 0.301239\n",
      "batch 473: loss 0.312822\n",
      "batch 474: loss 0.296800\n",
      "batch 475: loss 0.270016\n",
      "batch 476: loss 0.195851\n",
      "batch 477: loss 0.232882\n",
      "batch 478: loss 0.225386\n",
      "batch 479: loss 0.168467\n",
      "batch 480: loss 0.221688\n",
      "batch 481: loss 0.262753\n",
      "batch 482: loss 0.143397\n",
      "batch 483: loss 0.392458\n",
      "batch 484: loss 0.314585\n",
      "batch 485: loss 0.173377\n",
      "batch 486: loss 0.198175\n",
      "batch 487: loss 0.141704\n",
      "batch 488: loss 0.255040\n",
      "batch 489: loss 0.187927\n",
      "batch 490: loss 0.173447\n",
      "batch 491: loss 0.271912\n",
      "batch 492: loss 0.179636\n",
      "batch 493: loss 0.133817\n",
      "batch 494: loss 0.175576\n",
      "batch 495: loss 0.236406\n",
      "batch 496: loss 0.092406\n",
      "batch 497: loss 0.295583\n",
      "batch 498: loss 0.316255\n",
      "batch 499: loss 0.207768\n",
      "batch 500: loss 0.132500\n",
      "batch 501: loss 0.254092\n",
      "batch 502: loss 0.119145\n",
      "batch 503: loss 0.219046\n",
      "batch 504: loss 0.099285\n",
      "batch 505: loss 0.282148\n",
      "batch 506: loss 0.206721\n",
      "batch 507: loss 0.343343\n",
      "batch 508: loss 0.528558\n",
      "batch 509: loss 0.103463\n",
      "batch 510: loss 0.214624\n",
      "batch 511: loss 0.149895\n",
      "batch 512: loss 0.380121\n",
      "batch 513: loss 0.431803\n",
      "batch 514: loss 0.152891\n",
      "batch 515: loss 0.113981\n",
      "batch 516: loss 0.379908\n",
      "batch 517: loss 0.261424\n",
      "batch 518: loss 0.208890\n",
      "batch 519: loss 0.098628\n",
      "batch 520: loss 0.168596\n",
      "batch 521: loss 0.198525\n",
      "batch 522: loss 0.228654\n",
      "batch 523: loss 0.233472\n",
      "batch 524: loss 0.278073\n",
      "batch 525: loss 0.074893\n",
      "batch 526: loss 0.199377\n",
      "batch 527: loss 0.260213\n",
      "batch 528: loss 0.264357\n",
      "batch 529: loss 0.199502\n",
      "batch 530: loss 0.159856\n",
      "batch 531: loss 0.188922\n",
      "batch 532: loss 0.189436\n",
      "batch 533: loss 0.263329\n",
      "batch 534: loss 0.390024\n",
      "batch 535: loss 0.254186\n",
      "batch 536: loss 0.272084\n",
      "batch 537: loss 0.334116\n",
      "batch 538: loss 0.188748\n",
      "batch 539: loss 0.473394\n",
      "batch 540: loss 0.328128\n",
      "batch 541: loss 0.206857\n",
      "batch 542: loss 0.194955\n",
      "batch 543: loss 0.130688\n",
      "batch 544: loss 0.218871\n",
      "batch 545: loss 0.236328\n",
      "batch 546: loss 0.396701\n",
      "batch 547: loss 0.171554\n",
      "batch 548: loss 0.402805\n",
      "batch 549: loss 0.216170\n",
      "batch 550: loss 0.235752\n",
      "batch 551: loss 0.110795\n",
      "batch 552: loss 0.311976\n",
      "batch 553: loss 0.549216\n",
      "batch 554: loss 0.161678\n",
      "batch 555: loss 0.204614\n",
      "batch 556: loss 0.408733\n",
      "batch 557: loss 0.223122\n",
      "batch 558: loss 0.185636\n",
      "batch 559: loss 0.256840\n",
      "batch 560: loss 0.180725\n",
      "batch 561: loss 0.349486\n",
      "batch 562: loss 0.094519\n",
      "batch 563: loss 0.117842\n",
      "batch 564: loss 0.523209\n",
      "batch 565: loss 0.209467\n",
      "batch 566: loss 0.214064\n",
      "batch 567: loss 0.139295\n",
      "batch 568: loss 0.250951\n",
      "batch 569: loss 0.229943\n",
      "batch 570: loss 0.221655\n",
      "batch 571: loss 0.112312\n",
      "batch 572: loss 0.145805\n",
      "batch 573: loss 0.157811\n",
      "batch 574: loss 0.327017\n",
      "batch 575: loss 0.243549\n",
      "batch 576: loss 0.349795\n",
      "batch 577: loss 0.160307\n",
      "batch 578: loss 0.215647\n",
      "batch 579: loss 0.253997\n",
      "batch 580: loss 0.220790\n",
      "batch 581: loss 0.109997\n",
      "batch 582: loss 0.213039\n",
      "batch 583: loss 0.344218\n",
      "batch 584: loss 0.131595\n",
      "batch 585: loss 0.243302\n",
      "batch 586: loss 0.260876\n",
      "batch 587: loss 0.330965\n",
      "batch 588: loss 0.189943\n",
      "batch 589: loss 0.130536\n",
      "batch 590: loss 0.353105\n",
      "batch 591: loss 0.104667\n",
      "batch 592: loss 0.270269\n",
      "batch 593: loss 0.257714\n",
      "batch 594: loss 0.185275\n",
      "batch 595: loss 0.163038\n",
      "batch 596: loss 0.296503\n",
      "batch 597: loss 0.285545\n",
      "batch 598: loss 0.122631\n",
      "batch 599: loss 0.265907\n",
      "batch 600: loss 0.155447\n",
      "batch 601: loss 0.109725\n",
      "batch 602: loss 0.115513\n",
      "batch 603: loss 0.236318\n",
      "batch 604: loss 0.266045\n",
      "batch 605: loss 0.187180\n",
      "batch 606: loss 0.289246\n",
      "batch 607: loss 0.120849\n",
      "batch 608: loss 0.094064\n",
      "batch 609: loss 0.210906\n",
      "batch 610: loss 0.172137\n",
      "batch 611: loss 0.329550\n",
      "batch 612: loss 0.194824\n",
      "batch 613: loss 0.157501\n",
      "batch 614: loss 0.286969\n",
      "batch 615: loss 0.558555\n",
      "batch 616: loss 0.496097\n",
      "batch 617: loss 0.181762\n",
      "batch 618: loss 0.105657\n",
      "batch 619: loss 0.182027\n",
      "batch 620: loss 0.152116\n",
      "batch 621: loss 0.225058\n",
      "batch 622: loss 0.090208\n",
      "batch 623: loss 0.153835\n",
      "batch 624: loss 0.149123\n",
      "batch 625: loss 0.206021\n",
      "batch 626: loss 0.305408\n",
      "batch 627: loss 0.146797\n",
      "batch 628: loss 0.267289\n",
      "batch 629: loss 0.208243\n",
      "batch 630: loss 0.307721\n",
      "batch 631: loss 0.120426\n",
      "batch 632: loss 0.239540\n",
      "batch 633: loss 0.210969\n",
      "batch 634: loss 0.116140\n",
      "batch 635: loss 0.094277\n",
      "batch 636: loss 0.124469\n",
      "batch 637: loss 0.436840\n",
      "batch 638: loss 0.222791\n",
      "batch 639: loss 0.228299\n",
      "batch 640: loss 0.301037\n",
      "batch 641: loss 0.444152\n",
      "batch 642: loss 0.195456\n",
      "batch 643: loss 0.239738\n",
      "batch 644: loss 0.268621\n",
      "batch 645: loss 0.111918\n",
      "batch 646: loss 0.344871\n",
      "batch 647: loss 0.116967\n",
      "batch 648: loss 0.189450\n",
      "batch 649: loss 0.177374\n",
      "batch 650: loss 0.250076\n",
      "batch 651: loss 0.159781\n",
      "batch 652: loss 0.134508\n",
      "batch 653: loss 0.341720\n",
      "batch 654: loss 0.108673\n",
      "batch 655: loss 0.208073\n",
      "batch 656: loss 0.292292\n",
      "batch 657: loss 0.156510\n",
      "batch 658: loss 0.195101\n",
      "batch 659: loss 0.254390\n",
      "batch 660: loss 0.201910\n",
      "batch 661: loss 0.227635\n",
      "batch 662: loss 0.166911\n",
      "batch 663: loss 0.336489\n",
      "batch 664: loss 0.124982\n",
      "batch 665: loss 0.477026\n",
      "batch 666: loss 0.369388\n",
      "batch 667: loss 0.280498\n",
      "batch 668: loss 0.156565\n",
      "batch 669: loss 0.463694\n",
      "batch 670: loss 0.236519\n",
      "batch 671: loss 0.142575\n",
      "batch 672: loss 0.225274\n",
      "batch 673: loss 0.287906\n",
      "batch 674: loss 0.340758\n",
      "batch 675: loss 0.128834\n",
      "batch 676: loss 0.133836\n",
      "batch 677: loss 0.116236\n",
      "batch 678: loss 0.123989\n",
      "batch 679: loss 0.195499\n",
      "batch 680: loss 0.177724\n",
      "batch 681: loss 0.148774\n",
      "batch 682: loss 0.184408\n",
      "batch 683: loss 0.223140\n",
      "batch 684: loss 0.315197\n",
      "batch 685: loss 0.136728\n",
      "batch 686: loss 0.247038\n",
      "batch 687: loss 0.143526\n",
      "batch 688: loss 0.373676\n",
      "batch 689: loss 0.060409\n",
      "batch 690: loss 0.107860\n",
      "batch 691: loss 0.351264\n",
      "batch 692: loss 0.152759\n",
      "batch 693: loss 0.130380\n",
      "batch 694: loss 0.303091\n",
      "batch 695: loss 0.105884\n",
      "batch 696: loss 0.121449\n",
      "batch 697: loss 0.226701\n",
      "batch 698: loss 0.410919\n",
      "batch 699: loss 0.302520\n",
      "batch 700: loss 0.062943\n",
      "batch 701: loss 0.158904\n",
      "batch 702: loss 0.120923\n",
      "batch 703: loss 0.203345\n",
      "batch 704: loss 0.253041\n",
      "batch 705: loss 0.109267\n",
      "batch 706: loss 0.408058\n",
      "batch 707: loss 0.298648\n",
      "batch 708: loss 0.222295\n",
      "batch 709: loss 0.034040\n",
      "batch 710: loss 0.147604\n",
      "batch 711: loss 0.314075\n",
      "batch 712: loss 0.168374\n",
      "batch 713: loss 0.176448\n",
      "batch 714: loss 0.080259\n",
      "batch 715: loss 0.108017\n",
      "batch 716: loss 0.235582\n",
      "batch 717: loss 0.185472\n",
      "batch 718: loss 0.130192\n",
      "batch 719: loss 0.255063\n",
      "batch 720: loss 0.473187\n",
      "batch 721: loss 0.182087\n",
      "batch 722: loss 0.426043\n",
      "batch 723: loss 0.156885\n",
      "batch 724: loss 0.172030\n",
      "batch 725: loss 0.181788\n",
      "batch 726: loss 0.299033\n",
      "batch 727: loss 0.221014\n",
      "batch 728: loss 0.209393\n",
      "batch 729: loss 0.293176\n",
      "batch 730: loss 0.251520\n",
      "batch 731: loss 0.392689\n",
      "batch 732: loss 0.233215\n",
      "batch 733: loss 0.264047\n",
      "batch 734: loss 0.197570\n",
      "batch 735: loss 0.174799\n",
      "batch 736: loss 0.174572\n",
      "batch 737: loss 0.233809\n",
      "batch 738: loss 0.417963\n",
      "batch 739: loss 0.133563\n",
      "batch 740: loss 0.184606\n",
      "batch 741: loss 0.119327\n",
      "batch 742: loss 0.157398\n",
      "batch 743: loss 0.191125\n",
      "batch 744: loss 0.064061\n",
      "batch 745: loss 0.182199\n",
      "batch 746: loss 0.233380\n",
      "batch 747: loss 0.114419\n",
      "batch 748: loss 0.219470\n",
      "batch 749: loss 0.142983\n",
      "batch 750: loss 0.279050\n",
      "batch 751: loss 0.132845\n",
      "batch 752: loss 0.190960\n",
      "batch 753: loss 0.134945\n",
      "batch 754: loss 0.126318\n",
      "batch 755: loss 0.184419\n",
      "batch 756: loss 0.168877\n",
      "batch 757: loss 0.175893\n",
      "batch 758: loss 0.170069\n",
      "batch 759: loss 0.297207\n",
      "batch 760: loss 0.242751\n",
      "batch 761: loss 0.111075\n",
      "batch 762: loss 0.106814\n",
      "batch 763: loss 0.060242\n",
      "batch 764: loss 0.148177\n",
      "batch 765: loss 0.180899\n",
      "batch 766: loss 0.170166\n",
      "batch 767: loss 0.268942\n",
      "batch 768: loss 0.247515\n",
      "batch 769: loss 0.087435\n",
      "batch 770: loss 0.112087\n",
      "batch 771: loss 0.104765\n",
      "batch 772: loss 0.248576\n",
      "batch 773: loss 0.232682\n",
      "batch 774: loss 0.270458\n",
      "batch 775: loss 0.050946\n",
      "batch 776: loss 0.281902\n",
      "batch 777: loss 0.184892\n",
      "batch 778: loss 0.400293\n",
      "batch 779: loss 0.134063\n",
      "batch 780: loss 0.202687\n",
      "batch 781: loss 0.054984\n",
      "batch 782: loss 0.260593\n",
      "batch 783: loss 0.095133\n",
      "batch 784: loss 0.166226\n",
      "batch 785: loss 0.557200\n",
      "batch 786: loss 0.446520\n",
      "batch 787: loss 0.235237\n",
      "batch 788: loss 0.331466\n",
      "batch 789: loss 0.249431\n",
      "batch 790: loss 0.062841\n",
      "batch 791: loss 0.089997\n",
      "batch 792: loss 0.240599\n",
      "batch 793: loss 0.154029\n",
      "batch 794: loss 0.201807\n",
      "batch 795: loss 0.190488\n",
      "batch 796: loss 0.205910\n",
      "batch 797: loss 0.289203\n",
      "batch 798: loss 0.362533\n",
      "batch 799: loss 0.256811\n",
      "batch 800: loss 0.171819\n",
      "batch 801: loss 0.116833\n",
      "batch 802: loss 0.057875\n",
      "batch 803: loss 0.124436\n",
      "batch 804: loss 0.110019\n",
      "batch 805: loss 0.278518\n",
      "batch 806: loss 0.501397\n",
      "batch 807: loss 0.459828\n",
      "batch 808: loss 0.220435\n",
      "batch 809: loss 0.266588\n",
      "batch 810: loss 0.216469\n",
      "batch 811: loss 0.147284\n",
      "batch 812: loss 0.211477\n",
      "batch 813: loss 0.189485\n",
      "batch 814: loss 0.220078\n",
      "batch 815: loss 0.237724\n",
      "batch 816: loss 0.131321\n",
      "batch 817: loss 0.099917\n",
      "batch 818: loss 0.248536\n",
      "batch 819: loss 0.285745\n",
      "batch 820: loss 0.220633\n",
      "batch 821: loss 0.150124\n",
      "batch 822: loss 0.082396\n",
      "batch 823: loss 0.223154\n",
      "batch 824: loss 0.172512\n",
      "batch 825: loss 0.131169\n",
      "batch 826: loss 0.061460\n",
      "batch 827: loss 0.119508\n",
      "batch 828: loss 0.121879\n",
      "batch 829: loss 0.193040\n",
      "batch 830: loss 0.097015\n",
      "batch 831: loss 0.116365\n",
      "batch 832: loss 0.237858\n",
      "batch 833: loss 0.121030\n",
      "batch 834: loss 0.295844\n",
      "batch 835: loss 0.342889\n",
      "batch 836: loss 0.280704\n",
      "batch 837: loss 0.285715\n",
      "batch 838: loss 0.154972\n",
      "batch 839: loss 0.223179\n",
      "batch 840: loss 0.371085\n",
      "batch 841: loss 0.261559\n",
      "batch 842: loss 0.078438\n",
      "batch 843: loss 0.219768\n",
      "batch 844: loss 0.090372\n",
      "batch 845: loss 0.189135\n",
      "batch 846: loss 0.055503\n",
      "batch 847: loss 0.111007\n",
      "batch 848: loss 0.059137\n",
      "batch 849: loss 0.172517\n",
      "batch 850: loss 0.159127\n",
      "batch 851: loss 0.218087\n",
      "batch 852: loss 0.126668\n",
      "batch 853: loss 0.187913\n",
      "batch 854: loss 0.131422\n",
      "batch 855: loss 0.112207\n",
      "batch 856: loss 0.182220\n",
      "batch 857: loss 0.344523\n",
      "batch 858: loss 0.175489\n",
      "batch 859: loss 0.237403\n",
      "batch 860: loss 0.131337\n",
      "batch 861: loss 0.240089\n",
      "batch 862: loss 0.257929\n",
      "batch 863: loss 0.113451\n",
      "batch 864: loss 0.093149\n",
      "batch 865: loss 0.072976\n",
      "batch 866: loss 0.281641\n",
      "batch 867: loss 0.189496\n",
      "batch 868: loss 0.101854\n",
      "batch 869: loss 0.274034\n",
      "batch 870: loss 0.231346\n",
      "batch 871: loss 0.421141\n",
      "batch 872: loss 0.065912\n",
      "batch 873: loss 0.177050\n",
      "batch 874: loss 0.381727\n",
      "batch 875: loss 0.288866\n",
      "batch 876: loss 0.165391\n",
      "batch 877: loss 0.088047\n",
      "batch 878: loss 0.181709\n",
      "batch 879: loss 0.070465\n",
      "batch 880: loss 0.184463\n",
      "batch 881: loss 0.052412\n",
      "batch 882: loss 0.266373\n",
      "batch 883: loss 0.180207\n",
      "batch 884: loss 0.459718\n",
      "batch 885: loss 0.146861\n",
      "batch 886: loss 0.166590\n",
      "batch 887: loss 0.164041\n",
      "batch 888: loss 0.093230\n",
      "batch 889: loss 0.453916\n",
      "batch 890: loss 0.193381\n",
      "batch 891: loss 0.408738\n",
      "batch 892: loss 0.122687\n",
      "batch 893: loss 0.114529\n",
      "batch 894: loss 0.381763\n",
      "batch 895: loss 0.171378\n",
      "batch 896: loss 0.184021\n",
      "batch 897: loss 0.185828\n",
      "batch 898: loss 0.139617\n",
      "batch 899: loss 0.270453\n",
      "batch 900: loss 0.196397\n",
      "batch 901: loss 0.197214\n",
      "batch 902: loss 0.331119\n",
      "batch 903: loss 0.229827\n",
      "batch 904: loss 0.142425\n",
      "batch 905: loss 0.113295\n",
      "batch 906: loss 0.055472\n",
      "batch 907: loss 0.148447\n",
      "batch 908: loss 0.229509\n",
      "batch 909: loss 0.292249\n",
      "batch 910: loss 0.187493\n",
      "batch 911: loss 0.190626\n",
      "batch 912: loss 0.312511\n",
      "batch 913: loss 0.112060\n",
      "batch 914: loss 0.118325\n",
      "batch 915: loss 0.119003\n",
      "batch 916: loss 0.111534\n",
      "batch 917: loss 0.321756\n",
      "batch 918: loss 0.219135\n",
      "batch 919: loss 0.126908\n",
      "batch 920: loss 0.285716\n",
      "batch 921: loss 0.185297\n",
      "batch 922: loss 0.098765\n",
      "batch 923: loss 0.196280\n",
      "batch 924: loss 0.094661\n",
      "batch 925: loss 0.084798\n",
      "batch 926: loss 0.316464\n",
      "batch 927: loss 0.176912\n",
      "batch 928: loss 0.245786\n",
      "batch 929: loss 0.143274\n",
      "batch 930: loss 0.214346\n",
      "batch 931: loss 0.146514\n",
      "batch 932: loss 0.078681\n",
      "batch 933: loss 0.151180\n",
      "batch 934: loss 0.165735\n",
      "batch 935: loss 0.130902\n",
      "batch 936: loss 0.159255\n",
      "batch 937: loss 0.126460\n",
      "batch 938: loss 0.139694\n",
      "batch 939: loss 0.296845\n",
      "batch 940: loss 0.300107\n",
      "batch 941: loss 0.183813\n",
      "batch 942: loss 0.139139\n",
      "batch 943: loss 0.492294\n",
      "batch 944: loss 0.118074\n",
      "batch 945: loss 0.048867\n",
      "batch 946: loss 0.156152\n",
      "batch 947: loss 0.129381\n",
      "batch 948: loss 0.391078\n",
      "batch 949: loss 0.085401\n",
      "batch 950: loss 0.155897\n",
      "batch 951: loss 0.167783\n",
      "batch 952: loss 0.165123\n",
      "batch 953: loss 0.388287\n",
      "batch 954: loss 0.340552\n",
      "batch 955: loss 0.121672\n",
      "batch 956: loss 0.165045\n",
      "batch 957: loss 0.184689\n",
      "batch 958: loss 0.101296\n",
      "batch 959: loss 0.099517\n",
      "batch 960: loss 0.109418\n",
      "batch 961: loss 0.282929\n",
      "batch 962: loss 0.074725\n",
      "batch 963: loss 0.063700\n",
      "batch 964: loss 0.101270\n",
      "batch 965: loss 0.265683\n",
      "batch 966: loss 0.097525\n",
      "batch 967: loss 0.172093\n",
      "batch 968: loss 0.179990\n",
      "batch 969: loss 0.099180\n",
      "batch 970: loss 0.086617\n",
      "batch 971: loss 0.332128\n",
      "batch 972: loss 0.138163\n",
      "batch 973: loss 0.093676\n",
      "batch 974: loss 0.138968\n",
      "batch 975: loss 0.091347\n",
      "batch 976: loss 0.270686\n",
      "batch 977: loss 0.107057\n",
      "batch 978: loss 0.218311\n",
      "batch 979: loss 0.279776\n",
      "batch 980: loss 0.245852\n",
      "batch 981: loss 0.192222\n",
      "batch 982: loss 0.157689\n",
      "batch 983: loss 0.084900\n",
      "batch 984: loss 0.347074\n",
      "batch 985: loss 0.217470\n",
      "batch 986: loss 0.120937\n",
      "batch 987: loss 0.139169\n",
      "batch 988: loss 0.160734\n",
      "batch 989: loss 0.245348\n",
      "batch 990: loss 0.389291\n",
      "batch 991: loss 0.229483\n",
      "batch 992: loss 0.158411\n",
      "batch 993: loss 0.121529\n",
      "batch 994: loss 0.465903\n",
      "batch 995: loss 0.249526\n",
      "batch 996: loss 0.131839\n",
      "batch 997: loss 0.237368\n",
      "batch 998: loss 0.107455\n",
      "batch 999: loss 0.134219\n",
      "batch 1000: loss 0.186098\n",
      "batch 1001: loss 0.104035\n",
      "batch 1002: loss 0.100598\n",
      "batch 1003: loss 0.188745\n",
      "batch 1004: loss 0.165015\n",
      "batch 1005: loss 0.242076\n",
      "batch 1006: loss 0.088367\n",
      "batch 1007: loss 0.234585\n",
      "batch 1008: loss 0.239466\n",
      "batch 1009: loss 0.258128\n",
      "batch 1010: loss 0.264560\n",
      "batch 1011: loss 0.100657\n",
      "batch 1012: loss 0.103046\n",
      "batch 1013: loss 0.185152\n",
      "batch 1014: loss 0.250930\n",
      "batch 1015: loss 0.125596\n",
      "batch 1016: loss 0.146739\n",
      "batch 1017: loss 0.196840\n",
      "batch 1018: loss 0.120004\n",
      "batch 1019: loss 0.217798\n",
      "batch 1020: loss 0.184770\n",
      "batch 1021: loss 0.106577\n",
      "batch 1022: loss 0.118866\n",
      "batch 1023: loss 0.205957\n",
      "batch 1024: loss 0.179155\n",
      "batch 1025: loss 0.092502\n",
      "batch 1026: loss 0.094982\n",
      "batch 1027: loss 0.305848\n",
      "batch 1028: loss 0.153042\n",
      "batch 1029: loss 0.165986\n",
      "batch 1030: loss 0.119045\n",
      "batch 1031: loss 0.087293\n",
      "batch 1032: loss 0.439752\n",
      "batch 1033: loss 0.096058\n",
      "batch 1034: loss 0.106036\n",
      "batch 1035: loss 0.254848\n",
      "batch 1036: loss 0.157952\n",
      "batch 1037: loss 0.106957\n",
      "batch 1038: loss 0.326424\n",
      "batch 1039: loss 0.117189\n",
      "batch 1040: loss 0.146047\n",
      "batch 1041: loss 0.177563\n",
      "batch 1042: loss 0.270210\n",
      "batch 1043: loss 0.033859\n",
      "batch 1044: loss 0.154976\n",
      "batch 1045: loss 0.098835\n",
      "batch 1046: loss 0.169767\n",
      "batch 1047: loss 0.030627\n",
      "batch 1048: loss 0.173619\n",
      "batch 1049: loss 0.155583\n",
      "batch 1050: loss 0.173377\n",
      "batch 1051: loss 0.344551\n",
      "batch 1052: loss 0.093776\n",
      "batch 1053: loss 0.187644\n",
      "batch 1054: loss 0.180409\n",
      "batch 1055: loss 0.070724\n",
      "batch 1056: loss 0.175470\n",
      "batch 1057: loss 0.339538\n",
      "batch 1058: loss 0.274253\n",
      "batch 1059: loss 0.033454\n",
      "batch 1060: loss 0.330601\n",
      "batch 1061: loss 0.173428\n",
      "batch 1062: loss 0.147823\n",
      "batch 1063: loss 0.191239\n",
      "batch 1064: loss 0.157896\n",
      "batch 1065: loss 0.327011\n",
      "batch 1066: loss 0.127728\n",
      "batch 1067: loss 0.194872\n",
      "batch 1068: loss 0.178685\n",
      "batch 1069: loss 0.200858\n",
      "batch 1070: loss 0.083586\n",
      "batch 1071: loss 0.349400\n",
      "batch 1072: loss 0.175625\n",
      "batch 1073: loss 0.175037\n",
      "batch 1074: loss 0.124628\n",
      "batch 1075: loss 0.258962\n",
      "batch 1076: loss 0.209016\n",
      "batch 1077: loss 0.223044\n",
      "batch 1078: loss 0.220621\n",
      "batch 1079: loss 0.162673\n",
      "batch 1080: loss 0.174808\n",
      "batch 1081: loss 0.220907\n",
      "batch 1082: loss 0.164659\n",
      "batch 1083: loss 0.108062\n",
      "batch 1084: loss 0.294947\n",
      "batch 1085: loss 0.338879\n",
      "batch 1086: loss 0.093297\n",
      "batch 1087: loss 0.160573\n",
      "batch 1088: loss 0.292287\n",
      "batch 1089: loss 0.184992\n",
      "batch 1090: loss 0.224919\n",
      "batch 1091: loss 0.131681\n",
      "batch 1092: loss 0.060349\n",
      "batch 1093: loss 0.089176\n",
      "batch 1094: loss 0.158537\n",
      "batch 1095: loss 0.138421\n",
      "batch 1096: loss 0.232866\n",
      "batch 1097: loss 0.036484\n",
      "batch 1098: loss 0.156067\n",
      "batch 1099: loss 0.139177\n",
      "batch 1100: loss 0.068833\n",
      "batch 1101: loss 0.115884\n",
      "batch 1102: loss 0.274961\n",
      "batch 1103: loss 0.139354\n",
      "batch 1104: loss 0.133319\n",
      "batch 1105: loss 0.166508\n",
      "batch 1106: loss 0.088983\n",
      "batch 1107: loss 0.125683\n",
      "batch 1108: loss 0.079736\n",
      "batch 1109: loss 0.070331\n",
      "batch 1110: loss 0.265867\n",
      "batch 1111: loss 0.203708\n",
      "batch 1112: loss 0.229660\n",
      "batch 1113: loss 0.205811\n",
      "batch 1114: loss 0.172890\n",
      "batch 1115: loss 0.124115\n",
      "batch 1116: loss 0.279864\n",
      "batch 1117: loss 0.207885\n",
      "batch 1118: loss 0.208996\n",
      "batch 1119: loss 0.109230\n",
      "batch 1120: loss 0.063642\n",
      "batch 1121: loss 0.182742\n",
      "batch 1122: loss 0.173475\n",
      "batch 1123: loss 0.136659\n",
      "batch 1124: loss 0.107289\n",
      "batch 1125: loss 0.138065\n",
      "batch 1126: loss 0.268624\n",
      "batch 1127: loss 0.231482\n",
      "batch 1128: loss 0.055004\n",
      "batch 1129: loss 0.302806\n",
      "batch 1130: loss 0.089982\n",
      "batch 1131: loss 0.179197\n",
      "batch 1132: loss 0.088716\n",
      "batch 1133: loss 0.379740\n",
      "batch 1134: loss 0.055728\n",
      "batch 1135: loss 0.095478\n",
      "batch 1136: loss 0.133464\n",
      "batch 1137: loss 0.166412\n",
      "batch 1138: loss 0.212449\n",
      "batch 1139: loss 0.074236\n",
      "batch 1140: loss 0.120538\n",
      "batch 1141: loss 0.159244\n",
      "batch 1142: loss 0.234775\n",
      "batch 1143: loss 0.395737\n",
      "batch 1144: loss 0.152304\n",
      "batch 1145: loss 0.113665\n",
      "batch 1146: loss 0.130589\n",
      "batch 1147: loss 0.123585\n",
      "batch 1148: loss 0.169462\n",
      "batch 1149: loss 0.119075\n",
      "batch 1150: loss 0.056366\n",
      "batch 1151: loss 0.068089\n",
      "batch 1152: loss 0.126005\n",
      "batch 1153: loss 0.144115\n",
      "batch 1154: loss 0.247782\n",
      "batch 1155: loss 0.185111\n",
      "batch 1156: loss 0.207601\n",
      "batch 1157: loss 0.159618\n",
      "batch 1158: loss 0.066673\n",
      "batch 1159: loss 0.195399\n",
      "batch 1160: loss 0.147393\n",
      "batch 1161: loss 0.114872\n",
      "batch 1162: loss 0.256580\n",
      "batch 1163: loss 0.153029\n",
      "batch 1164: loss 0.083144\n",
      "batch 1165: loss 0.302032\n",
      "batch 1166: loss 0.115461\n",
      "batch 1167: loss 0.127720\n",
      "batch 1168: loss 0.115092\n",
      "batch 1169: loss 0.100245\n",
      "batch 1170: loss 0.213190\n",
      "batch 1171: loss 0.282114\n",
      "batch 1172: loss 0.101903\n",
      "batch 1173: loss 0.187827\n",
      "batch 1174: loss 0.067100\n",
      "batch 1175: loss 0.167785\n",
      "batch 1176: loss 0.230385\n",
      "batch 1177: loss 0.092041\n",
      "batch 1178: loss 0.131946\n",
      "batch 1179: loss 0.119349\n",
      "batch 1180: loss 0.299299\n",
      "batch 1181: loss 0.153307\n",
      "batch 1182: loss 0.067711\n",
      "batch 1183: loss 0.117969\n",
      "batch 1184: loss 0.073404\n",
      "batch 1185: loss 0.144544\n",
      "batch 1186: loss 0.108529\n",
      "batch 1187: loss 0.231292\n",
      "batch 1188: loss 0.071138\n",
      "batch 1189: loss 0.213953\n",
      "batch 1190: loss 0.166297\n",
      "batch 1191: loss 0.092201\n",
      "batch 1192: loss 0.099408\n",
      "batch 1193: loss 0.126469\n",
      "batch 1194: loss 0.124075\n",
      "batch 1195: loss 0.155984\n",
      "batch 1196: loss 0.111099\n",
      "batch 1197: loss 0.168680\n",
      "batch 1198: loss 0.155435\n",
      "batch 1199: loss 0.146284\n",
      "batch 1200: loss 0.025371\n",
      "batch 1201: loss 0.102532\n",
      "batch 1202: loss 0.176767\n",
      "batch 1203: loss 0.182719\n",
      "batch 1204: loss 0.110800\n",
      "batch 1205: loss 0.212286\n",
      "batch 1206: loss 0.273663\n",
      "batch 1207: loss 0.207073\n",
      "batch 1208: loss 0.077199\n",
      "batch 1209: loss 0.307078\n",
      "batch 1210: loss 0.141868\n",
      "batch 1211: loss 0.206234\n",
      "batch 1212: loss 0.170620\n",
      "batch 1213: loss 0.074693\n",
      "batch 1214: loss 0.217704\n",
      "batch 1215: loss 0.099406\n",
      "batch 1216: loss 0.107509\n",
      "batch 1217: loss 0.219466\n",
      "batch 1218: loss 0.052221\n",
      "batch 1219: loss 0.090446\n",
      "batch 1220: loss 0.131095\n",
      "batch 1221: loss 0.130426\n",
      "batch 1222: loss 0.132052\n",
      "batch 1223: loss 0.082217\n",
      "batch 1224: loss 0.070390\n",
      "batch 1225: loss 0.206540\n",
      "batch 1226: loss 0.137279\n",
      "batch 1227: loss 0.063354\n",
      "batch 1228: loss 0.136853\n",
      "batch 1229: loss 0.088265\n",
      "batch 1230: loss 0.299426\n",
      "batch 1231: loss 0.027788\n",
      "batch 1232: loss 0.162741\n",
      "batch 1233: loss 0.395558\n",
      "batch 1234: loss 0.191822\n",
      "batch 1235: loss 0.099911\n",
      "batch 1236: loss 0.182889\n",
      "batch 1237: loss 0.268490\n",
      "batch 1238: loss 0.121860\n",
      "batch 1239: loss 0.139108\n",
      "batch 1240: loss 0.165831\n",
      "batch 1241: loss 0.189903\n",
      "batch 1242: loss 0.112683\n",
      "batch 1243: loss 0.134368\n",
      "batch 1244: loss 0.134221\n",
      "batch 1245: loss 0.438356\n",
      "batch 1246: loss 0.200343\n",
      "batch 1247: loss 0.139087\n",
      "batch 1248: loss 0.121066\n",
      "batch 1249: loss 0.140450\n",
      "batch 1250: loss 0.193090\n",
      "batch 1251: loss 0.144550\n",
      "batch 1252: loss 0.097831\n",
      "batch 1253: loss 0.115062\n",
      "batch 1254: loss 0.030203\n",
      "batch 1255: loss 0.045427\n",
      "batch 1256: loss 0.214377\n",
      "batch 1257: loss 0.074858\n",
      "batch 1258: loss 0.052960\n",
      "batch 1259: loss 0.261529\n",
      "batch 1260: loss 0.074368\n",
      "batch 1261: loss 0.180007\n",
      "batch 1262: loss 0.128053\n",
      "batch 1263: loss 0.198414\n",
      "batch 1264: loss 0.064466\n",
      "batch 1265: loss 0.180403\n",
      "batch 1266: loss 0.040318\n",
      "batch 1267: loss 0.063696\n",
      "batch 1268: loss 0.158649\n",
      "batch 1269: loss 0.110928\n",
      "batch 1270: loss 0.224991\n",
      "batch 1271: loss 0.346438\n",
      "batch 1272: loss 0.071763\n",
      "batch 1273: loss 0.176856\n",
      "batch 1274: loss 0.187347\n",
      "batch 1275: loss 0.296944\n",
      "batch 1276: loss 0.152481\n",
      "batch 1277: loss 0.043891\n",
      "batch 1278: loss 0.215067\n",
      "batch 1279: loss 0.177247\n",
      "batch 1280: loss 0.295152\n",
      "batch 1281: loss 0.187837\n",
      "batch 1282: loss 0.116239\n",
      "batch 1283: loss 0.169190\n",
      "batch 1284: loss 0.075246\n",
      "batch 1285: loss 0.096099\n",
      "batch 1286: loss 0.194057\n",
      "batch 1287: loss 0.104660\n",
      "batch 1288: loss 0.087596\n",
      "batch 1289: loss 0.058102\n",
      "batch 1290: loss 0.091209\n",
      "batch 1291: loss 0.134080\n",
      "batch 1292: loss 0.096960\n",
      "batch 1293: loss 0.097533\n",
      "batch 1294: loss 0.099805\n",
      "batch 1295: loss 0.228055\n",
      "batch 1296: loss 0.040357\n",
      "batch 1297: loss 0.138212\n",
      "batch 1298: loss 0.079399\n",
      "batch 1299: loss 0.230966\n",
      "batch 1300: loss 0.107451\n",
      "batch 1301: loss 0.195673\n",
      "batch 1302: loss 0.130444\n",
      "batch 1303: loss 0.167098\n",
      "batch 1304: loss 0.081544\n",
      "batch 1305: loss 0.137750\n",
      "batch 1306: loss 0.095040\n",
      "batch 1307: loss 0.115074\n",
      "batch 1308: loss 0.136445\n",
      "batch 1309: loss 0.080899\n",
      "batch 1310: loss 0.183737\n",
      "batch 1311: loss 0.179660\n",
      "batch 1312: loss 0.283575\n",
      "batch 1313: loss 0.186997\n",
      "batch 1314: loss 0.110118\n",
      "batch 1315: loss 0.143403\n",
      "batch 1316: loss 0.061641\n",
      "batch 1317: loss 0.123785\n",
      "batch 1318: loss 0.100488\n",
      "batch 1319: loss 0.073349\n",
      "batch 1320: loss 0.042418\n",
      "batch 1321: loss 0.294257\n",
      "batch 1322: loss 0.310387\n",
      "batch 1323: loss 0.107436\n",
      "batch 1324: loss 0.257776\n",
      "batch 1325: loss 0.238381\n",
      "batch 1326: loss 0.138359\n",
      "batch 1327: loss 0.086685\n",
      "batch 1328: loss 0.114906\n",
      "batch 1329: loss 0.095298\n",
      "batch 1330: loss 0.102732\n",
      "batch 1331: loss 0.085876\n",
      "batch 1332: loss 0.138279\n",
      "batch 1333: loss 0.042346\n",
      "batch 1334: loss 0.030331\n",
      "batch 1335: loss 0.030490\n",
      "batch 1336: loss 0.293801\n",
      "batch 1337: loss 0.111819\n",
      "batch 1338: loss 0.122458\n",
      "batch 1339: loss 0.063260\n",
      "batch 1340: loss 0.239295\n",
      "batch 1341: loss 0.173259\n",
      "batch 1342: loss 0.069630\n",
      "batch 1343: loss 0.079352\n",
      "batch 1344: loss 0.211258\n",
      "batch 1345: loss 0.132897\n",
      "batch 1346: loss 0.182913\n",
      "batch 1347: loss 0.249266\n",
      "batch 1348: loss 0.364117\n",
      "batch 1349: loss 0.115039\n",
      "batch 1350: loss 0.097020\n",
      "batch 1351: loss 0.094260\n",
      "batch 1352: loss 0.171029\n",
      "batch 1353: loss 0.342492\n",
      "batch 1354: loss 0.061999\n",
      "batch 1355: loss 0.110885\n",
      "batch 1356: loss 0.241748\n",
      "batch 1357: loss 0.291999\n",
      "batch 1358: loss 0.277414\n",
      "batch 1359: loss 0.091453\n",
      "batch 1360: loss 0.141219\n",
      "batch 1361: loss 0.030177\n",
      "batch 1362: loss 0.156952\n",
      "batch 1363: loss 0.052503\n",
      "batch 1364: loss 0.098889\n",
      "batch 1365: loss 0.107798\n",
      "batch 1366: loss 0.133746\n",
      "batch 1367: loss 0.233556\n",
      "batch 1368: loss 0.245231\n",
      "batch 1369: loss 0.234779\n",
      "batch 1370: loss 0.104678\n",
      "batch 1371: loss 0.238982\n",
      "batch 1372: loss 0.221894\n",
      "batch 1373: loss 0.071304\n",
      "batch 1374: loss 0.097550\n",
      "batch 1375: loss 0.045041\n",
      "batch 1376: loss 0.133083\n",
      "batch 1377: loss 0.094581\n",
      "batch 1378: loss 0.095152\n",
      "batch 1379: loss 0.252666\n",
      "batch 1380: loss 0.120216\n",
      "batch 1381: loss 0.127872\n",
      "batch 1382: loss 0.095317\n",
      "batch 1383: loss 0.095437\n",
      "batch 1384: loss 0.112094\n",
      "batch 1385: loss 0.134869\n",
      "batch 1386: loss 0.113887\n",
      "batch 1387: loss 0.129369\n",
      "batch 1388: loss 0.062593\n",
      "batch 1389: loss 0.153484\n",
      "batch 1390: loss 0.118335\n",
      "batch 1391: loss 0.226684\n",
      "batch 1392: loss 0.098148\n",
      "batch 1393: loss 0.310874\n",
      "batch 1394: loss 0.223586\n",
      "batch 1395: loss 0.249076\n",
      "batch 1396: loss 0.137835\n",
      "batch 1397: loss 0.082275\n",
      "batch 1398: loss 0.063003\n",
      "batch 1399: loss 0.117765\n",
      "batch 1400: loss 0.182266\n",
      "batch 1401: loss 0.266216\n",
      "batch 1402: loss 0.052342\n",
      "batch 1403: loss 0.138438\n",
      "batch 1404: loss 0.070592\n",
      "batch 1405: loss 0.065214\n",
      "batch 1406: loss 0.200978\n",
      "batch 1407: loss 0.148095\n",
      "batch 1408: loss 0.170226\n",
      "batch 1409: loss 0.222719\n",
      "batch 1410: loss 0.063411\n",
      "batch 1411: loss 0.115553\n",
      "batch 1412: loss 0.175457\n",
      "batch 1413: loss 0.095806\n",
      "batch 1414: loss 0.063838\n",
      "batch 1415: loss 0.180512\n",
      "batch 1416: loss 0.145619\n",
      "batch 1417: loss 0.035774\n",
      "batch 1418: loss 0.070830\n",
      "batch 1419: loss 0.125267\n",
      "batch 1420: loss 0.250993\n",
      "batch 1421: loss 0.163247\n",
      "batch 1422: loss 0.112793\n",
      "batch 1423: loss 0.096417\n",
      "batch 1424: loss 0.049205\n",
      "batch 1425: loss 0.127113\n",
      "batch 1426: loss 0.055573\n",
      "batch 1427: loss 0.114747\n",
      "batch 1428: loss 0.147379\n",
      "batch 1429: loss 0.169443\n",
      "batch 1430: loss 0.187288\n",
      "batch 1431: loss 0.173180\n",
      "batch 1432: loss 0.071177\n",
      "batch 1433: loss 0.186148\n",
      "batch 1434: loss 0.108158\n",
      "batch 1435: loss 0.051410\n",
      "batch 1436: loss 0.170390\n",
      "batch 1437: loss 0.151714\n",
      "batch 1438: loss 0.122995\n",
      "batch 1439: loss 0.083499\n",
      "batch 1440: loss 0.104777\n",
      "batch 1441: loss 0.091928\n",
      "batch 1442: loss 0.279916\n",
      "batch 1443: loss 0.133678\n",
      "batch 1444: loss 0.045376\n",
      "batch 1445: loss 0.310753\n",
      "batch 1446: loss 0.167839\n",
      "batch 1447: loss 0.080043\n",
      "batch 1448: loss 0.120708\n",
      "batch 1449: loss 0.143312\n",
      "batch 1450: loss 0.241471\n",
      "batch 1451: loss 0.137692\n",
      "batch 1452: loss 0.153128\n",
      "batch 1453: loss 0.195228\n",
      "batch 1454: loss 0.097833\n",
      "batch 1455: loss 0.267057\n",
      "batch 1456: loss 0.300279\n",
      "batch 1457: loss 0.066623\n",
      "batch 1458: loss 0.070973\n",
      "batch 1459: loss 0.025259\n",
      "batch 1460: loss 0.179690\n",
      "batch 1461: loss 0.137221\n",
      "batch 1462: loss 0.092403\n",
      "batch 1463: loss 0.019424\n",
      "batch 1464: loss 0.096031\n",
      "batch 1465: loss 0.188104\n",
      "batch 1466: loss 0.172328\n",
      "batch 1467: loss 0.235239\n",
      "batch 1468: loss 0.224828\n",
      "batch 1469: loss 0.105343\n",
      "batch 1470: loss 0.127413\n",
      "batch 1471: loss 0.152485\n",
      "batch 1472: loss 0.087052\n",
      "batch 1473: loss 0.062034\n",
      "batch 1474: loss 0.052423\n",
      "batch 1475: loss 0.112310\n",
      "batch 1476: loss 0.048289\n",
      "batch 1477: loss 0.147305\n",
      "batch 1478: loss 0.037318\n",
      "batch 1479: loss 0.159888\n",
      "batch 1480: loss 0.112274\n",
      "batch 1481: loss 0.078614\n",
      "batch 1482: loss 0.340071\n",
      "batch 1483: loss 0.131415\n",
      "batch 1484: loss 0.254126\n",
      "batch 1485: loss 0.223322\n",
      "batch 1486: loss 0.177714\n",
      "batch 1487: loss 0.083434\n",
      "batch 1488: loss 0.089519\n",
      "batch 1489: loss 0.266045\n",
      "batch 1490: loss 0.184205\n",
      "batch 1491: loss 0.150237\n",
      "batch 1492: loss 0.111135\n",
      "batch 1493: loss 0.061179\n",
      "batch 1494: loss 0.172070\n",
      "batch 1495: loss 0.125716\n",
      "batch 1496: loss 0.271842\n",
      "batch 1497: loss 0.040442\n",
      "batch 1498: loss 0.171082\n",
      "batch 1499: loss 0.167972\n",
      "batch 1500: loss 0.121392\n",
      "batch 1501: loss 0.104558\n",
      "batch 1502: loss 0.055197\n",
      "batch 1503: loss 0.141011\n",
      "batch 1504: loss 0.104728\n",
      "batch 1505: loss 0.070117\n",
      "batch 1506: loss 0.188496\n",
      "batch 1507: loss 0.062272\n",
      "batch 1508: loss 0.122398\n",
      "batch 1509: loss 0.020874\n",
      "batch 1510: loss 0.193479\n",
      "batch 1511: loss 0.055007\n",
      "batch 1512: loss 0.048973\n",
      "batch 1513: loss 0.044558\n",
      "batch 1514: loss 0.304358\n",
      "batch 1515: loss 0.133293\n",
      "batch 1516: loss 0.521026\n",
      "batch 1517: loss 0.142560\n",
      "batch 1518: loss 0.044530\n",
      "batch 1519: loss 0.073520\n",
      "batch 1520: loss 0.148987\n",
      "batch 1521: loss 0.129455\n",
      "batch 1522: loss 0.081048\n",
      "batch 1523: loss 0.173344\n",
      "batch 1524: loss 0.206013\n",
      "batch 1525: loss 0.240473\n",
      "batch 1526: loss 0.190998\n",
      "batch 1527: loss 0.213442\n",
      "batch 1528: loss 0.111088\n",
      "batch 1529: loss 0.081499\n",
      "batch 1530: loss 0.056749\n",
      "batch 1531: loss 0.133117\n",
      "batch 1532: loss 0.175670\n",
      "batch 1533: loss 0.252906\n",
      "batch 1534: loss 0.242899\n",
      "batch 1535: loss 0.131729\n",
      "batch 1536: loss 0.114714\n",
      "batch 1537: loss 0.308342\n",
      "batch 1538: loss 0.109762\n",
      "batch 1539: loss 0.106099\n",
      "batch 1540: loss 0.148931\n",
      "batch 1541: loss 0.104775\n",
      "batch 1542: loss 0.072668\n",
      "batch 1543: loss 0.169916\n",
      "batch 1544: loss 0.191467\n",
      "batch 1545: loss 0.148325\n",
      "batch 1546: loss 0.022818\n",
      "batch 1547: loss 0.108411\n",
      "batch 1548: loss 0.083155\n",
      "batch 1549: loss 0.087689\n",
      "batch 1550: loss 0.053618\n",
      "batch 1551: loss 0.164602\n",
      "batch 1552: loss 0.186598\n",
      "batch 1553: loss 0.163241\n",
      "batch 1554: loss 0.267648\n",
      "batch 1555: loss 0.063102\n",
      "batch 1556: loss 0.077092\n",
      "batch 1557: loss 0.187320\n",
      "batch 1558: loss 0.158247\n",
      "batch 1559: loss 0.052941\n",
      "batch 1560: loss 0.200040\n",
      "batch 1561: loss 0.178882\n",
      "batch 1562: loss 0.174738\n",
      "batch 1563: loss 0.102497\n",
      "batch 1564: loss 0.034423\n",
      "batch 1565: loss 0.049261\n",
      "batch 1566: loss 0.162405\n",
      "batch 1567: loss 0.061224\n",
      "batch 1568: loss 0.269160\n",
      "batch 1569: loss 0.050528\n",
      "batch 1570: loss 0.158166\n",
      "batch 1571: loss 0.145959\n",
      "batch 1572: loss 0.177307\n",
      "batch 1573: loss 0.044075\n",
      "batch 1574: loss 0.087371\n",
      "batch 1575: loss 0.184766\n",
      "batch 1576: loss 0.084441\n",
      "batch 1577: loss 0.136367\n",
      "batch 1578: loss 0.086516\n",
      "batch 1579: loss 0.278212\n",
      "batch 1580: loss 0.099608\n",
      "batch 1581: loss 0.056418\n",
      "batch 1582: loss 0.136576\n",
      "batch 1583: loss 0.195186\n",
      "batch 1584: loss 0.191983\n",
      "batch 1585: loss 0.259282\n",
      "batch 1586: loss 0.143995\n",
      "batch 1587: loss 0.132220\n",
      "batch 1588: loss 0.057254\n",
      "batch 1589: loss 0.082881\n",
      "batch 1590: loss 0.120735\n",
      "batch 1591: loss 0.146563\n",
      "batch 1592: loss 0.108566\n",
      "batch 1593: loss 0.058252\n",
      "batch 1594: loss 0.306272\n",
      "batch 1595: loss 0.169034\n",
      "batch 1596: loss 0.095450\n",
      "batch 1597: loss 0.089345\n",
      "batch 1598: loss 0.133427\n",
      "batch 1599: loss 0.177791\n",
      "batch 1600: loss 0.145613\n",
      "batch 1601: loss 0.210679\n",
      "batch 1602: loss 0.096925\n",
      "batch 1603: loss 0.084562\n",
      "batch 1604: loss 0.197068\n",
      "batch 1605: loss 0.021823\n",
      "batch 1606: loss 0.018499\n",
      "batch 1607: loss 0.058040\n",
      "batch 1608: loss 0.303999\n",
      "batch 1609: loss 0.168193\n",
      "batch 1610: loss 0.205334\n",
      "batch 1611: loss 0.109124\n",
      "batch 1612: loss 0.244119\n",
      "batch 1613: loss 0.192435\n",
      "batch 1614: loss 0.080898\n",
      "batch 1615: loss 0.095583\n",
      "batch 1616: loss 0.129711\n",
      "batch 1617: loss 0.084663\n",
      "batch 1618: loss 0.225595\n",
      "batch 1619: loss 0.171548\n",
      "batch 1620: loss 0.147192\n",
      "batch 1621: loss 0.053617\n",
      "batch 1622: loss 0.146798\n",
      "batch 1623: loss 0.242663\n",
      "batch 1624: loss 0.131390\n",
      "batch 1625: loss 0.107277\n",
      "batch 1626: loss 0.250323\n",
      "batch 1627: loss 0.230026\n",
      "batch 1628: loss 0.097116\n",
      "batch 1629: loss 0.182039\n",
      "batch 1630: loss 0.111817\n",
      "batch 1631: loss 0.207704\n",
      "batch 1632: loss 0.034648\n",
      "batch 1633: loss 0.109407\n",
      "batch 1634: loss 0.145182\n",
      "batch 1635: loss 0.197435\n",
      "batch 1636: loss 0.117173\n",
      "batch 1637: loss 0.221824\n",
      "batch 1638: loss 0.391672\n",
      "batch 1639: loss 0.044825\n",
      "batch 1640: loss 0.118074\n",
      "batch 1641: loss 0.202652\n",
      "batch 1642: loss 0.093219\n",
      "batch 1643: loss 0.163047\n",
      "batch 1644: loss 0.196687\n",
      "batch 1645: loss 0.043430\n",
      "batch 1646: loss 0.082587\n",
      "batch 1647: loss 0.119556\n",
      "batch 1648: loss 0.139067\n",
      "batch 1649: loss 0.148865\n",
      "batch 1650: loss 0.342496\n",
      "batch 1651: loss 0.053016\n",
      "batch 1652: loss 0.139718\n",
      "batch 1653: loss 0.084163\n",
      "batch 1654: loss 0.115814\n",
      "batch 1655: loss 0.083765\n",
      "batch 1656: loss 0.244397\n",
      "batch 1657: loss 0.103933\n",
      "batch 1658: loss 0.092261\n",
      "batch 1659: loss 0.380247\n",
      "batch 1660: loss 0.173167\n",
      "batch 1661: loss 0.030009\n",
      "batch 1662: loss 0.035193\n",
      "batch 1663: loss 0.267727\n",
      "batch 1664: loss 0.163174\n",
      "batch 1665: loss 0.093787\n",
      "batch 1666: loss 0.040281\n",
      "batch 1667: loss 0.077275\n",
      "batch 1668: loss 0.159435\n",
      "batch 1669: loss 0.151136\n",
      "batch 1670: loss 0.140561\n",
      "batch 1671: loss 0.177619\n",
      "batch 1672: loss 0.159263\n",
      "batch 1673: loss 0.104959\n",
      "batch 1674: loss 0.085590\n",
      "batch 1675: loss 0.104435\n",
      "batch 1676: loss 0.062589\n",
      "batch 1677: loss 0.189859\n",
      "batch 1678: loss 0.125872\n",
      "batch 1679: loss 0.252312\n",
      "batch 1680: loss 0.083109\n",
      "batch 1681: loss 0.085428\n",
      "batch 1682: loss 0.102112\n",
      "batch 1683: loss 0.067097\n",
      "batch 1684: loss 0.080412\n",
      "batch 1685: loss 0.076207\n",
      "batch 1686: loss 0.235341\n",
      "batch 1687: loss 0.177311\n",
      "batch 1688: loss 0.100964\n",
      "batch 1689: loss 0.115291\n",
      "batch 1690: loss 0.131416\n",
      "batch 1691: loss 0.063437\n",
      "batch 1692: loss 0.061376\n",
      "batch 1693: loss 0.049342\n",
      "batch 1694: loss 0.129058\n",
      "batch 1695: loss 0.094527\n",
      "batch 1696: loss 0.120438\n",
      "batch 1697: loss 0.080121\n",
      "batch 1698: loss 0.213343\n",
      "batch 1699: loss 0.096818\n",
      "batch 1700: loss 0.162217\n",
      "batch 1701: loss 0.136428\n",
      "batch 1702: loss 0.051420\n",
      "batch 1703: loss 0.107438\n",
      "batch 1704: loss 0.055422\n",
      "batch 1705: loss 0.211689\n",
      "batch 1706: loss 0.074491\n",
      "batch 1707: loss 0.066276\n",
      "batch 1708: loss 0.097239\n",
      "batch 1709: loss 0.362036\n",
      "batch 1710: loss 0.058211\n",
      "batch 1711: loss 0.075801\n",
      "batch 1712: loss 0.074859\n",
      "batch 1713: loss 0.047832\n",
      "batch 1714: loss 0.162477\n",
      "batch 1715: loss 0.081752\n",
      "batch 1716: loss 0.044283\n",
      "batch 1717: loss 0.079143\n",
      "batch 1718: loss 0.116590\n",
      "batch 1719: loss 0.207383\n",
      "batch 1720: loss 0.095851\n",
      "batch 1721: loss 0.238605\n",
      "batch 1722: loss 0.226118\n",
      "batch 1723: loss 0.403267\n",
      "batch 1724: loss 0.068080\n",
      "batch 1725: loss 0.082663\n",
      "batch 1726: loss 0.075519\n",
      "batch 1727: loss 0.063995\n",
      "batch 1728: loss 0.133477\n",
      "batch 1729: loss 0.092797\n",
      "batch 1730: loss 0.155122\n",
      "batch 1731: loss 0.047918\n",
      "batch 1732: loss 0.196136\n",
      "batch 1733: loss 0.258786\n",
      "batch 1734: loss 0.213300\n",
      "batch 1735: loss 0.196411\n",
      "batch 1736: loss 0.182710\n",
      "batch 1737: loss 0.241272\n",
      "batch 1738: loss 0.093489\n",
      "batch 1739: loss 0.113983\n",
      "batch 1740: loss 0.212890\n",
      "batch 1741: loss 0.087688\n",
      "batch 1742: loss 0.067661\n",
      "batch 1743: loss 0.189383\n",
      "batch 1744: loss 0.034185\n",
      "batch 1745: loss 0.153956\n",
      "batch 1746: loss 0.047372\n",
      "batch 1747: loss 0.061607\n",
      "batch 1748: loss 0.117308\n",
      "batch 1749: loss 0.114274\n",
      "batch 1750: loss 0.056717\n",
      "batch 1751: loss 0.028257\n",
      "batch 1752: loss 0.035077\n",
      "batch 1753: loss 0.193034\n",
      "batch 1754: loss 0.187627\n",
      "batch 1755: loss 0.054277\n",
      "batch 1756: loss 0.169156\n",
      "batch 1757: loss 0.111985\n",
      "batch 1758: loss 0.149536\n",
      "batch 1759: loss 0.037777\n",
      "batch 1760: loss 0.028178\n",
      "batch 1761: loss 0.190907\n",
      "batch 1762: loss 0.038109\n",
      "batch 1763: loss 0.264366\n",
      "batch 1764: loss 0.063657\n",
      "batch 1765: loss 0.220361\n",
      "batch 1766: loss 0.056356\n",
      "batch 1767: loss 0.086445\n",
      "batch 1768: loss 0.219625\n",
      "batch 1769: loss 0.132660\n",
      "batch 1770: loss 0.139916\n",
      "batch 1771: loss 0.121900\n",
      "batch 1772: loss 0.066773\n",
      "batch 1773: loss 0.056708\n",
      "batch 1774: loss 0.088625\n",
      "batch 1775: loss 0.070946\n",
      "batch 1776: loss 0.066705\n",
      "batch 1777: loss 0.086598\n",
      "batch 1778: loss 0.226592\n",
      "batch 1779: loss 0.162988\n",
      "batch 1780: loss 0.098880\n",
      "batch 1781: loss 0.095047\n",
      "batch 1782: loss 0.188151\n",
      "batch 1783: loss 0.236578\n",
      "batch 1784: loss 0.159350\n",
      "batch 1785: loss 0.034847\n",
      "batch 1786: loss 0.054521\n",
      "batch 1787: loss 0.129167\n",
      "batch 1788: loss 0.063812\n",
      "batch 1789: loss 0.099562\n",
      "batch 1790: loss 0.162441\n",
      "batch 1791: loss 0.074839\n",
      "batch 1792: loss 0.203424\n",
      "batch 1793: loss 0.116423\n",
      "batch 1794: loss 0.161588\n",
      "batch 1795: loss 0.038906\n",
      "batch 1796: loss 0.109134\n",
      "batch 1797: loss 0.203059\n",
      "batch 1798: loss 0.156644\n",
      "batch 1799: loss 0.131743\n",
      "batch 1800: loss 0.116571\n",
      "batch 1801: loss 0.207314\n",
      "batch 1802: loss 0.098703\n",
      "batch 1803: loss 0.174523\n",
      "batch 1804: loss 0.046107\n",
      "batch 1805: loss 0.091248\n",
      "batch 1806: loss 0.039196\n",
      "batch 1807: loss 0.134444\n",
      "batch 1808: loss 0.052979\n",
      "batch 1809: loss 0.159025\n",
      "batch 1810: loss 0.083757\n",
      "batch 1811: loss 0.169733\n",
      "batch 1812: loss 0.046018\n",
      "batch 1813: loss 0.140033\n",
      "batch 1814: loss 0.106708\n",
      "batch 1815: loss 0.117682\n",
      "batch 1816: loss 0.321713\n",
      "batch 1817: loss 0.170465\n",
      "batch 1818: loss 0.066607\n",
      "batch 1819: loss 0.048801\n",
      "batch 1820: loss 0.079330\n",
      "batch 1821: loss 0.040294\n",
      "batch 1822: loss 0.045933\n",
      "batch 1823: loss 0.086500\n",
      "batch 1824: loss 0.096427\n",
      "batch 1825: loss 0.047945\n",
      "batch 1826: loss 0.098253\n",
      "batch 1827: loss 0.153703\n",
      "batch 1828: loss 0.068514\n",
      "batch 1829: loss 0.139135\n",
      "batch 1830: loss 0.245756\n",
      "batch 1831: loss 0.152198\n",
      "batch 1832: loss 0.205486\n",
      "batch 1833: loss 0.114487\n",
      "batch 1834: loss 0.094549\n",
      "batch 1835: loss 0.024503\n",
      "batch 1836: loss 0.060221\n",
      "batch 1837: loss 0.348039\n",
      "batch 1838: loss 0.093412\n",
      "batch 1839: loss 0.235957\n",
      "batch 1840: loss 0.083262\n",
      "batch 1841: loss 0.059756\n",
      "batch 1842: loss 0.283098\n",
      "batch 1843: loss 0.086972\n",
      "batch 1844: loss 0.068373\n",
      "batch 1845: loss 0.034460\n",
      "batch 1846: loss 0.233406\n",
      "batch 1847: loss 0.081451\n",
      "batch 1848: loss 0.223442\n",
      "batch 1849: loss 0.084816\n",
      "batch 1850: loss 0.060720\n",
      "batch 1851: loss 0.090409\n",
      "batch 1852: loss 0.108736\n",
      "batch 1853: loss 0.156532\n",
      "batch 1854: loss 0.102483\n",
      "batch 1855: loss 0.033704\n",
      "batch 1856: loss 0.086904\n",
      "batch 1857: loss 0.085091\n",
      "batch 1858: loss 0.070183\n",
      "batch 1859: loss 0.246330\n",
      "batch 1860: loss 0.095842\n",
      "batch 1861: loss 0.098819\n",
      "batch 1862: loss 0.207786\n",
      "batch 1863: loss 0.108723\n",
      "batch 1864: loss 0.114886\n",
      "batch 1865: loss 0.045183\n",
      "batch 1866: loss 0.037889\n",
      "batch 1867: loss 0.035169\n",
      "batch 1868: loss 0.062393\n",
      "batch 1869: loss 0.083058\n",
      "batch 1870: loss 0.180205\n",
      "batch 1871: loss 0.142348\n",
      "batch 1872: loss 0.231209\n",
      "batch 1873: loss 0.043940\n",
      "batch 1874: loss 0.165300\n",
      "batch 1875: loss 0.073821\n",
      "batch 1876: loss 0.100963\n",
      "batch 1877: loss 0.041911\n",
      "batch 1878: loss 0.104121\n",
      "batch 1879: loss 0.232418\n",
      "batch 1880: loss 0.054226\n",
      "batch 1881: loss 0.180795\n",
      "batch 1882: loss 0.121111\n",
      "batch 1883: loss 0.272725\n",
      "batch 1884: loss 0.061745\n",
      "batch 1885: loss 0.129315\n",
      "batch 1886: loss 0.108520\n",
      "batch 1887: loss 0.057556\n",
      "batch 1888: loss 0.134048\n",
      "batch 1889: loss 0.108352\n",
      "batch 1890: loss 0.062632\n",
      "batch 1891: loss 0.140373\n",
      "batch 1892: loss 0.063568\n",
      "batch 1893: loss 0.302639\n",
      "batch 1894: loss 0.085124\n",
      "batch 1895: loss 0.219338\n",
      "batch 1896: loss 0.141303\n",
      "batch 1897: loss 0.043334\n",
      "batch 1898: loss 0.118652\n",
      "batch 1899: loss 0.044623\n",
      "batch 1900: loss 0.155982\n",
      "batch 1901: loss 0.169646\n",
      "batch 1902: loss 0.081495\n",
      "batch 1903: loss 0.023354\n",
      "batch 1904: loss 0.089048\n",
      "batch 1905: loss 0.082012\n",
      "batch 1906: loss 0.141544\n",
      "batch 1907: loss 0.149468\n",
      "batch 1908: loss 0.070144\n",
      "batch 1909: loss 0.141977\n",
      "batch 1910: loss 0.072438\n",
      "batch 1911: loss 0.106569\n",
      "batch 1912: loss 0.014921\n",
      "batch 1913: loss 0.128634\n",
      "batch 1914: loss 0.142927\n",
      "batch 1915: loss 0.210433\n",
      "batch 1916: loss 0.234519\n",
      "batch 1917: loss 0.066995\n",
      "batch 1918: loss 0.152830\n",
      "batch 1919: loss 0.146470\n",
      "batch 1920: loss 0.074471\n",
      "batch 1921: loss 0.070976\n",
      "batch 1922: loss 0.076910\n",
      "batch 1923: loss 0.128291\n",
      "batch 1924: loss 0.113880\n",
      "batch 1925: loss 0.064779\n",
      "batch 1926: loss 0.117814\n",
      "batch 1927: loss 0.101090\n",
      "batch 1928: loss 0.103361\n",
      "batch 1929: loss 0.057276\n",
      "batch 1930: loss 0.049813\n",
      "batch 1931: loss 0.138718\n",
      "batch 1932: loss 0.030547\n",
      "batch 1933: loss 0.094750\n",
      "batch 1934: loss 0.117073\n",
      "batch 1935: loss 0.129640\n",
      "batch 1936: loss 0.157188\n",
      "batch 1937: loss 0.058174\n",
      "batch 1938: loss 0.060872\n",
      "batch 1939: loss 0.111471\n",
      "batch 1940: loss 0.191367\n",
      "batch 1941: loss 0.227159\n",
      "batch 1942: loss 0.220245\n",
      "batch 1943: loss 0.296393\n",
      "batch 1944: loss 0.100999\n",
      "batch 1945: loss 0.048733\n",
      "batch 1946: loss 0.061346\n",
      "batch 1947: loss 0.340459\n",
      "batch 1948: loss 0.235849\n",
      "batch 1949: loss 0.190895\n",
      "batch 1950: loss 0.063815\n",
      "batch 1951: loss 0.210979\n",
      "batch 1952: loss 0.130185\n",
      "batch 1953: loss 0.087309\n",
      "batch 1954: loss 0.035381\n",
      "batch 1955: loss 0.040936\n",
      "batch 1956: loss 0.119044\n",
      "batch 1957: loss 0.104421\n",
      "batch 1958: loss 0.294004\n",
      "batch 1959: loss 0.098908\n",
      "batch 1960: loss 0.083333\n",
      "batch 1961: loss 0.214531\n",
      "batch 1962: loss 0.398485\n",
      "batch 1963: loss 0.242319\n",
      "batch 1964: loss 0.195348\n",
      "batch 1965: loss 0.073231\n",
      "batch 1966: loss 0.368214\n",
      "batch 1967: loss 0.088639\n",
      "batch 1968: loss 0.193234\n",
      "batch 1969: loss 0.084968\n",
      "batch 1970: loss 0.190775\n",
      "batch 1971: loss 0.064083\n",
      "batch 1972: loss 0.089730\n",
      "batch 1973: loss 0.190916\n",
      "batch 1974: loss 0.247108\n",
      "batch 1975: loss 0.277657\n",
      "batch 1976: loss 0.039480\n",
      "batch 1977: loss 0.092384\n",
      "batch 1978: loss 0.087230\n",
      "batch 1979: loss 0.340581\n",
      "batch 1980: loss 0.104410\n",
      "batch 1981: loss 0.091557\n",
      "batch 1982: loss 0.217207\n",
      "batch 1983: loss 0.222775\n",
      "batch 1984: loss 0.344278\n",
      "batch 1985: loss 0.278740\n",
      "batch 1986: loss 0.118002\n",
      "batch 1987: loss 0.140894\n",
      "batch 1988: loss 0.197415\n",
      "batch 1989: loss 0.122932\n",
      "batch 1990: loss 0.061456\n",
      "batch 1991: loss 0.288760\n",
      "batch 1992: loss 0.047046\n",
      "batch 1993: loss 0.243694\n",
      "batch 1994: loss 0.102603\n",
      "batch 1995: loss 0.093216\n",
      "batch 1996: loss 0.198309\n",
      "batch 1997: loss 0.288670\n",
      "batch 1998: loss 0.080598\n",
      "batch 1999: loss 0.080314\n",
      "batch 2000: loss 0.066881\n",
      "batch 2001: loss 0.048541\n",
      "batch 2002: loss 0.198837\n",
      "batch 2003: loss 0.099000\n",
      "batch 2004: loss 0.096440\n",
      "batch 2005: loss 0.367310\n",
      "batch 2006: loss 0.118443\n",
      "batch 2007: loss 0.124029\n",
      "batch 2008: loss 0.060706\n",
      "batch 2009: loss 0.043197\n",
      "batch 2010: loss 0.133919\n",
      "batch 2011: loss 0.164295\n",
      "batch 2012: loss 0.125205\n",
      "batch 2013: loss 0.180661\n",
      "batch 2014: loss 0.123630\n",
      "batch 2015: loss 0.181988\n",
      "batch 2016: loss 0.125771\n",
      "batch 2017: loss 0.196219\n",
      "batch 2018: loss 0.063428\n",
      "batch 2019: loss 0.048919\n",
      "batch 2020: loss 0.246213\n",
      "batch 2021: loss 0.203878\n",
      "batch 2022: loss 0.056428\n",
      "batch 2023: loss 0.104128\n",
      "batch 2024: loss 0.112778\n",
      "batch 2025: loss 0.080395\n",
      "batch 2026: loss 0.108831\n",
      "batch 2027: loss 0.092106\n",
      "batch 2028: loss 0.052248\n",
      "batch 2029: loss 0.059930\n",
      "batch 2030: loss 0.083923\n",
      "batch 2031: loss 0.139556\n",
      "batch 2032: loss 0.110455\n",
      "batch 2033: loss 0.111295\n",
      "batch 2034: loss 0.057778\n",
      "batch 2035: loss 0.332246\n",
      "batch 2036: loss 0.096682\n",
      "batch 2037: loss 0.118776\n",
      "batch 2038: loss 0.026125\n",
      "batch 2039: loss 0.078803\n",
      "batch 2040: loss 0.246842\n",
      "batch 2041: loss 0.134316\n",
      "batch 2042: loss 0.104729\n",
      "batch 2043: loss 0.103778\n",
      "batch 2044: loss 0.211885\n",
      "batch 2045: loss 0.159396\n",
      "batch 2046: loss 0.100568\n",
      "batch 2047: loss 0.201320\n",
      "batch 2048: loss 0.061405\n",
      "batch 2049: loss 0.071595\n",
      "batch 2050: loss 0.127022\n",
      "batch 2051: loss 0.077670\n",
      "batch 2052: loss 0.194204\n",
      "batch 2053: loss 0.214234\n",
      "batch 2054: loss 0.177248\n",
      "batch 2055: loss 0.078227\n",
      "batch 2056: loss 0.041978\n",
      "batch 2057: loss 0.081580\n",
      "batch 2058: loss 0.084125\n",
      "batch 2059: loss 0.088505\n",
      "batch 2060: loss 0.394712\n",
      "batch 2061: loss 0.048302\n",
      "batch 2062: loss 0.132525\n",
      "batch 2063: loss 0.152186\n",
      "batch 2064: loss 0.050376\n",
      "batch 2065: loss 0.134363\n",
      "batch 2066: loss 0.127097\n",
      "batch 2067: loss 0.099716\n",
      "batch 2068: loss 0.030566\n",
      "batch 2069: loss 0.158100\n",
      "batch 2070: loss 0.135460\n",
      "batch 2071: loss 0.186541\n",
      "batch 2072: loss 0.161536\n",
      "batch 2073: loss 0.221939\n",
      "batch 2074: loss 0.184812\n",
      "batch 2075: loss 0.130018\n",
      "batch 2076: loss 0.093275\n",
      "batch 2077: loss 0.176583\n",
      "batch 2078: loss 0.118063\n",
      "batch 2079: loss 0.073370\n",
      "batch 2080: loss 0.300207\n",
      "batch 2081: loss 0.094773\n",
      "batch 2082: loss 0.118111\n",
      "batch 2083: loss 0.170803\n",
      "batch 2084: loss 0.023255\n",
      "batch 2085: loss 0.121550\n",
      "batch 2086: loss 0.034955\n",
      "batch 2087: loss 0.246304\n",
      "batch 2088: loss 0.043956\n",
      "batch 2089: loss 0.049913\n",
      "batch 2090: loss 0.127444\n",
      "batch 2091: loss 0.064256\n",
      "batch 2092: loss 0.097295\n",
      "batch 2093: loss 0.081893\n",
      "batch 2094: loss 0.048171\n",
      "batch 2095: loss 0.093224\n",
      "batch 2096: loss 0.116514\n",
      "batch 2097: loss 0.037093\n",
      "batch 2098: loss 0.042082\n",
      "batch 2099: loss 0.048686\n",
      "batch 2100: loss 0.120421\n",
      "batch 2101: loss 0.089347\n",
      "batch 2102: loss 0.143578\n",
      "batch 2103: loss 0.139880\n",
      "batch 2104: loss 0.139621\n",
      "batch 2105: loss 0.194549\n",
      "batch 2106: loss 0.082593\n",
      "batch 2107: loss 0.127265\n",
      "batch 2108: loss 0.131962\n",
      "batch 2109: loss 0.054245\n",
      "batch 2110: loss 0.097047\n",
      "batch 2111: loss 0.057087\n",
      "batch 2112: loss 0.094475\n",
      "batch 2113: loss 0.117052\n",
      "batch 2114: loss 0.068602\n",
      "batch 2115: loss 0.076904\n",
      "batch 2116: loss 0.200529\n",
      "batch 2117: loss 0.044450\n",
      "batch 2118: loss 0.061857\n",
      "batch 2119: loss 0.083966\n",
      "batch 2120: loss 0.111994\n",
      "batch 2121: loss 0.044780\n",
      "batch 2122: loss 0.036662\n",
      "batch 2123: loss 0.057914\n",
      "batch 2124: loss 0.035534\n",
      "batch 2125: loss 0.236914\n",
      "batch 2126: loss 0.173042\n",
      "batch 2127: loss 0.068699\n",
      "batch 2128: loss 0.253376\n",
      "batch 2129: loss 0.072565\n",
      "batch 2130: loss 0.021433\n",
      "batch 2131: loss 0.094950\n",
      "batch 2132: loss 0.080356\n",
      "batch 2133: loss 0.128854\n",
      "batch 2134: loss 0.073272\n",
      "batch 2135: loss 0.130005\n",
      "batch 2136: loss 0.141453\n",
      "batch 2137: loss 0.109277\n",
      "batch 2138: loss 0.274442\n",
      "batch 2139: loss 0.114673\n",
      "batch 2140: loss 0.152009\n",
      "batch 2141: loss 0.061448\n",
      "batch 2142: loss 0.099308\n",
      "batch 2143: loss 0.061632\n",
      "batch 2144: loss 0.179233\n",
      "batch 2145: loss 0.075288\n",
      "batch 2146: loss 0.122762\n",
      "batch 2147: loss 0.139036\n",
      "batch 2148: loss 0.101450\n",
      "batch 2149: loss 0.155229\n",
      "batch 2150: loss 0.280878\n",
      "batch 2151: loss 0.056309\n",
      "batch 2152: loss 0.053621\n",
      "batch 2153: loss 0.068437\n",
      "batch 2154: loss 0.047904\n",
      "batch 2155: loss 0.143220\n",
      "batch 2156: loss 0.023298\n",
      "batch 2157: loss 0.061003\n",
      "batch 2158: loss 0.067927\n",
      "batch 2159: loss 0.180568\n",
      "batch 2160: loss 0.131404\n",
      "batch 2161: loss 0.217948\n",
      "batch 2162: loss 0.073290\n",
      "batch 2163: loss 0.053121\n",
      "batch 2164: loss 0.089942\n",
      "batch 2165: loss 0.094879\n",
      "batch 2166: loss 0.127517\n",
      "batch 2167: loss 0.113999\n",
      "batch 2168: loss 0.076352\n",
      "batch 2169: loss 0.036298\n",
      "batch 2170: loss 0.082897\n",
      "batch 2171: loss 0.195540\n",
      "batch 2172: loss 0.180288\n",
      "batch 2173: loss 0.036156\n",
      "batch 2174: loss 0.189961\n",
      "batch 2175: loss 0.060840\n",
      "batch 2176: loss 0.157109\n",
      "batch 2177: loss 0.037704\n",
      "batch 2178: loss 0.184406\n",
      "batch 2179: loss 0.195565\n",
      "batch 2180: loss 0.047945\n",
      "batch 2181: loss 0.032054\n",
      "batch 2182: loss 0.092154\n",
      "batch 2183: loss 0.375367\n",
      "batch 2184: loss 0.100792\n",
      "batch 2185: loss 0.165148\n",
      "batch 2186: loss 0.123544\n",
      "batch 2187: loss 0.063118\n",
      "batch 2188: loss 0.186767\n",
      "batch 2189: loss 0.024654\n",
      "batch 2190: loss 0.133743\n",
      "batch 2191: loss 0.226455\n",
      "batch 2192: loss 0.056520\n",
      "batch 2193: loss 0.130845\n",
      "batch 2194: loss 0.081159\n",
      "batch 2195: loss 0.140201\n",
      "batch 2196: loss 0.067913\n",
      "batch 2197: loss 0.081194\n",
      "batch 2198: loss 0.051149\n",
      "batch 2199: loss 0.196666\n",
      "batch 2200: loss 0.103964\n",
      "batch 2201: loss 0.204503\n",
      "batch 2202: loss 0.346235\n",
      "batch 2203: loss 0.120927\n",
      "batch 2204: loss 0.096769\n",
      "batch 2205: loss 0.043690\n",
      "batch 2206: loss 0.128983\n",
      "batch 2207: loss 0.168282\n",
      "batch 2208: loss 0.125739\n",
      "batch 2209: loss 0.158398\n",
      "batch 2210: loss 0.238803\n",
      "batch 2211: loss 0.345198\n",
      "batch 2212: loss 0.089808\n",
      "batch 2213: loss 0.205639\n",
      "batch 2214: loss 0.033912\n",
      "batch 2215: loss 0.178833\n",
      "batch 2216: loss 0.090903\n",
      "batch 2217: loss 0.452258\n",
      "batch 2218: loss 0.185728\n",
      "batch 2219: loss 0.118061\n",
      "batch 2220: loss 0.087414\n",
      "batch 2221: loss 0.077650\n",
      "batch 2222: loss 0.157471\n",
      "batch 2223: loss 0.086244\n",
      "batch 2224: loss 0.163055\n",
      "batch 2225: loss 0.126539\n",
      "batch 2226: loss 0.164852\n",
      "batch 2227: loss 0.058491\n",
      "batch 2228: loss 0.109080\n",
      "batch 2229: loss 0.036125\n",
      "batch 2230: loss 0.094288\n",
      "batch 2231: loss 0.060530\n",
      "batch 2232: loss 0.162427\n",
      "batch 2233: loss 0.046298\n",
      "batch 2234: loss 0.087775\n",
      "batch 2235: loss 0.190999\n",
      "batch 2236: loss 0.066336\n",
      "batch 2237: loss 0.130964\n",
      "batch 2238: loss 0.169586\n",
      "batch 2239: loss 0.033988\n",
      "batch 2240: loss 0.129657\n",
      "batch 2241: loss 0.023475\n",
      "batch 2242: loss 0.050692\n",
      "batch 2243: loss 0.079043\n",
      "batch 2244: loss 0.051164\n",
      "batch 2245: loss 0.078257\n",
      "batch 2246: loss 0.212734\n",
      "batch 2247: loss 0.206819\n",
      "batch 2248: loss 0.069856\n",
      "batch 2249: loss 0.053947\n",
      "batch 2250: loss 0.128737\n",
      "batch 2251: loss 0.202134\n",
      "batch 2252: loss 0.063382\n",
      "batch 2253: loss 0.104457\n",
      "batch 2254: loss 0.166568\n",
      "batch 2255: loss 0.057296\n",
      "batch 2256: loss 0.082700\n",
      "batch 2257: loss 0.069759\n",
      "batch 2258: loss 0.117785\n",
      "batch 2259: loss 0.169151\n",
      "batch 2260: loss 0.063746\n",
      "batch 2261: loss 0.105883\n",
      "batch 2262: loss 0.106507\n",
      "batch 2263: loss 0.079016\n",
      "batch 2264: loss 0.069141\n",
      "batch 2265: loss 0.281535\n",
      "batch 2266: loss 0.089185\n",
      "batch 2267: loss 0.115779\n",
      "batch 2268: loss 0.280673\n",
      "batch 2269: loss 0.099858\n",
      "batch 2270: loss 0.080530\n",
      "batch 2271: loss 0.176179\n",
      "batch 2272: loss 0.116644\n",
      "batch 2273: loss 0.071896\n",
      "batch 2274: loss 0.083543\n",
      "batch 2275: loss 0.091615\n",
      "batch 2276: loss 0.058923\n",
      "batch 2277: loss 0.246124\n",
      "batch 2278: loss 0.175371\n",
      "batch 2279: loss 0.080369\n",
      "batch 2280: loss 0.110433\n",
      "batch 2281: loss 0.088490\n",
      "batch 2282: loss 0.188750\n",
      "batch 2283: loss 0.159557\n",
      "batch 2284: loss 0.094451\n",
      "batch 2285: loss 0.072990\n",
      "batch 2286: loss 0.143113\n",
      "batch 2287: loss 0.037346\n",
      "batch 2288: loss 0.239615\n",
      "batch 2289: loss 0.237184\n",
      "batch 2290: loss 0.247408\n",
      "batch 2291: loss 0.117438\n",
      "batch 2292: loss 0.113465\n",
      "batch 2293: loss 0.055844\n",
      "batch 2294: loss 0.096358\n",
      "batch 2295: loss 0.040177\n",
      "batch 2296: loss 0.267412\n",
      "batch 2297: loss 0.268780\n",
      "batch 2298: loss 0.058725\n",
      "batch 2299: loss 0.161850\n",
      "batch 2300: loss 0.074556\n",
      "batch 2301: loss 0.173133\n",
      "batch 2302: loss 0.108958\n",
      "batch 2303: loss 0.067003\n",
      "batch 2304: loss 0.201210\n",
      "batch 2305: loss 0.111421\n",
      "batch 2306: loss 0.042961\n",
      "batch 2307: loss 0.206329\n",
      "batch 2308: loss 0.064663\n",
      "batch 2309: loss 0.076414\n",
      "batch 2310: loss 0.213192\n",
      "batch 2311: loss 0.115479\n",
      "batch 2312: loss 0.061002\n",
      "batch 2313: loss 0.111684\n",
      "batch 2314: loss 0.089825\n",
      "batch 2315: loss 0.091785\n",
      "batch 2316: loss 0.066080\n",
      "batch 2317: loss 0.185773\n",
      "batch 2318: loss 0.242014\n",
      "batch 2319: loss 0.124990\n",
      "batch 2320: loss 0.155673\n",
      "batch 2321: loss 0.090141\n",
      "batch 2322: loss 0.015924\n",
      "batch 2323: loss 0.051859\n",
      "batch 2324: loss 0.081460\n",
      "batch 2325: loss 0.048516\n",
      "batch 2326: loss 0.158212\n",
      "batch 2327: loss 0.087367\n",
      "batch 2328: loss 0.123507\n",
      "batch 2329: loss 0.159142\n",
      "batch 2330: loss 0.062791\n",
      "batch 2331: loss 0.054840\n",
      "batch 2332: loss 0.078021\n",
      "batch 2333: loss 0.110691\n",
      "batch 2334: loss 0.172285\n",
      "batch 2335: loss 0.055445\n",
      "batch 2336: loss 0.076788\n",
      "batch 2337: loss 0.040809\n",
      "batch 2338: loss 0.026136\n",
      "batch 2339: loss 0.047353\n",
      "batch 2340: loss 0.163864\n",
      "batch 2341: loss 0.043279\n",
      "batch 2342: loss 0.073827\n",
      "batch 2343: loss 0.098316\n",
      "batch 2344: loss 0.107119\n",
      "batch 2345: loss 0.092286\n",
      "batch 2346: loss 0.270963\n",
      "batch 2347: loss 0.079075\n",
      "batch 2348: loss 0.019025\n",
      "batch 2349: loss 0.092365\n",
      "batch 2350: loss 0.108869\n",
      "batch 2351: loss 0.143148\n",
      "batch 2352: loss 0.165010\n",
      "batch 2353: loss 0.084619\n",
      "batch 2354: loss 0.064112\n",
      "batch 2355: loss 0.085038\n",
      "batch 2356: loss 0.110492\n",
      "batch 2357: loss 0.078644\n",
      "batch 2358: loss 0.125547\n",
      "batch 2359: loss 0.137556\n",
      "batch 2360: loss 0.050980\n",
      "batch 2361: loss 0.037913\n",
      "batch 2362: loss 0.161421\n",
      "batch 2363: loss 0.047815\n",
      "batch 2364: loss 0.165054\n",
      "batch 2365: loss 0.121041\n",
      "batch 2366: loss 0.116975\n",
      "batch 2367: loss 0.025078\n",
      "batch 2368: loss 0.212804\n",
      "batch 2369: loss 0.198732\n",
      "batch 2370: loss 0.063806\n",
      "batch 2371: loss 0.029578\n",
      "batch 2372: loss 0.142013\n",
      "batch 2373: loss 0.064068\n",
      "batch 2374: loss 0.067156\n",
      "batch 2375: loss 0.093695\n",
      "batch 2376: loss 0.221888\n",
      "batch 2377: loss 0.017636\n",
      "batch 2378: loss 0.073987\n",
      "batch 2379: loss 0.099155\n",
      "batch 2380: loss 0.097025\n",
      "batch 2381: loss 0.072044\n",
      "batch 2382: loss 0.046761\n",
      "batch 2383: loss 0.062376\n",
      "batch 2384: loss 0.146130\n",
      "batch 2385: loss 0.073831\n",
      "batch 2386: loss 0.026567\n",
      "batch 2387: loss 0.053395\n",
      "batch 2388: loss 0.145041\n",
      "batch 2389: loss 0.040418\n",
      "batch 2390: loss 0.071023\n",
      "batch 2391: loss 0.026929\n",
      "batch 2392: loss 0.035629\n",
      "batch 2393: loss 0.150949\n",
      "batch 2394: loss 0.074423\n",
      "batch 2395: loss 0.070495\n",
      "batch 2396: loss 0.082793\n",
      "batch 2397: loss 0.058754\n",
      "batch 2398: loss 0.101186\n",
      "batch 2399: loss 0.112155\n",
      "batch 2400: loss 0.035251\n",
      "batch 2401: loss 0.026270\n",
      "batch 2402: loss 0.034426\n",
      "batch 2403: loss 0.181216\n",
      "batch 2404: loss 0.094846\n",
      "batch 2405: loss 0.015644\n",
      "batch 2406: loss 0.035400\n",
      "batch 2407: loss 0.045226\n",
      "batch 2408: loss 0.379647\n",
      "batch 2409: loss 0.084071\n",
      "batch 2410: loss 0.195733\n",
      "batch 2411: loss 0.133809\n",
      "batch 2412: loss 0.023634\n",
      "batch 2413: loss 0.211176\n",
      "batch 2414: loss 0.233007\n",
      "batch 2415: loss 0.197228\n",
      "batch 2416: loss 0.056752\n",
      "batch 2417: loss 0.031861\n",
      "batch 2418: loss 0.152525\n",
      "batch 2419: loss 0.106437\n",
      "batch 2420: loss 0.086092\n",
      "batch 2421: loss 0.137034\n",
      "batch 2422: loss 0.096643\n",
      "batch 2423: loss 0.041923\n",
      "batch 2424: loss 0.088343\n",
      "batch 2425: loss 0.122102\n",
      "batch 2426: loss 0.172736\n",
      "batch 2427: loss 0.047436\n",
      "batch 2428: loss 0.310032\n",
      "batch 2429: loss 0.029743\n",
      "batch 2430: loss 0.125652\n",
      "batch 2431: loss 0.101138\n",
      "batch 2432: loss 0.047836\n",
      "batch 2433: loss 0.082650\n",
      "batch 2434: loss 0.041184\n",
      "batch 2435: loss 0.061467\n",
      "batch 2436: loss 0.177547\n",
      "batch 2437: loss 0.270700\n",
      "batch 2438: loss 0.139849\n",
      "batch 2439: loss 0.102057\n",
      "batch 2440: loss 0.078423\n",
      "batch 2441: loss 0.177460\n",
      "batch 2442: loss 0.124725\n",
      "batch 2443: loss 0.151970\n",
      "batch 2444: loss 0.047717\n",
      "batch 2445: loss 0.137786\n",
      "batch 2446: loss 0.240301\n",
      "batch 2447: loss 0.030629\n",
      "batch 2448: loss 0.272138\n",
      "batch 2449: loss 0.065241\n",
      "batch 2450: loss 0.063972\n",
      "batch 2451: loss 0.035377\n",
      "batch 2452: loss 0.039978\n",
      "batch 2453: loss 0.020065\n",
      "batch 2454: loss 0.154382\n",
      "batch 2455: loss 0.015275\n",
      "batch 2456: loss 0.064815\n",
      "batch 2457: loss 0.037752\n",
      "batch 2458: loss 0.077572\n",
      "batch 2459: loss 0.072934\n",
      "batch 2460: loss 0.061476\n",
      "batch 2461: loss 0.119553\n",
      "batch 2462: loss 0.157063\n",
      "batch 2463: loss 0.156708\n",
      "batch 2464: loss 0.026938\n",
      "batch 2465: loss 0.165331\n",
      "batch 2466: loss 0.044650\n",
      "batch 2467: loss 0.123508\n",
      "batch 2468: loss 0.174823\n",
      "batch 2469: loss 0.188843\n",
      "batch 2470: loss 0.040620\n",
      "batch 2471: loss 0.142123\n",
      "batch 2472: loss 0.046193\n",
      "batch 2473: loss 0.078838\n",
      "batch 2474: loss 0.235831\n",
      "batch 2475: loss 0.055272\n",
      "batch 2476: loss 0.042827\n",
      "batch 2477: loss 0.035533\n",
      "batch 2478: loss 0.200349\n",
      "batch 2479: loss 0.030563\n",
      "batch 2480: loss 0.032392\n",
      "batch 2481: loss 0.031747\n",
      "batch 2482: loss 0.151100\n",
      "batch 2483: loss 0.033981\n",
      "batch 2484: loss 0.148553\n",
      "batch 2485: loss 0.151321\n",
      "batch 2486: loss 0.022875\n",
      "batch 2487: loss 0.161174\n",
      "batch 2488: loss 0.057128\n",
      "batch 2489: loss 0.144147\n",
      "batch 2490: loss 0.032228\n",
      "batch 2491: loss 0.141102\n",
      "batch 2492: loss 0.221710\n",
      "batch 2493: loss 0.029701\n",
      "batch 2494: loss 0.155008\n",
      "batch 2495: loss 0.028648\n",
      "batch 2496: loss 0.018030\n",
      "batch 2497: loss 0.033223\n",
      "batch 2498: loss 0.046776\n",
      "batch 2499: loss 0.086412\n",
      "batch 2500: loss 0.080401\n",
      "batch 2501: loss 0.123042\n",
      "batch 2502: loss 0.161384\n",
      "batch 2503: loss 0.088769\n",
      "batch 2504: loss 0.181011\n",
      "batch 2505: loss 0.264993\n",
      "batch 2506: loss 0.075869\n",
      "batch 2507: loss 0.132767\n",
      "batch 2508: loss 0.105638\n",
      "batch 2509: loss 0.024956\n",
      "batch 2510: loss 0.100890\n",
      "batch 2511: loss 0.040137\n",
      "batch 2512: loss 0.175164\n",
      "batch 2513: loss 0.143084\n",
      "batch 2514: loss 0.023075\n",
      "batch 2515: loss 0.067354\n",
      "batch 2516: loss 0.040696\n",
      "batch 2517: loss 0.143083\n",
      "batch 2518: loss 0.195090\n",
      "batch 2519: loss 0.177046\n",
      "batch 2520: loss 0.050389\n",
      "batch 2521: loss 0.040510\n",
      "batch 2522: loss 0.106375\n",
      "batch 2523: loss 0.072735\n",
      "batch 2524: loss 0.048721\n",
      "batch 2525: loss 0.056092\n",
      "batch 2526: loss 0.121908\n",
      "batch 2527: loss 0.153013\n",
      "batch 2528: loss 0.043416\n",
      "batch 2529: loss 0.115016\n",
      "batch 2530: loss 0.034243\n",
      "batch 2531: loss 0.082986\n",
      "batch 2532: loss 0.124150\n",
      "batch 2533: loss 0.115051\n",
      "batch 2534: loss 0.071036\n",
      "batch 2535: loss 0.108203\n",
      "batch 2536: loss 0.044586\n",
      "batch 2537: loss 0.115439\n",
      "batch 2538: loss 0.101267\n",
      "batch 2539: loss 0.079434\n",
      "batch 2540: loss 0.091229\n",
      "batch 2541: loss 0.042593\n",
      "batch 2542: loss 0.098966\n",
      "batch 2543: loss 0.078517\n",
      "batch 2544: loss 0.221113\n",
      "batch 2545: loss 0.035939\n",
      "batch 2546: loss 0.080540\n",
      "batch 2547: loss 0.099735\n",
      "batch 2548: loss 0.146761\n",
      "batch 2549: loss 0.058944\n",
      "batch 2550: loss 0.141737\n",
      "batch 2551: loss 0.133497\n",
      "batch 2552: loss 0.254289\n",
      "batch 2553: loss 0.056674\n",
      "batch 2554: loss 0.126159\n",
      "batch 2555: loss 0.117621\n",
      "batch 2556: loss 0.046361\n",
      "batch 2557: loss 0.182586\n",
      "batch 2558: loss 0.125078\n",
      "batch 2559: loss 0.136355\n",
      "batch 2560: loss 0.069810\n",
      "batch 2561: loss 0.032368\n",
      "batch 2562: loss 0.145994\n",
      "batch 2563: loss 0.055100\n",
      "batch 2564: loss 0.078426\n",
      "batch 2565: loss 0.143413\n",
      "batch 2566: loss 0.076530\n",
      "batch 2567: loss 0.054676\n",
      "batch 2568: loss 0.080381\n",
      "batch 2569: loss 0.122355\n",
      "batch 2570: loss 0.154579\n",
      "batch 2571: loss 0.055415\n",
      "batch 2572: loss 0.070268\n",
      "batch 2573: loss 0.053695\n",
      "batch 2574: loss 0.054059\n",
      "batch 2575: loss 0.062645\n",
      "batch 2576: loss 0.077217\n",
      "batch 2577: loss 0.052327\n",
      "batch 2578: loss 0.071832\n",
      "batch 2579: loss 0.096879\n",
      "batch 2580: loss 0.090731\n",
      "batch 2581: loss 0.048145\n",
      "batch 2582: loss 0.113203\n",
      "batch 2583: loss 0.075535\n",
      "batch 2584: loss 0.086192\n",
      "batch 2585: loss 0.079607\n",
      "batch 2586: loss 0.392364\n",
      "batch 2587: loss 0.228971\n",
      "batch 2588: loss 0.138325\n",
      "batch 2589: loss 0.057788\n",
      "batch 2590: loss 0.079062\n",
      "batch 2591: loss 0.110100\n",
      "batch 2592: loss 0.117493\n",
      "batch 2593: loss 0.045846\n",
      "batch 2594: loss 0.120592\n",
      "batch 2595: loss 0.025388\n",
      "batch 2596: loss 0.154875\n",
      "batch 2597: loss 0.036366\n",
      "batch 2598: loss 0.039648\n",
      "batch 2599: loss 0.056764\n",
      "batch 2600: loss 0.194268\n",
      "batch 2601: loss 0.048272\n",
      "batch 2602: loss 0.064356\n",
      "batch 2603: loss 0.098182\n",
      "batch 2604: loss 0.042065\n",
      "batch 2605: loss 0.038253\n",
      "batch 2606: loss 0.087246\n",
      "batch 2607: loss 0.083379\n",
      "batch 2608: loss 0.128012\n",
      "batch 2609: loss 0.043626\n",
      "batch 2610: loss 0.033973\n",
      "batch 2611: loss 0.051908\n",
      "batch 2612: loss 0.181141\n",
      "batch 2613: loss 0.055461\n",
      "batch 2614: loss 0.046297\n",
      "batch 2615: loss 0.025071\n",
      "batch 2616: loss 0.191147\n",
      "batch 2617: loss 0.149008\n",
      "batch 2618: loss 0.047090\n",
      "batch 2619: loss 0.111332\n",
      "batch 2620: loss 0.043286\n",
      "batch 2621: loss 0.247788\n",
      "batch 2622: loss 0.073528\n",
      "batch 2623: loss 0.137303\n",
      "batch 2624: loss 0.118191\n",
      "batch 2625: loss 0.071712\n",
      "batch 2626: loss 0.173696\n",
      "batch 2627: loss 0.148977\n",
      "batch 2628: loss 0.060225\n",
      "batch 2629: loss 0.078806\n",
      "batch 2630: loss 0.047043\n",
      "batch 2631: loss 0.026511\n",
      "batch 2632: loss 0.026739\n",
      "batch 2633: loss 0.038324\n",
      "batch 2634: loss 0.175044\n",
      "batch 2635: loss 0.140352\n",
      "batch 2636: loss 0.104892\n",
      "batch 2637: loss 0.031280\n",
      "batch 2638: loss 0.159350\n",
      "batch 2639: loss 0.073915\n",
      "batch 2640: loss 0.030548\n",
      "batch 2641: loss 0.151096\n",
      "batch 2642: loss 0.235769\n",
      "batch 2643: loss 0.094340\n",
      "batch 2644: loss 0.053247\n",
      "batch 2645: loss 0.061692\n",
      "batch 2646: loss 0.078676\n",
      "batch 2647: loss 0.099208\n",
      "batch 2648: loss 0.204744\n",
      "batch 2649: loss 0.130382\n",
      "batch 2650: loss 0.165918\n",
      "batch 2651: loss 0.116534\n",
      "batch 2652: loss 0.076953\n",
      "batch 2653: loss 0.047166\n",
      "batch 2654: loss 0.187931\n",
      "batch 2655: loss 0.060524\n",
      "batch 2656: loss 0.197908\n",
      "batch 2657: loss 0.052878\n",
      "batch 2658: loss 0.113733\n",
      "batch 2659: loss 0.021222\n",
      "batch 2660: loss 0.226318\n",
      "batch 2661: loss 0.070183\n",
      "batch 2662: loss 0.071629\n",
      "batch 2663: loss 0.101698\n",
      "batch 2664: loss 0.094712\n",
      "batch 2665: loss 0.149549\n",
      "batch 2666: loss 0.039006\n",
      "batch 2667: loss 0.030890\n",
      "batch 2668: loss 0.195061\n",
      "batch 2669: loss 0.204145\n",
      "batch 2670: loss 0.108837\n",
      "batch 2671: loss 0.082926\n",
      "batch 2672: loss 0.103382\n",
      "batch 2673: loss 0.119943\n",
      "batch 2674: loss 0.081315\n",
      "batch 2675: loss 0.083802\n",
      "batch 2676: loss 0.119183\n",
      "batch 2677: loss 0.240882\n",
      "batch 2678: loss 0.091307\n",
      "batch 2679: loss 0.062796\n",
      "batch 2680: loss 0.417620\n",
      "batch 2681: loss 0.224829\n",
      "batch 2682: loss 0.048669\n",
      "batch 2683: loss 0.147240\n",
      "batch 2684: loss 0.027872\n",
      "batch 2685: loss 0.060319\n",
      "batch 2686: loss 0.071342\n",
      "batch 2687: loss 0.109315\n",
      "batch 2688: loss 0.058627\n",
      "batch 2689: loss 0.059339\n",
      "batch 2690: loss 0.071378\n",
      "batch 2691: loss 0.109290\n",
      "batch 2692: loss 0.030779\n",
      "batch 2693: loss 0.054657\n",
      "batch 2694: loss 0.038709\n",
      "batch 2695: loss 0.049071\n",
      "batch 2696: loss 0.041559\n",
      "batch 2697: loss 0.060830\n",
      "batch 2698: loss 0.103664\n",
      "batch 2699: loss 0.103320\n",
      "batch 2700: loss 0.205798\n",
      "batch 2701: loss 0.098106\n",
      "batch 2702: loss 0.056967\n",
      "batch 2703: loss 0.158879\n",
      "batch 2704: loss 0.020247\n",
      "batch 2705: loss 0.121947\n",
      "batch 2706: loss 0.130208\n",
      "batch 2707: loss 0.011353\n",
      "batch 2708: loss 0.150747\n",
      "batch 2709: loss 0.035503\n",
      "batch 2710: loss 0.122524\n",
      "batch 2711: loss 0.024061\n",
      "batch 2712: loss 0.136662\n",
      "batch 2713: loss 0.272415\n",
      "batch 2714: loss 0.051110\n",
      "batch 2715: loss 0.091783\n",
      "batch 2716: loss 0.033442\n",
      "batch 2717: loss 0.111747\n",
      "batch 2718: loss 0.213440\n",
      "batch 2719: loss 0.040331\n",
      "batch 2720: loss 0.115423\n",
      "batch 2721: loss 0.035001\n",
      "batch 2722: loss 0.062305\n",
      "batch 2723: loss 0.070760\n",
      "batch 2724: loss 0.066879\n",
      "batch 2725: loss 0.131816\n",
      "batch 2726: loss 0.180618\n",
      "batch 2727: loss 0.212391\n",
      "batch 2728: loss 0.068836\n",
      "batch 2729: loss 0.055759\n",
      "batch 2730: loss 0.087937\n",
      "batch 2731: loss 0.036439\n",
      "batch 2732: loss 0.023218\n",
      "batch 2733: loss 0.020413\n",
      "batch 2734: loss 0.122163\n",
      "batch 2735: loss 0.213073\n",
      "batch 2736: loss 0.044961\n",
      "batch 2737: loss 0.167840\n",
      "batch 2738: loss 0.077714\n",
      "batch 2739: loss 0.044333\n",
      "batch 2740: loss 0.156682\n",
      "batch 2741: loss 0.009024\n",
      "batch 2742: loss 0.015563\n",
      "batch 2743: loss 0.055641\n",
      "batch 2744: loss 0.115204\n",
      "batch 2745: loss 0.032587\n",
      "batch 2746: loss 0.119688\n",
      "batch 2747: loss 0.116428\n",
      "batch 2748: loss 0.061556\n",
      "batch 2749: loss 0.074587\n",
      "batch 2750: loss 0.088867\n",
      "batch 2751: loss 0.182895\n",
      "batch 2752: loss 0.078456\n",
      "batch 2753: loss 0.058451\n",
      "batch 2754: loss 0.160523\n",
      "batch 2755: loss 0.177648\n",
      "batch 2756: loss 0.080825\n",
      "batch 2757: loss 0.123233\n",
      "batch 2758: loss 0.078439\n",
      "batch 2759: loss 0.012171\n",
      "batch 2760: loss 0.059284\n",
      "batch 2761: loss 0.174902\n",
      "batch 2762: loss 0.061262\n",
      "batch 2763: loss 0.092715\n",
      "batch 2764: loss 0.117533\n",
      "batch 2765: loss 0.072427\n",
      "batch 2766: loss 0.094734\n",
      "batch 2767: loss 0.038367\n",
      "batch 2768: loss 0.033558\n",
      "batch 2769: loss 0.009112\n",
      "batch 2770: loss 0.161900\n",
      "batch 2771: loss 0.085924\n",
      "batch 2772: loss 0.063447\n",
      "batch 2773: loss 0.096404\n",
      "batch 2774: loss 0.151429\n",
      "batch 2775: loss 0.026624\n",
      "batch 2776: loss 0.080435\n",
      "batch 2777: loss 0.148687\n",
      "batch 2778: loss 0.040200\n",
      "batch 2779: loss 0.032152\n",
      "batch 2780: loss 0.042428\n",
      "batch 2781: loss 0.114528\n",
      "batch 2782: loss 0.048164\n",
      "batch 2783: loss 0.086493\n",
      "batch 2784: loss 0.052803\n",
      "batch 2785: loss 0.042597\n",
      "batch 2786: loss 0.078496\n",
      "batch 2787: loss 0.137935\n",
      "batch 2788: loss 0.082383\n",
      "batch 2789: loss 0.081613\n",
      "batch 2790: loss 0.094870\n",
      "batch 2791: loss 0.058065\n",
      "batch 2792: loss 0.100370\n",
      "batch 2793: loss 0.044133\n",
      "batch 2794: loss 0.040918\n",
      "batch 2795: loss 0.269577\n",
      "batch 2796: loss 0.049130\n",
      "batch 2797: loss 0.011201\n",
      "batch 2798: loss 0.236784\n",
      "batch 2799: loss 0.054074\n",
      "batch 2800: loss 0.029656\n",
      "batch 2801: loss 0.128313\n",
      "batch 2802: loss 0.125157\n",
      "batch 2803: loss 0.106965\n",
      "batch 2804: loss 0.029828\n",
      "batch 2805: loss 0.143891\n",
      "batch 2806: loss 0.096754\n",
      "batch 2807: loss 0.220266\n",
      "batch 2808: loss 0.025972\n",
      "batch 2809: loss 0.167080\n",
      "batch 2810: loss 0.119162\n",
      "batch 2811: loss 0.097691\n",
      "batch 2812: loss 0.085985\n",
      "batch 2813: loss 0.126993\n",
      "batch 2814: loss 0.028897\n",
      "batch 2815: loss 0.094027\n",
      "batch 2816: loss 0.079110\n",
      "batch 2817: loss 0.047644\n",
      "batch 2818: loss 0.289996\n",
      "batch 2819: loss 0.162333\n",
      "batch 2820: loss 0.132766\n",
      "batch 2821: loss 0.154112\n",
      "batch 2822: loss 0.060050\n",
      "batch 2823: loss 0.114726\n",
      "batch 2824: loss 0.071930\n",
      "batch 2825: loss 0.058097\n",
      "batch 2826: loss 0.082059\n",
      "batch 2827: loss 0.032870\n",
      "batch 2828: loss 0.075653\n",
      "batch 2829: loss 0.017543\n",
      "batch 2830: loss 0.069852\n",
      "batch 2831: loss 0.034936\n",
      "batch 2832: loss 0.192533\n",
      "batch 2833: loss 0.053420\n",
      "batch 2834: loss 0.079520\n",
      "batch 2835: loss 0.136978\n",
      "batch 2836: loss 0.015866\n",
      "batch 2837: loss 0.038703\n",
      "batch 2838: loss 0.078034\n",
      "batch 2839: loss 0.027196\n",
      "batch 2840: loss 0.039310\n",
      "batch 2841: loss 0.046158\n",
      "batch 2842: loss 0.040957\n",
      "batch 2843: loss 0.117618\n",
      "batch 2844: loss 0.062679\n",
      "batch 2845: loss 0.223321\n",
      "batch 2846: loss 0.092890\n",
      "batch 2847: loss 0.085001\n",
      "batch 2848: loss 0.056770\n",
      "batch 2849: loss 0.072224\n",
      "batch 2850: loss 0.038431\n",
      "batch 2851: loss 0.129134\n",
      "batch 2852: loss 0.009106\n",
      "batch 2853: loss 0.045317\n",
      "batch 2854: loss 0.089127\n",
      "batch 2855: loss 0.148379\n",
      "batch 2856: loss 0.139530\n",
      "batch 2857: loss 0.087232\n",
      "batch 2858: loss 0.082154\n",
      "batch 2859: loss 0.049541\n",
      "batch 2860: loss 0.219199\n",
      "batch 2861: loss 0.125936\n",
      "batch 2862: loss 0.045564\n",
      "batch 2863: loss 0.103685\n",
      "batch 2864: loss 0.058279\n",
      "batch 2865: loss 0.057912\n",
      "batch 2866: loss 0.192141\n",
      "batch 2867: loss 0.189385\n",
      "batch 2868: loss 0.044282\n",
      "batch 2869: loss 0.056158\n",
      "batch 2870: loss 0.022491\n",
      "batch 2871: loss 0.047635\n",
      "batch 2872: loss 0.041511\n",
      "batch 2873: loss 0.057500\n",
      "batch 2874: loss 0.049111\n",
      "batch 2875: loss 0.013758\n",
      "batch 2876: loss 0.036424\n",
      "batch 2877: loss 0.160054\n",
      "batch 2878: loss 0.236435\n",
      "batch 2879: loss 0.042608\n",
      "batch 2880: loss 0.132469\n",
      "batch 2881: loss 0.176969\n",
      "batch 2882: loss 0.011111\n",
      "batch 2883: loss 0.036138\n",
      "batch 2884: loss 0.072871\n",
      "batch 2885: loss 0.048557\n",
      "batch 2886: loss 0.140685\n",
      "batch 2887: loss 0.106718\n",
      "batch 2888: loss 0.225440\n",
      "batch 2889: loss 0.102606\n",
      "batch 2890: loss 0.032146\n",
      "batch 2891: loss 0.188539\n",
      "batch 2892: loss 0.037291\n",
      "batch 2893: loss 0.077301\n",
      "batch 2894: loss 0.130280\n",
      "batch 2895: loss 0.056833\n",
      "batch 2896: loss 0.262034\n",
      "batch 2897: loss 0.087319\n",
      "batch 2898: loss 0.040414\n",
      "batch 2899: loss 0.244243\n",
      "batch 2900: loss 0.366672\n",
      "batch 2901: loss 0.095580\n",
      "batch 2902: loss 0.040268\n",
      "batch 2903: loss 0.101387\n",
      "batch 2904: loss 0.141675\n",
      "batch 2905: loss 0.059170\n",
      "batch 2906: loss 0.074241\n",
      "batch 2907: loss 0.164825\n",
      "batch 2908: loss 0.077975\n",
      "batch 2909: loss 0.072852\n",
      "batch 2910: loss 0.096719\n",
      "batch 2911: loss 0.046807\n",
      "batch 2912: loss 0.090304\n",
      "batch 2913: loss 0.126304\n",
      "batch 2914: loss 0.041093\n",
      "batch 2915: loss 0.139333\n",
      "batch 2916: loss 0.028073\n",
      "batch 2917: loss 0.040927\n",
      "batch 2918: loss 0.145418\n",
      "batch 2919: loss 0.113099\n",
      "batch 2920: loss 0.029845\n",
      "batch 2921: loss 0.077865\n",
      "batch 2922: loss 0.071366\n",
      "batch 2923: loss 0.056359\n",
      "batch 2924: loss 0.013697\n",
      "batch 2925: loss 0.055808\n",
      "batch 2926: loss 0.174281\n",
      "batch 2927: loss 0.122809\n",
      "batch 2928: loss 0.234368\n",
      "batch 2929: loss 0.126973\n",
      "batch 2930: loss 0.139830\n",
      "batch 2931: loss 0.139194\n",
      "batch 2932: loss 0.026435\n",
      "batch 2933: loss 0.036984\n",
      "batch 2934: loss 0.041986\n",
      "batch 2935: loss 0.110674\n",
      "batch 2936: loss 0.143192\n",
      "batch 2937: loss 0.166860\n",
      "batch 2938: loss 0.110945\n",
      "batch 2939: loss 0.070319\n",
      "batch 2940: loss 0.075627\n",
      "batch 2941: loss 0.092110\n",
      "batch 2942: loss 0.137882\n",
      "batch 2943: loss 0.018201\n",
      "batch 2944: loss 0.032598\n",
      "batch 2945: loss 0.081636\n",
      "batch 2946: loss 0.054686\n",
      "batch 2947: loss 0.113385\n",
      "batch 2948: loss 0.275301\n",
      "batch 2949: loss 0.049527\n",
      "batch 2950: loss 0.147884\n",
      "batch 2951: loss 0.162331\n",
      "batch 2952: loss 0.048180\n",
      "batch 2953: loss 0.051055\n",
      "batch 2954: loss 0.076354\n",
      "batch 2955: loss 0.092148\n",
      "batch 2956: loss 0.082919\n",
      "batch 2957: loss 0.052302\n",
      "batch 2958: loss 0.078438\n",
      "batch 2959: loss 0.062872\n",
      "batch 2960: loss 0.118971\n",
      "batch 2961: loss 0.076694\n",
      "batch 2962: loss 0.063358\n",
      "batch 2963: loss 0.093303\n",
      "batch 2964: loss 0.055688\n",
      "batch 2965: loss 0.033765\n",
      "batch 2966: loss 0.062756\n",
      "batch 2967: loss 0.084633\n",
      "batch 2968: loss 0.026959\n",
      "batch 2969: loss 0.080374\n",
      "batch 2970: loss 0.078641\n",
      "batch 2971: loss 0.087337\n",
      "batch 2972: loss 0.281098\n",
      "batch 2973: loss 0.218181\n",
      "batch 2974: loss 0.110303\n",
      "batch 2975: loss 0.133174\n",
      "batch 2976: loss 0.069109\n",
      "batch 2977: loss 0.072368\n",
      "batch 2978: loss 0.113326\n",
      "batch 2979: loss 0.063618\n",
      "batch 2980: loss 0.068233\n",
      "batch 2981: loss 0.042828\n",
      "batch 2982: loss 0.135695\n",
      "batch 2983: loss 0.125614\n",
      "batch 2984: loss 0.062212\n",
      "batch 2985: loss 0.123361\n",
      "batch 2986: loss 0.098687\n",
      "batch 2987: loss 0.022831\n",
      "batch 2988: loss 0.070567\n",
      "batch 2989: loss 0.066452\n",
      "batch 2990: loss 0.047570\n",
      "batch 2991: loss 0.101479\n",
      "batch 2992: loss 0.099809\n",
      "batch 2993: loss 0.192256\n",
      "batch 2994: loss 0.099362\n",
      "batch 2995: loss 0.121694\n",
      "batch 2996: loss 0.037175\n",
      "batch 2997: loss 0.020424\n",
      "batch 2998: loss 0.030056\n",
      "batch 2999: loss 0.052898\n",
      "batch 3000: loss 0.053621\n",
      "batch 3001: loss 0.037766\n",
      "batch 3002: loss 0.052458\n",
      "batch 3003: loss 0.082449\n",
      "batch 3004: loss 0.080475\n",
      "batch 3005: loss 0.112856\n",
      "batch 3006: loss 0.018617\n",
      "batch 3007: loss 0.053840\n",
      "batch 3008: loss 0.218158\n",
      "batch 3009: loss 0.050311\n",
      "batch 3010: loss 0.167599\n",
      "batch 3011: loss 0.110964\n",
      "batch 3012: loss 0.080937\n",
      "batch 3013: loss 0.150956\n",
      "batch 3014: loss 0.059966\n",
      "batch 3015: loss 0.036721\n",
      "batch 3016: loss 0.250211\n",
      "batch 3017: loss 0.077107\n",
      "batch 3018: loss 0.214941\n",
      "batch 3019: loss 0.066586\n",
      "batch 3020: loss 0.093027\n",
      "batch 3021: loss 0.020136\n",
      "batch 3022: loss 0.033348\n",
      "batch 3023: loss 0.081648\n",
      "batch 3024: loss 0.222765\n",
      "batch 3025: loss 0.076278\n",
      "batch 3026: loss 0.076682\n",
      "batch 3027: loss 0.062469\n",
      "batch 3028: loss 0.281340\n",
      "batch 3029: loss 0.054256\n",
      "batch 3030: loss 0.071489\n",
      "batch 3031: loss 0.083917\n",
      "batch 3032: loss 0.106199\n",
      "batch 3033: loss 0.162819\n",
      "batch 3034: loss 0.202509\n",
      "batch 3035: loss 0.031996\n",
      "batch 3036: loss 0.248229\n",
      "batch 3037: loss 0.112799\n",
      "batch 3038: loss 0.090182\n",
      "batch 3039: loss 0.156359\n",
      "batch 3040: loss 0.032317\n",
      "batch 3041: loss 0.067054\n",
      "batch 3042: loss 0.049191\n",
      "batch 3043: loss 0.148171\n",
      "batch 3044: loss 0.023695\n",
      "batch 3045: loss 0.157246\n",
      "batch 3046: loss 0.053971\n",
      "batch 3047: loss 0.069017\n",
      "batch 3048: loss 0.038659\n",
      "batch 3049: loss 0.139948\n",
      "batch 3050: loss 0.156583\n",
      "batch 3051: loss 0.120982\n",
      "batch 3052: loss 0.026625\n",
      "batch 3053: loss 0.133828\n",
      "batch 3054: loss 0.188147\n",
      "batch 3055: loss 0.021585\n",
      "batch 3056: loss 0.140473\n",
      "batch 3057: loss 0.044870\n",
      "batch 3058: loss 0.204348\n",
      "batch 3059: loss 0.131002\n",
      "batch 3060: loss 0.056152\n",
      "batch 3061: loss 0.195912\n",
      "batch 3062: loss 0.178973\n",
      "batch 3063: loss 0.051228\n",
      "batch 3064: loss 0.129570\n",
      "batch 3065: loss 0.019722\n",
      "batch 3066: loss 0.010015\n",
      "batch 3067: loss 0.045918\n",
      "batch 3068: loss 0.095448\n",
      "batch 3069: loss 0.137693\n",
      "batch 3070: loss 0.034356\n",
      "batch 3071: loss 0.140597\n",
      "batch 3072: loss 0.043334\n",
      "batch 3073: loss 0.029779\n",
      "batch 3074: loss 0.067344\n",
      "batch 3075: loss 0.122304\n",
      "batch 3076: loss 0.060437\n",
      "batch 3077: loss 0.203423\n",
      "batch 3078: loss 0.135258\n",
      "batch 3079: loss 0.224376\n",
      "batch 3080: loss 0.091876\n",
      "batch 3081: loss 0.061153\n",
      "batch 3082: loss 0.128285\n",
      "batch 3083: loss 0.028682\n",
      "batch 3084: loss 0.040682\n",
      "batch 3085: loss 0.101304\n",
      "batch 3086: loss 0.016182\n",
      "batch 3087: loss 0.209939\n",
      "batch 3088: loss 0.103626\n",
      "batch 3089: loss 0.108590\n",
      "batch 3090: loss 0.124511\n",
      "batch 3091: loss 0.044380\n",
      "batch 3092: loss 0.103168\n",
      "batch 3093: loss 0.034751\n",
      "batch 3094: loss 0.031646\n",
      "batch 3095: loss 0.055449\n",
      "batch 3096: loss 0.113808\n",
      "batch 3097: loss 0.083291\n",
      "batch 3098: loss 0.120843\n",
      "batch 3099: loss 0.114065\n",
      "batch 3100: loss 0.061830\n",
      "batch 3101: loss 0.069987\n",
      "batch 3102: loss 0.149007\n",
      "batch 3103: loss 0.063218\n",
      "batch 3104: loss 0.177903\n",
      "batch 3105: loss 0.034472\n",
      "batch 3106: loss 0.065178\n",
      "batch 3107: loss 0.079896\n",
      "batch 3108: loss 0.060217\n",
      "batch 3109: loss 0.028158\n",
      "batch 3110: loss 0.029344\n",
      "batch 3111: loss 0.023786\n",
      "batch 3112: loss 0.114159\n",
      "batch 3113: loss 0.062631\n",
      "batch 3114: loss 0.085345\n",
      "batch 3115: loss 0.121717\n",
      "batch 3116: loss 0.046303\n",
      "batch 3117: loss 0.111844\n",
      "batch 3118: loss 0.085820\n",
      "batch 3119: loss 0.056046\n",
      "batch 3120: loss 0.046468\n",
      "batch 3121: loss 0.032431\n",
      "batch 3122: loss 0.010265\n",
      "batch 3123: loss 0.024529\n",
      "batch 3124: loss 0.139540\n",
      "batch 3125: loss 0.050411\n",
      "batch 3126: loss 0.028525\n",
      "batch 3127: loss 0.040292\n",
      "batch 3128: loss 0.129355\n",
      "batch 3129: loss 0.067674\n",
      "batch 3130: loss 0.037433\n",
      "batch 3131: loss 0.066707\n",
      "batch 3132: loss 0.098416\n",
      "batch 3133: loss 0.028075\n",
      "batch 3134: loss 0.022555\n",
      "batch 3135: loss 0.081538\n",
      "batch 3136: loss 0.043959\n",
      "batch 3137: loss 0.061145\n",
      "batch 3138: loss 0.082145\n",
      "batch 3139: loss 0.155132\n",
      "batch 3140: loss 0.070589\n",
      "batch 3141: loss 0.005285\n",
      "batch 3142: loss 0.043085\n",
      "batch 3143: loss 0.275136\n",
      "batch 3144: loss 0.094904\n",
      "batch 3145: loss 0.046530\n",
      "batch 3146: loss 0.184470\n",
      "batch 3147: loss 0.012617\n",
      "batch 3148: loss 0.044371\n",
      "batch 3149: loss 0.025165\n",
      "batch 3150: loss 0.099246\n",
      "batch 3151: loss 0.088080\n",
      "batch 3152: loss 0.120886\n",
      "batch 3153: loss 0.096404\n",
      "batch 3154: loss 0.024175\n",
      "batch 3155: loss 0.036270\n",
      "batch 3156: loss 0.080151\n",
      "batch 3157: loss 0.129064\n",
      "batch 3158: loss 0.175975\n",
      "batch 3159: loss 0.072027\n",
      "batch 3160: loss 0.099288\n",
      "batch 3161: loss 0.014878\n",
      "batch 3162: loss 0.083593\n",
      "batch 3163: loss 0.026887\n",
      "batch 3164: loss 0.074228\n",
      "batch 3165: loss 0.065174\n",
      "batch 3166: loss 0.058673\n",
      "batch 3167: loss 0.219625\n",
      "batch 3168: loss 0.070450\n",
      "batch 3169: loss 0.071531\n",
      "batch 3170: loss 0.060144\n",
      "batch 3171: loss 0.018538\n",
      "batch 3172: loss 0.048375\n",
      "batch 3173: loss 0.047128\n",
      "batch 3174: loss 0.284255\n",
      "batch 3175: loss 0.093732\n",
      "batch 3176: loss 0.052706\n",
      "batch 3177: loss 0.108654\n",
      "batch 3178: loss 0.173313\n",
      "batch 3179: loss 0.134952\n",
      "batch 3180: loss 0.100013\n",
      "batch 3181: loss 0.064279\n",
      "batch 3182: loss 0.053488\n",
      "batch 3183: loss 0.042018\n",
      "batch 3184: loss 0.024261\n",
      "batch 3185: loss 0.038001\n",
      "batch 3186: loss 0.056083\n",
      "batch 3187: loss 0.056548\n",
      "batch 3188: loss 0.183829\n",
      "batch 3189: loss 0.114291\n",
      "batch 3190: loss 0.025266\n",
      "batch 3191: loss 0.031690\n",
      "batch 3192: loss 0.035260\n",
      "batch 3193: loss 0.119359\n",
      "batch 3194: loss 0.125944\n",
      "batch 3195: loss 0.051428\n",
      "batch 3196: loss 0.125615\n",
      "batch 3197: loss 0.041468\n",
      "batch 3198: loss 0.051825\n",
      "batch 3199: loss 0.033509\n",
      "batch 3200: loss 0.067030\n",
      "batch 3201: loss 0.043711\n",
      "batch 3202: loss 0.155253\n",
      "batch 3203: loss 0.076521\n",
      "batch 3204: loss 0.172777\n",
      "batch 3205: loss 0.094170\n",
      "batch 3206: loss 0.074270\n",
      "batch 3207: loss 0.058551\n",
      "batch 3208: loss 0.056200\n",
      "batch 3209: loss 0.121917\n",
      "batch 3210: loss 0.144951\n",
      "batch 3211: loss 0.227921\n",
      "batch 3212: loss 0.106627\n",
      "batch 3213: loss 0.130791\n",
      "batch 3214: loss 0.023593\n",
      "batch 3215: loss 0.035529\n",
      "batch 3216: loss 0.089519\n",
      "batch 3217: loss 0.078721\n",
      "batch 3218: loss 0.025445\n",
      "batch 3219: loss 0.052899\n",
      "batch 3220: loss 0.117100\n",
      "batch 3221: loss 0.063561\n",
      "batch 3222: loss 0.113918\n",
      "batch 3223: loss 0.077524\n",
      "batch 3224: loss 0.064014\n",
      "batch 3225: loss 0.075203\n",
      "batch 3226: loss 0.092020\n",
      "batch 3227: loss 0.061314\n",
      "batch 3228: loss 0.136237\n",
      "batch 3229: loss 0.044544\n",
      "batch 3230: loss 0.060208\n",
      "batch 3231: loss 0.105849\n",
      "batch 3232: loss 0.146822\n",
      "batch 3233: loss 0.043423\n",
      "batch 3234: loss 0.302275\n",
      "batch 3235: loss 0.151441\n",
      "batch 3236: loss 0.150570\n",
      "batch 3237: loss 0.136395\n",
      "batch 3238: loss 0.325600\n",
      "batch 3239: loss 0.118029\n",
      "batch 3240: loss 0.040585\n",
      "batch 3241: loss 0.064223\n",
      "batch 3242: loss 0.088084\n",
      "batch 3243: loss 0.104097\n",
      "batch 3244: loss 0.035995\n",
      "batch 3245: loss 0.152274\n",
      "batch 3246: loss 0.145419\n",
      "batch 3247: loss 0.075846\n",
      "batch 3248: loss 0.067531\n",
      "batch 3249: loss 0.270946\n",
      "batch 3250: loss 0.163082\n",
      "batch 3251: loss 0.059782\n",
      "batch 3252: loss 0.073655\n",
      "batch 3253: loss 0.065885\n",
      "batch 3254: loss 0.322769\n",
      "batch 3255: loss 0.062021\n",
      "batch 3256: loss 0.070707\n",
      "batch 3257: loss 0.149545\n",
      "batch 3258: loss 0.060069\n",
      "batch 3259: loss 0.145691\n",
      "batch 3260: loss 0.052095\n",
      "batch 3261: loss 0.036125\n",
      "batch 3262: loss 0.032982\n",
      "batch 3263: loss 0.071380\n",
      "batch 3264: loss 0.037057\n",
      "batch 3265: loss 0.053449\n",
      "batch 3266: loss 0.120095\n",
      "batch 3267: loss 0.222162\n",
      "batch 3268: loss 0.043067\n",
      "batch 3269: loss 0.039206\n",
      "batch 3270: loss 0.011932\n",
      "batch 3271: loss 0.103544\n",
      "batch 3272: loss 0.234263\n",
      "batch 3273: loss 0.125522\n",
      "batch 3274: loss 0.051321\n",
      "batch 3275: loss 0.052346\n",
      "batch 3276: loss 0.012295\n",
      "batch 3277: loss 0.071808\n",
      "batch 3278: loss 0.125127\n",
      "batch 3279: loss 0.066354\n",
      "batch 3280: loss 0.056507\n",
      "batch 3281: loss 0.128674\n",
      "batch 3282: loss 0.056576\n",
      "batch 3283: loss 0.027631\n",
      "batch 3284: loss 0.051734\n",
      "batch 3285: loss 0.078717\n",
      "batch 3286: loss 0.056203\n",
      "batch 3287: loss 0.059015\n",
      "batch 3288: loss 0.073052\n",
      "batch 3289: loss 0.058889\n",
      "batch 3290: loss 0.156355\n",
      "batch 3291: loss 0.020296\n",
      "batch 3292: loss 0.177059\n",
      "batch 3293: loss 0.077751\n",
      "batch 3294: loss 0.172730\n",
      "batch 3295: loss 0.043117\n",
      "batch 3296: loss 0.194321\n",
      "batch 3297: loss 0.150042\n",
      "batch 3298: loss 0.013878\n",
      "batch 3299: loss 0.069599\n",
      "batch 3300: loss 0.112936\n",
      "batch 3301: loss 0.031747\n",
      "batch 3302: loss 0.052473\n",
      "batch 3303: loss 0.042512\n",
      "batch 3304: loss 0.133411\n",
      "batch 3305: loss 0.142409\n",
      "batch 3306: loss 0.028836\n",
      "batch 3307: loss 0.082857\n",
      "batch 3308: loss 0.139017\n",
      "batch 3309: loss 0.029907\n",
      "batch 3310: loss 0.035857\n",
      "batch 3311: loss 0.328876\n",
      "batch 3312: loss 0.125041\n",
      "batch 3313: loss 0.024192\n",
      "batch 3314: loss 0.044732\n",
      "batch 3315: loss 0.040666\n",
      "batch 3316: loss 0.040951\n",
      "batch 3317: loss 0.101134\n",
      "batch 3318: loss 0.020392\n",
      "batch 3319: loss 0.142868\n",
      "batch 3320: loss 0.071505\n",
      "batch 3321: loss 0.115458\n",
      "batch 3322: loss 0.056658\n",
      "batch 3323: loss 0.115132\n",
      "batch 3324: loss 0.050874\n",
      "batch 3325: loss 0.082079\n",
      "batch 3326: loss 0.027017\n",
      "batch 3327: loss 0.045486\n",
      "batch 3328: loss 0.064475\n",
      "batch 3329: loss 0.125176\n",
      "batch 3330: loss 0.170770\n",
      "batch 3331: loss 0.048398\n",
      "batch 3332: loss 0.024297\n",
      "batch 3333: loss 0.098038\n",
      "batch 3334: loss 0.091689\n",
      "batch 3335: loss 0.071421\n",
      "batch 3336: loss 0.055453\n",
      "batch 3337: loss 0.222149\n",
      "batch 3338: loss 0.059761\n",
      "batch 3339: loss 0.018895\n",
      "batch 3340: loss 0.016953\n",
      "batch 3341: loss 0.053981\n",
      "batch 3342: loss 0.020594\n",
      "batch 3343: loss 0.127239\n",
      "batch 3344: loss 0.062158\n",
      "batch 3345: loss 0.044715\n",
      "batch 3346: loss 0.026906\n",
      "batch 3347: loss 0.021591\n",
      "batch 3348: loss 0.064079\n",
      "batch 3349: loss 0.019176\n",
      "batch 3350: loss 0.077955\n",
      "batch 3351: loss 0.058088\n",
      "batch 3352: loss 0.032887\n",
      "batch 3353: loss 0.082021\n",
      "batch 3354: loss 0.033752\n",
      "batch 3355: loss 0.024861\n",
      "batch 3356: loss 0.037184\n",
      "batch 3357: loss 0.065333\n",
      "batch 3358: loss 0.019818\n",
      "batch 3359: loss 0.026995\n",
      "batch 3360: loss 0.048001\n",
      "batch 3361: loss 0.114131\n",
      "batch 3362: loss 0.064006\n",
      "batch 3363: loss 0.069965\n",
      "batch 3364: loss 0.086888\n",
      "batch 3365: loss 0.029897\n",
      "batch 3366: loss 0.279002\n",
      "batch 3367: loss 0.053650\n",
      "batch 3368: loss 0.042954\n",
      "batch 3369: loss 0.123093\n",
      "batch 3370: loss 0.256371\n",
      "batch 3371: loss 0.021987\n",
      "batch 3372: loss 0.038155\n",
      "batch 3373: loss 0.037937\n",
      "batch 3374: loss 0.110385\n",
      "batch 3375: loss 0.043955\n",
      "batch 3376: loss 0.087854\n",
      "batch 3377: loss 0.071688\n",
      "batch 3378: loss 0.091018\n",
      "batch 3379: loss 0.028389\n",
      "batch 3380: loss 0.058200\n",
      "batch 3381: loss 0.035645\n",
      "batch 3382: loss 0.047677\n",
      "batch 3383: loss 0.061566\n",
      "batch 3384: loss 0.057192\n",
      "batch 3385: loss 0.057428\n",
      "batch 3386: loss 0.275743\n",
      "batch 3387: loss 0.077154\n",
      "batch 3388: loss 0.043220\n",
      "batch 3389: loss 0.055074\n",
      "batch 3390: loss 0.136690\n",
      "batch 3391: loss 0.137102\n",
      "batch 3392: loss 0.065303\n",
      "batch 3393: loss 0.077210\n",
      "batch 3394: loss 0.058772\n",
      "batch 3395: loss 0.068318\n",
      "batch 3396: loss 0.064647\n",
      "batch 3397: loss 0.131444\n",
      "batch 3398: loss 0.097561\n",
      "batch 3399: loss 0.080508\n",
      "batch 3400: loss 0.069848\n",
      "batch 3401: loss 0.037225\n",
      "batch 3402: loss 0.049362\n",
      "batch 3403: loss 0.025004\n",
      "batch 3404: loss 0.098607\n",
      "batch 3405: loss 0.079482\n",
      "batch 3406: loss 0.085757\n",
      "batch 3407: loss 0.022139\n",
      "batch 3408: loss 0.112148\n",
      "batch 3409: loss 0.028530\n",
      "batch 3410: loss 0.126756\n",
      "batch 3411: loss 0.049324\n",
      "batch 3412: loss 0.269670\n",
      "batch 3413: loss 0.124286\n",
      "batch 3414: loss 0.190692\n",
      "batch 3415: loss 0.219141\n",
      "batch 3416: loss 0.029623\n",
      "batch 3417: loss 0.061301\n",
      "batch 3418: loss 0.051318\n",
      "batch 3419: loss 0.198214\n",
      "batch 3420: loss 0.010665\n",
      "batch 3421: loss 0.060812\n",
      "batch 3422: loss 0.092283\n",
      "batch 3423: loss 0.124937\n",
      "batch 3424: loss 0.074236\n",
      "batch 3425: loss 0.035771\n",
      "batch 3426: loss 0.168400\n",
      "batch 3427: loss 0.037782\n",
      "batch 3428: loss 0.055820\n",
      "batch 3429: loss 0.024427\n",
      "batch 3430: loss 0.098196\n",
      "batch 3431: loss 0.069550\n",
      "batch 3432: loss 0.041058\n",
      "batch 3433: loss 0.074876\n",
      "batch 3434: loss 0.082844\n",
      "batch 3435: loss 0.046803\n",
      "batch 3436: loss 0.103682\n",
      "batch 3437: loss 0.112791\n",
      "batch 3438: loss 0.054357\n",
      "batch 3439: loss 0.077833\n",
      "batch 3440: loss 0.098138\n",
      "batch 3441: loss 0.033503\n",
      "batch 3442: loss 0.103655\n",
      "batch 3443: loss 0.022000\n",
      "batch 3444: loss 0.013713\n",
      "batch 3445: loss 0.164170\n",
      "batch 3446: loss 0.036637\n",
      "batch 3447: loss 0.062273\n",
      "batch 3448: loss 0.044741\n",
      "batch 3449: loss 0.034770\n",
      "batch 3450: loss 0.165071\n",
      "batch 3451: loss 0.068895\n",
      "batch 3452: loss 0.132800\n",
      "batch 3453: loss 0.143753\n",
      "batch 3454: loss 0.022600\n",
      "batch 3455: loss 0.083155\n",
      "batch 3456: loss 0.049221\n",
      "batch 3457: loss 0.035305\n",
      "batch 3458: loss 0.098586\n",
      "batch 3459: loss 0.016412\n",
      "batch 3460: loss 0.049372\n",
      "batch 3461: loss 0.010743\n",
      "batch 3462: loss 0.082202\n",
      "batch 3463: loss 0.111848\n",
      "batch 3464: loss 0.044665\n",
      "batch 3465: loss 0.041130\n",
      "batch 3466: loss 0.144332\n",
      "batch 3467: loss 0.040548\n",
      "batch 3468: loss 0.212623\n",
      "batch 3469: loss 0.079902\n",
      "batch 3470: loss 0.048506\n",
      "batch 3471: loss 0.039976\n",
      "batch 3472: loss 0.035277\n",
      "batch 3473: loss 0.049959\n",
      "batch 3474: loss 0.143809\n",
      "batch 3475: loss 0.116741\n",
      "batch 3476: loss 0.090654\n",
      "batch 3477: loss 0.121736\n",
      "batch 3478: loss 0.132480\n",
      "batch 3479: loss 0.024310\n",
      "batch 3480: loss 0.065648\n",
      "batch 3481: loss 0.144360\n",
      "batch 3482: loss 0.066007\n",
      "batch 3483: loss 0.320182\n",
      "batch 3484: loss 0.087452\n",
      "batch 3485: loss 0.012506\n",
      "batch 3486: loss 0.125759\n",
      "batch 3487: loss 0.289964\n",
      "batch 3488: loss 0.086423\n",
      "batch 3489: loss 0.064900\n",
      "batch 3490: loss 0.233206\n",
      "batch 3491: loss 0.042567\n",
      "batch 3492: loss 0.044088\n",
      "batch 3493: loss 0.048221\n",
      "batch 3494: loss 0.029990\n",
      "batch 3495: loss 0.070370\n",
      "batch 3496: loss 0.074588\n",
      "batch 3497: loss 0.029313\n",
      "batch 3498: loss 0.040363\n",
      "batch 3499: loss 0.017614\n",
      "batch 3500: loss 0.072721\n",
      "batch 3501: loss 0.052711\n",
      "batch 3502: loss 0.035105\n",
      "batch 3503: loss 0.043466\n",
      "batch 3504: loss 0.053454\n",
      "batch 3505: loss 0.053126\n",
      "batch 3506: loss 0.145549\n",
      "batch 3507: loss 0.143153\n",
      "batch 3508: loss 0.071363\n",
      "batch 3509: loss 0.020934\n",
      "batch 3510: loss 0.072171\n",
      "batch 3511: loss 0.069734\n",
      "batch 3512: loss 0.117425\n",
      "batch 3513: loss 0.058915\n",
      "batch 3514: loss 0.014108\n",
      "batch 3515: loss 0.115435\n",
      "batch 3516: loss 0.019908\n",
      "batch 3517: loss 0.069797\n",
      "batch 3518: loss 0.041115\n",
      "batch 3519: loss 0.039688\n",
      "batch 3520: loss 0.104374\n",
      "batch 3521: loss 0.121506\n",
      "batch 3522: loss 0.165527\n",
      "batch 3523: loss 0.093919\n",
      "batch 3524: loss 0.092485\n",
      "batch 3525: loss 0.019366\n",
      "batch 3526: loss 0.037925\n",
      "batch 3527: loss 0.033478\n",
      "batch 3528: loss 0.023271\n",
      "batch 3529: loss 0.062016\n",
      "batch 3530: loss 0.146413\n",
      "batch 3531: loss 0.048401\n",
      "batch 3532: loss 0.096221\n",
      "batch 3533: loss 0.048428\n",
      "batch 3534: loss 0.029613\n",
      "batch 3535: loss 0.126967\n",
      "batch 3536: loss 0.041364\n",
      "batch 3537: loss 0.045331\n",
      "batch 3538: loss 0.173174\n",
      "batch 3539: loss 0.197546\n",
      "batch 3540: loss 0.108428\n",
      "batch 3541: loss 0.052833\n",
      "batch 3542: loss 0.194418\n",
      "batch 3543: loss 0.033115\n",
      "batch 3544: loss 0.055378\n",
      "batch 3545: loss 0.055494\n",
      "batch 3546: loss 0.229811\n",
      "batch 3547: loss 0.024692\n",
      "batch 3548: loss 0.159880\n",
      "batch 3549: loss 0.101078\n",
      "batch 3550: loss 0.041666\n",
      "batch 3551: loss 0.152614\n",
      "batch 3552: loss 0.049702\n",
      "batch 3553: loss 0.058522\n",
      "batch 3554: loss 0.096825\n",
      "batch 3555: loss 0.176428\n",
      "batch 3556: loss 0.017460\n",
      "batch 3557: loss 0.179555\n",
      "batch 3558: loss 0.120877\n",
      "batch 3559: loss 0.033069\n",
      "batch 3560: loss 0.036390\n",
      "batch 3561: loss 0.177485\n",
      "batch 3562: loss 0.048552\n",
      "batch 3563: loss 0.052780\n",
      "batch 3564: loss 0.120946\n",
      "batch 3565: loss 0.052873\n",
      "batch 3566: loss 0.034764\n",
      "batch 3567: loss 0.013185\n",
      "batch 3568: loss 0.095599\n",
      "batch 3569: loss 0.024108\n",
      "batch 3570: loss 0.074976\n",
      "batch 3571: loss 0.068810\n",
      "batch 3572: loss 0.090616\n",
      "batch 3573: loss 0.103327\n",
      "batch 3574: loss 0.238960\n",
      "batch 3575: loss 0.096677\n",
      "batch 3576: loss 0.175423\n",
      "batch 3577: loss 0.023094\n",
      "batch 3578: loss 0.095032\n",
      "batch 3579: loss 0.012917\n",
      "batch 3580: loss 0.061268\n",
      "batch 3581: loss 0.026898\n",
      "batch 3582: loss 0.044297\n",
      "batch 3583: loss 0.055057\n",
      "batch 3584: loss 0.055270\n",
      "batch 3585: loss 0.122309\n",
      "batch 3586: loss 0.051323\n",
      "batch 3587: loss 0.011718\n",
      "batch 3588: loss 0.062697\n",
      "batch 3589: loss 0.107045\n",
      "batch 3590: loss 0.190377\n",
      "batch 3591: loss 0.174842\n",
      "batch 3592: loss 0.035367\n",
      "batch 3593: loss 0.118290\n",
      "batch 3594: loss 0.075450\n",
      "batch 3595: loss 0.118129\n",
      "batch 3596: loss 0.201500\n",
      "batch 3597: loss 0.038012\n",
      "batch 3598: loss 0.298267\n",
      "batch 3599: loss 0.059707\n",
      "batch 3600: loss 0.085967\n",
      "batch 3601: loss 0.070416\n",
      "batch 3602: loss 0.044324\n",
      "batch 3603: loss 0.074684\n",
      "batch 3604: loss 0.057287\n",
      "batch 3605: loss 0.100882\n",
      "batch 3606: loss 0.050820\n",
      "batch 3607: loss 0.174437\n",
      "batch 3608: loss 0.030380\n",
      "batch 3609: loss 0.272590\n",
      "batch 3610: loss 0.066382\n",
      "batch 3611: loss 0.057577\n",
      "batch 3612: loss 0.093878\n",
      "batch 3613: loss 0.113444\n",
      "batch 3614: loss 0.050434\n",
      "batch 3615: loss 0.028006\n",
      "batch 3616: loss 0.170780\n",
      "batch 3617: loss 0.031957\n",
      "batch 3618: loss 0.060814\n",
      "batch 3619: loss 0.077460\n",
      "batch 3620: loss 0.026632\n",
      "batch 3621: loss 0.055984\n",
      "batch 3622: loss 0.087887\n",
      "batch 3623: loss 0.017766\n",
      "batch 3624: loss 0.036617\n",
      "batch 3625: loss 0.026883\n",
      "batch 3626: loss 0.165103\n",
      "batch 3627: loss 0.063390\n",
      "batch 3628: loss 0.051629\n",
      "batch 3629: loss 0.067892\n",
      "batch 3630: loss 0.063738\n",
      "batch 3631: loss 0.091182\n",
      "batch 3632: loss 0.058121\n",
      "batch 3633: loss 0.009084\n",
      "batch 3634: loss 0.133355\n",
      "batch 3635: loss 0.047496\n",
      "batch 3636: loss 0.151196\n",
      "batch 3637: loss 0.071689\n",
      "batch 3638: loss 0.016510\n",
      "batch 3639: loss 0.051893\n",
      "batch 3640: loss 0.171978\n",
      "batch 3641: loss 0.091996\n",
      "batch 3642: loss 0.052772\n",
      "batch 3643: loss 0.033375\n",
      "batch 3644: loss 0.090087\n",
      "batch 3645: loss 0.011025\n",
      "batch 3646: loss 0.019767\n",
      "batch 3647: loss 0.082996\n",
      "batch 3648: loss 0.053862\n",
      "batch 3649: loss 0.040446\n",
      "batch 3650: loss 0.106009\n",
      "batch 3651: loss 0.131902\n",
      "batch 3652: loss 0.106572\n",
      "batch 3653: loss 0.163746\n",
      "batch 3654: loss 0.104640\n",
      "batch 3655: loss 0.049244\n",
      "batch 3656: loss 0.016461\n",
      "batch 3657: loss 0.063101\n",
      "batch 3658: loss 0.049498\n",
      "batch 3659: loss 0.144106\n",
      "batch 3660: loss 0.036107\n",
      "batch 3661: loss 0.199923\n",
      "batch 3662: loss 0.170095\n",
      "batch 3663: loss 0.058126\n",
      "batch 3664: loss 0.137426\n",
      "batch 3665: loss 0.019513\n",
      "batch 3666: loss 0.039904\n",
      "batch 3667: loss 0.035064\n",
      "batch 3668: loss 0.034518\n",
      "batch 3669: loss 0.058064\n",
      "batch 3670: loss 0.052802\n",
      "batch 3671: loss 0.029540\n",
      "batch 3672: loss 0.081727\n",
      "batch 3673: loss 0.212326\n",
      "batch 3674: loss 0.041702\n",
      "batch 3675: loss 0.015206\n",
      "batch 3676: loss 0.060790\n",
      "batch 3677: loss 0.083762\n",
      "batch 3678: loss 0.012143\n",
      "batch 3679: loss 0.029276\n",
      "batch 3680: loss 0.061310\n",
      "batch 3681: loss 0.043793\n",
      "batch 3682: loss 0.209601\n",
      "batch 3683: loss 0.065877\n",
      "batch 3684: loss 0.081716\n",
      "batch 3685: loss 0.222480\n",
      "batch 3686: loss 0.088865\n",
      "batch 3687: loss 0.037018\n",
      "batch 3688: loss 0.081282\n",
      "batch 3689: loss 0.137253\n",
      "batch 3690: loss 0.142402\n",
      "batch 3691: loss 0.085181\n",
      "batch 3692: loss 0.172696\n",
      "batch 3693: loss 0.061198\n",
      "batch 3694: loss 0.041155\n",
      "batch 3695: loss 0.077962\n",
      "batch 3696: loss 0.119619\n",
      "batch 3697: loss 0.139501\n",
      "batch 3698: loss 0.030582\n",
      "batch 3699: loss 0.046305\n",
      "batch 3700: loss 0.050773\n",
      "batch 3701: loss 0.176010\n",
      "batch 3702: loss 0.084497\n",
      "batch 3703: loss 0.020518\n",
      "batch 3704: loss 0.286588\n",
      "batch 3705: loss 0.085665\n",
      "batch 3706: loss 0.020064\n",
      "batch 3707: loss 0.057561\n",
      "batch 3708: loss 0.120794\n",
      "batch 3709: loss 0.074301\n",
      "batch 3710: loss 0.090982\n",
      "batch 3711: loss 0.145258\n",
      "batch 3712: loss 0.201746\n",
      "batch 3713: loss 0.040270\n",
      "batch 3714: loss 0.136329\n",
      "batch 3715: loss 0.135941\n",
      "batch 3716: loss 0.105982\n",
      "batch 3717: loss 0.146011\n",
      "batch 3718: loss 0.022439\n",
      "batch 3719: loss 0.125670\n",
      "batch 3720: loss 0.029036\n",
      "batch 3721: loss 0.124046\n",
      "batch 3722: loss 0.254174\n",
      "batch 3723: loss 0.021083\n",
      "batch 3724: loss 0.093224\n",
      "batch 3725: loss 0.123858\n",
      "batch 3726: loss 0.155458\n",
      "batch 3727: loss 0.023099\n",
      "batch 3728: loss 0.047390\n",
      "batch 3729: loss 0.235850\n",
      "batch 3730: loss 0.030606\n",
      "batch 3731: loss 0.052702\n",
      "batch 3732: loss 0.137051\n",
      "batch 3733: loss 0.012466\n",
      "batch 3734: loss 0.094095\n",
      "batch 3735: loss 0.092924\n",
      "batch 3736: loss 0.048744\n",
      "batch 3737: loss 0.094150\n",
      "batch 3738: loss 0.120289\n",
      "batch 3739: loss 0.026750\n",
      "batch 3740: loss 0.124765\n",
      "batch 3741: loss 0.045352\n",
      "batch 3742: loss 0.081041\n",
      "batch 3743: loss 0.073291\n",
      "batch 3744: loss 0.059552\n",
      "batch 3745: loss 0.043369\n",
      "batch 3746: loss 0.054309\n",
      "batch 3747: loss 0.110215\n",
      "batch 3748: loss 0.008349\n",
      "batch 3749: loss 0.047680\n",
      "batch 3750: loss 0.017176\n",
      "batch 3751: loss 0.125739\n",
      "batch 3752: loss 0.026599\n",
      "batch 3753: loss 0.038403\n",
      "batch 3754: loss 0.165504\n",
      "batch 3755: loss 0.018917\n",
      "batch 3756: loss 0.096820\n",
      "batch 3757: loss 0.072024\n",
      "batch 3758: loss 0.035237\n",
      "batch 3759: loss 0.088916\n",
      "batch 3760: loss 0.051439\n",
      "batch 3761: loss 0.080784\n",
      "batch 3762: loss 0.080144\n",
      "batch 3763: loss 0.038421\n",
      "batch 3764: loss 0.057775\n",
      "batch 3765: loss 0.021278\n",
      "batch 3766: loss 0.042709\n",
      "batch 3767: loss 0.017196\n",
      "batch 3768: loss 0.068097\n",
      "batch 3769: loss 0.024307\n",
      "batch 3770: loss 0.150203\n",
      "batch 3771: loss 0.323778\n",
      "batch 3772: loss 0.070247\n",
      "batch 3773: loss 0.054835\n",
      "batch 3774: loss 0.024433\n",
      "batch 3775: loss 0.055764\n",
      "batch 3776: loss 0.077538\n",
      "batch 3777: loss 0.208011\n",
      "batch 3778: loss 0.027961\n",
      "batch 3779: loss 0.108893\n",
      "batch 3780: loss 0.097988\n",
      "batch 3781: loss 0.043399\n",
      "batch 3782: loss 0.116173\n",
      "batch 3783: loss 0.034040\n",
      "batch 3784: loss 0.047889\n",
      "batch 3785: loss 0.226702\n",
      "batch 3786: loss 0.171210\n",
      "batch 3787: loss 0.004612\n",
      "batch 3788: loss 0.143112\n",
      "batch 3789: loss 0.032160\n",
      "batch 3790: loss 0.060547\n",
      "batch 3791: loss 0.050763\n",
      "batch 3792: loss 0.306719\n",
      "batch 3793: loss 0.049733\n",
      "batch 3794: loss 0.031305\n",
      "batch 3795: loss 0.040505\n",
      "batch 3796: loss 0.048152\n",
      "batch 3797: loss 0.135890\n",
      "batch 3798: loss 0.098796\n",
      "batch 3799: loss 0.296965\n",
      "batch 3800: loss 0.227759\n",
      "batch 3801: loss 0.128421\n",
      "batch 3802: loss 0.020358\n",
      "batch 3803: loss 0.032741\n",
      "batch 3804: loss 0.033936\n",
      "batch 3805: loss 0.179654\n",
      "batch 3806: loss 0.113245\n",
      "batch 3807: loss 0.057243\n",
      "batch 3808: loss 0.226860\n",
      "batch 3809: loss 0.044492\n",
      "batch 3810: loss 0.042453\n",
      "batch 3811: loss 0.026550\n",
      "batch 3812: loss 0.067273\n",
      "batch 3813: loss 0.222145\n",
      "batch 3814: loss 0.120873\n",
      "batch 3815: loss 0.038390\n",
      "batch 3816: loss 0.039532\n",
      "batch 3817: loss 0.055735\n",
      "batch 3818: loss 0.020591\n",
      "batch 3819: loss 0.132076\n",
      "batch 3820: loss 0.109138\n",
      "batch 3821: loss 0.052277\n",
      "batch 3822: loss 0.099575\n",
      "batch 3823: loss 0.043771\n",
      "batch 3824: loss 0.114484\n",
      "batch 3825: loss 0.039936\n",
      "batch 3826: loss 0.046754\n",
      "batch 3827: loss 0.047048\n",
      "batch 3828: loss 0.012418\n",
      "batch 3829: loss 0.118996\n",
      "batch 3830: loss 0.072952\n",
      "batch 3831: loss 0.043674\n",
      "batch 3832: loss 0.146698\n",
      "batch 3833: loss 0.044859\n",
      "batch 3834: loss 0.074108\n",
      "batch 3835: loss 0.011815\n",
      "batch 3836: loss 0.200930\n",
      "batch 3837: loss 0.023759\n",
      "batch 3838: loss 0.026309\n",
      "batch 3839: loss 0.015192\n",
      "batch 3840: loss 0.086150\n",
      "batch 3841: loss 0.047632\n",
      "batch 3842: loss 0.026694\n",
      "batch 3843: loss 0.046402\n",
      "batch 3844: loss 0.105968\n",
      "batch 3845: loss 0.074194\n",
      "batch 3846: loss 0.023977\n",
      "batch 3847: loss 0.082711\n",
      "batch 3848: loss 0.272701\n",
      "batch 3849: loss 0.045968\n",
      "batch 3850: loss 0.054507\n",
      "batch 3851: loss 0.048512\n",
      "batch 3852: loss 0.040148\n",
      "batch 3853: loss 0.109414\n",
      "batch 3854: loss 0.040716\n",
      "batch 3855: loss 0.049816\n",
      "batch 3856: loss 0.027863\n",
      "batch 3857: loss 0.040792\n",
      "batch 3858: loss 0.054258\n",
      "batch 3859: loss 0.105861\n",
      "batch 3860: loss 0.043449\n",
      "batch 3861: loss 0.175871\n",
      "batch 3862: loss 0.047706\n",
      "batch 3863: loss 0.075101\n",
      "batch 3864: loss 0.118004\n",
      "batch 3865: loss 0.216518\n",
      "batch 3866: loss 0.124134\n",
      "batch 3867: loss 0.055090\n",
      "batch 3868: loss 0.106262\n",
      "batch 3869: loss 0.035293\n",
      "batch 3870: loss 0.011304\n",
      "batch 3871: loss 0.062336\n",
      "batch 3872: loss 0.050991\n",
      "batch 3873: loss 0.111649\n",
      "batch 3874: loss 0.035386\n",
      "batch 3875: loss 0.034143\n",
      "batch 3876: loss 0.325318\n",
      "batch 3877: loss 0.048993\n",
      "batch 3878: loss 0.101707\n",
      "batch 3879: loss 0.128591\n",
      "batch 3880: loss 0.071541\n",
      "batch 3881: loss 0.060825\n",
      "batch 3882: loss 0.025803\n",
      "batch 3883: loss 0.062718\n",
      "batch 3884: loss 0.137396\n",
      "batch 3885: loss 0.066905\n",
      "batch 3886: loss 0.090963\n",
      "batch 3887: loss 0.115372\n",
      "batch 3888: loss 0.192932\n",
      "batch 3889: loss 0.070068\n",
      "batch 3890: loss 0.032097\n",
      "batch 3891: loss 0.054162\n",
      "batch 3892: loss 0.013762\n",
      "batch 3893: loss 0.034996\n",
      "batch 3894: loss 0.071991\n",
      "batch 3895: loss 0.135694\n",
      "batch 3896: loss 0.130938\n",
      "batch 3897: loss 0.045658\n",
      "batch 3898: loss 0.058737\n",
      "batch 3899: loss 0.026074\n",
      "batch 3900: loss 0.020973\n",
      "batch 3901: loss 0.027314\n",
      "batch 3902: loss 0.066669\n",
      "batch 3903: loss 0.028695\n",
      "batch 3904: loss 0.113612\n",
      "batch 3905: loss 0.025739\n",
      "batch 3906: loss 0.164974\n",
      "batch 3907: loss 0.135803\n",
      "batch 3908: loss 0.080843\n",
      "batch 3909: loss 0.022712\n",
      "batch 3910: loss 0.040911\n",
      "batch 3911: loss 0.020800\n",
      "batch 3912: loss 0.064265\n",
      "batch 3913: loss 0.081953\n",
      "batch 3914: loss 0.111147\n",
      "batch 3915: loss 0.101357\n",
      "batch 3916: loss 0.190107\n",
      "batch 3917: loss 0.014276\n",
      "batch 3918: loss 0.068050\n",
      "batch 3919: loss 0.037811\n",
      "batch 3920: loss 0.038019\n",
      "batch 3921: loss 0.034946\n",
      "batch 3922: loss 0.056834\n",
      "batch 3923: loss 0.141285\n",
      "batch 3924: loss 0.105541\n",
      "batch 3925: loss 0.056866\n",
      "batch 3926: loss 0.151425\n",
      "batch 3927: loss 0.064034\n",
      "batch 3928: loss 0.028760\n",
      "batch 3929: loss 0.067524\n",
      "batch 3930: loss 0.159080\n",
      "batch 3931: loss 0.043778\n",
      "batch 3932: loss 0.077762\n",
      "batch 3933: loss 0.158251\n",
      "batch 3934: loss 0.082184\n",
      "batch 3935: loss 0.044987\n",
      "batch 3936: loss 0.047555\n",
      "batch 3937: loss 0.056063\n",
      "batch 3938: loss 0.086000\n",
      "batch 3939: loss 0.076501\n",
      "batch 3940: loss 0.021498\n",
      "batch 3941: loss 0.033963\n",
      "batch 3942: loss 0.114079\n",
      "batch 3943: loss 0.053162\n",
      "batch 3944: loss 0.077804\n",
      "batch 3945: loss 0.197953\n",
      "batch 3946: loss 0.055352\n",
      "batch 3947: loss 0.088710\n",
      "batch 3948: loss 0.016566\n",
      "batch 3949: loss 0.381507\n",
      "batch 3950: loss 0.092046\n",
      "batch 3951: loss 0.108446\n",
      "batch 3952: loss 0.184261\n",
      "batch 3953: loss 0.079165\n",
      "batch 3954: loss 0.055193\n",
      "batch 3955: loss 0.248044\n",
      "batch 3956: loss 0.053077\n",
      "batch 3957: loss 0.043831\n",
      "batch 3958: loss 0.137036\n",
      "batch 3959: loss 0.116443\n",
      "batch 3960: loss 0.236828\n",
      "batch 3961: loss 0.106705\n",
      "batch 3962: loss 0.072279\n",
      "batch 3963: loss 0.024773\n",
      "batch 3964: loss 0.099951\n",
      "batch 3965: loss 0.120100\n",
      "batch 3966: loss 0.161124\n",
      "batch 3967: loss 0.137167\n",
      "batch 3968: loss 0.040184\n",
      "batch 3969: loss 0.054486\n",
      "batch 3970: loss 0.051089\n",
      "batch 3971: loss 0.090060\n",
      "batch 3972: loss 0.068129\n",
      "batch 3973: loss 0.069864\n",
      "batch 3974: loss 0.052719\n",
      "batch 3975: loss 0.117651\n",
      "batch 3976: loss 0.062134\n",
      "batch 3977: loss 0.087253\n",
      "batch 3978: loss 0.119547\n",
      "batch 3979: loss 0.269319\n",
      "batch 3980: loss 0.255236\n",
      "batch 3981: loss 0.072107\n",
      "batch 3982: loss 0.094105\n",
      "batch 3983: loss 0.081094\n",
      "batch 3984: loss 0.077414\n",
      "batch 3985: loss 0.096891\n",
      "batch 3986: loss 0.094405\n",
      "batch 3987: loss 0.095971\n",
      "batch 3988: loss 0.080767\n",
      "batch 3989: loss 0.037825\n",
      "batch 3990: loss 0.051446\n",
      "batch 3991: loss 0.183926\n",
      "batch 3992: loss 0.151404\n",
      "batch 3993: loss 0.013444\n",
      "batch 3994: loss 0.093162\n",
      "batch 3995: loss 0.024128\n",
      "batch 3996: loss 0.016174\n",
      "batch 3997: loss 0.161518\n",
      "batch 3998: loss 0.027975\n",
      "batch 3999: loss 0.064287\n",
      "batch 4000: loss 0.016600\n",
      "batch 4001: loss 0.114647\n",
      "batch 4002: loss 0.117677\n",
      "batch 4003: loss 0.071670\n",
      "batch 4004: loss 0.060401\n",
      "batch 4005: loss 0.064590\n",
      "batch 4006: loss 0.179349\n",
      "batch 4007: loss 0.023783\n",
      "batch 4008: loss 0.084038\n",
      "batch 4009: loss 0.026887\n",
      "batch 4010: loss 0.127017\n",
      "batch 4011: loss 0.067874\n",
      "batch 4012: loss 0.098661\n",
      "batch 4013: loss 0.067283\n",
      "batch 4014: loss 0.027018\n",
      "batch 4015: loss 0.049519\n",
      "batch 4016: loss 0.042487\n",
      "batch 4017: loss 0.066116\n",
      "batch 4018: loss 0.092906\n",
      "batch 4019: loss 0.098385\n",
      "batch 4020: loss 0.230639\n",
      "batch 4021: loss 0.063593\n",
      "batch 4022: loss 0.048848\n",
      "batch 4023: loss 0.083819\n",
      "batch 4024: loss 0.078403\n",
      "batch 4025: loss 0.071040\n",
      "batch 4026: loss 0.128553\n",
      "batch 4027: loss 0.069341\n",
      "batch 4028: loss 0.076661\n",
      "batch 4029: loss 0.015403\n",
      "batch 4030: loss 0.088410\n",
      "batch 4031: loss 0.080036\n",
      "batch 4032: loss 0.080020\n",
      "batch 4033: loss 0.023621\n",
      "batch 4034: loss 0.051761\n",
      "batch 4035: loss 0.035506\n",
      "batch 4036: loss 0.040879\n",
      "batch 4037: loss 0.016844\n",
      "batch 4038: loss 0.047743\n",
      "batch 4039: loss 0.041447\n",
      "batch 4040: loss 0.045608\n",
      "batch 4041: loss 0.082175\n",
      "batch 4042: loss 0.192173\n",
      "batch 4043: loss 0.024404\n",
      "batch 4044: loss 0.069891\n",
      "batch 4045: loss 0.094495\n",
      "batch 4046: loss 0.088430\n",
      "batch 4047: loss 0.061749\n",
      "batch 4048: loss 0.058255\n",
      "batch 4049: loss 0.075964\n",
      "batch 4050: loss 0.007648\n",
      "batch 4051: loss 0.062131\n",
      "batch 4052: loss 0.052114\n",
      "batch 4053: loss 0.068167\n",
      "batch 4054: loss 0.078958\n",
      "batch 4055: loss 0.025180\n",
      "batch 4056: loss 0.044233\n",
      "batch 4057: loss 0.053723\n",
      "batch 4058: loss 0.020172\n",
      "batch 4059: loss 0.034053\n",
      "batch 4060: loss 0.147401\n",
      "batch 4061: loss 0.015495\n",
      "batch 4062: loss 0.033868\n",
      "batch 4063: loss 0.036574\n",
      "batch 4064: loss 0.059949\n",
      "batch 4065: loss 0.032482\n",
      "batch 4066: loss 0.065665\n",
      "batch 4067: loss 0.149766\n",
      "batch 4068: loss 0.106288\n",
      "batch 4069: loss 0.051826\n",
      "batch 4070: loss 0.024743\n",
      "batch 4071: loss 0.083322\n",
      "batch 4072: loss 0.048208\n",
      "batch 4073: loss 0.034471\n",
      "batch 4074: loss 0.028674\n",
      "batch 4075: loss 0.031258\n",
      "batch 4076: loss 0.307361\n",
      "batch 4077: loss 0.029411\n",
      "batch 4078: loss 0.039689\n",
      "batch 4079: loss 0.209752\n",
      "batch 4080: loss 0.075686\n",
      "batch 4081: loss 0.021070\n",
      "batch 4082: loss 0.066036\n",
      "batch 4083: loss 0.028200\n",
      "batch 4084: loss 0.028669\n",
      "batch 4085: loss 0.075717\n",
      "batch 4086: loss 0.052997\n",
      "batch 4087: loss 0.030965\n",
      "batch 4088: loss 0.023997\n",
      "batch 4089: loss 0.054449\n",
      "batch 4090: loss 0.048674\n",
      "batch 4091: loss 0.063736\n",
      "batch 4092: loss 0.029496\n",
      "batch 4093: loss 0.063599\n",
      "batch 4094: loss 0.040906\n",
      "batch 4095: loss 0.152529\n",
      "batch 4096: loss 0.179946\n",
      "batch 4097: loss 0.019685\n",
      "batch 4098: loss 0.040152\n",
      "batch 4099: loss 0.023577\n",
      "batch 4100: loss 0.091646\n",
      "batch 4101: loss 0.083527\n",
      "batch 4102: loss 0.131662\n",
      "batch 4103: loss 0.052367\n",
      "batch 4104: loss 0.091444\n",
      "batch 4105: loss 0.127231\n",
      "batch 4106: loss 0.072049\n",
      "batch 4107: loss 0.082687\n",
      "batch 4108: loss 0.024401\n",
      "batch 4109: loss 0.042042\n",
      "batch 4110: loss 0.012289\n",
      "batch 4111: loss 0.032400\n",
      "batch 4112: loss 0.057964\n",
      "batch 4113: loss 0.093830\n",
      "batch 4114: loss 0.019653\n",
      "batch 4115: loss 0.022998\n",
      "batch 4116: loss 0.050933\n",
      "batch 4117: loss 0.079461\n",
      "batch 4118: loss 0.075174\n",
      "batch 4119: loss 0.044412\n",
      "batch 4120: loss 0.064256\n",
      "batch 4121: loss 0.010493\n",
      "batch 4122: loss 0.075891\n",
      "batch 4123: loss 0.021586\n",
      "batch 4124: loss 0.058216\n",
      "batch 4125: loss 0.038190\n",
      "batch 4126: loss 0.027004\n",
      "batch 4127: loss 0.029714\n",
      "batch 4128: loss 0.085436\n",
      "batch 4129: loss 0.041453\n",
      "batch 4130: loss 0.026176\n",
      "batch 4131: loss 0.183875\n",
      "batch 4132: loss 0.039215\n",
      "batch 4133: loss 0.096842\n",
      "batch 4134: loss 0.260505\n",
      "batch 4135: loss 0.027401\n",
      "batch 4136: loss 0.110945\n",
      "batch 4137: loss 0.031084\n",
      "batch 4138: loss 0.177131\n",
      "batch 4139: loss 0.089561\n",
      "batch 4140: loss 0.062279\n",
      "batch 4141: loss 0.077106\n",
      "batch 4142: loss 0.039398\n",
      "batch 4143: loss 0.029757\n",
      "batch 4144: loss 0.022590\n",
      "batch 4145: loss 0.063027\n",
      "batch 4146: loss 0.068359\n",
      "batch 4147: loss 0.034240\n",
      "batch 4148: loss 0.209067\n",
      "batch 4149: loss 0.037886\n",
      "batch 4150: loss 0.098117\n",
      "batch 4151: loss 0.104961\n",
      "batch 4152: loss 0.095802\n",
      "batch 4153: loss 0.071954\n",
      "batch 4154: loss 0.042548\n",
      "batch 4155: loss 0.151151\n",
      "batch 4156: loss 0.078272\n",
      "batch 4157: loss 0.079410\n",
      "batch 4158: loss 0.038701\n",
      "batch 4159: loss 0.019251\n",
      "batch 4160: loss 0.023551\n",
      "batch 4161: loss 0.068690\n",
      "batch 4162: loss 0.036083\n",
      "batch 4163: loss 0.107371\n",
      "batch 4164: loss 0.064998\n",
      "batch 4165: loss 0.009000\n",
      "batch 4166: loss 0.040922\n",
      "batch 4167: loss 0.034100\n",
      "batch 4168: loss 0.155879\n",
      "batch 4169: loss 0.085536\n",
      "batch 4170: loss 0.033665\n",
      "batch 4171: loss 0.009441\n",
      "batch 4172: loss 0.088608\n",
      "batch 4173: loss 0.054387\n",
      "batch 4174: loss 0.017111\n",
      "batch 4175: loss 0.106570\n",
      "batch 4176: loss 0.014891\n",
      "batch 4177: loss 0.027854\n",
      "batch 4178: loss 0.152078\n",
      "batch 4179: loss 0.078258\n",
      "batch 4180: loss 0.044371\n",
      "batch 4181: loss 0.047202\n",
      "batch 4182: loss 0.023934\n",
      "batch 4183: loss 0.045853\n",
      "batch 4184: loss 0.073621\n",
      "batch 4185: loss 0.022781\n",
      "batch 4186: loss 0.026304\n",
      "batch 4187: loss 0.033443\n",
      "batch 4188: loss 0.118206\n",
      "batch 4189: loss 0.031635\n",
      "batch 4190: loss 0.069157\n",
      "batch 4191: loss 0.210126\n",
      "batch 4192: loss 0.069912\n",
      "batch 4193: loss 0.022842\n",
      "batch 4194: loss 0.029401\n",
      "batch 4195: loss 0.120764\n",
      "batch 4196: loss 0.062103\n",
      "batch 4197: loss 0.200866\n",
      "batch 4198: loss 0.087788\n",
      "batch 4199: loss 0.103744\n",
      "batch 4200: loss 0.385669\n",
      "batch 4201: loss 0.144240\n",
      "batch 4202: loss 0.290190\n",
      "batch 4203: loss 0.046158\n",
      "batch 4204: loss 0.125470\n",
      "batch 4205: loss 0.057752\n",
      "batch 4206: loss 0.053971\n",
      "batch 4207: loss 0.055215\n",
      "batch 4208: loss 0.060906\n",
      "batch 4209: loss 0.041649\n",
      "batch 4210: loss 0.009372\n",
      "batch 4211: loss 0.042153\n",
      "batch 4212: loss 0.058656\n",
      "batch 4213: loss 0.033424\n",
      "batch 4214: loss 0.011389\n",
      "batch 4215: loss 0.047410\n",
      "batch 4216: loss 0.017793\n",
      "batch 4217: loss 0.042393\n",
      "batch 4218: loss 0.090934\n",
      "batch 4219: loss 0.045753\n",
      "batch 4220: loss 0.173072\n",
      "batch 4221: loss 0.065815\n",
      "batch 4222: loss 0.028574\n",
      "batch 4223: loss 0.036143\n",
      "batch 4224: loss 0.022956\n",
      "batch 4225: loss 0.047675\n",
      "batch 4226: loss 0.035659\n",
      "batch 4227: loss 0.068982\n",
      "batch 4228: loss 0.072225\n",
      "batch 4229: loss 0.025298\n",
      "batch 4230: loss 0.067394\n",
      "batch 4231: loss 0.047836\n",
      "batch 4232: loss 0.018515\n",
      "batch 4233: loss 0.017042\n",
      "batch 4234: loss 0.050211\n",
      "batch 4235: loss 0.035952\n",
      "batch 4236: loss 0.013292\n",
      "batch 4237: loss 0.061051\n",
      "batch 4238: loss 0.026124\n",
      "batch 4239: loss 0.028667\n",
      "batch 4240: loss 0.029838\n",
      "batch 4241: loss 0.026786\n",
      "batch 4242: loss 0.036320\n",
      "batch 4243: loss 0.125675\n",
      "batch 4244: loss 0.107145\n",
      "batch 4245: loss 0.042908\n",
      "batch 4246: loss 0.176797\n",
      "batch 4247: loss 0.115898\n",
      "batch 4248: loss 0.116691\n",
      "batch 4249: loss 0.041498\n",
      "batch 4250: loss 0.081097\n",
      "batch 4251: loss 0.034287\n",
      "batch 4252: loss 0.163126\n",
      "batch 4253: loss 0.073230\n",
      "batch 4254: loss 0.041630\n",
      "batch 4255: loss 0.073446\n",
      "batch 4256: loss 0.095610\n",
      "batch 4257: loss 0.089762\n",
      "batch 4258: loss 0.068204\n",
      "batch 4259: loss 0.019755\n",
      "batch 4260: loss 0.106308\n",
      "batch 4261: loss 0.097318\n",
      "batch 4262: loss 0.012195\n",
      "batch 4263: loss 0.022432\n",
      "batch 4264: loss 0.019969\n",
      "batch 4265: loss 0.062541\n",
      "batch 4266: loss 0.106376\n",
      "batch 4267: loss 0.010735\n",
      "batch 4268: loss 0.058835\n",
      "batch 4269: loss 0.016618\n",
      "batch 4270: loss 0.067296\n",
      "batch 4271: loss 0.029366\n",
      "batch 4272: loss 0.080599\n",
      "batch 4273: loss 0.070340\n",
      "batch 4274: loss 0.019785\n",
      "batch 4275: loss 0.035037\n",
      "batch 4276: loss 0.120236\n",
      "batch 4277: loss 0.021727\n",
      "batch 4278: loss 0.093097\n",
      "batch 4279: loss 0.098496\n",
      "batch 4280: loss 0.093274\n",
      "batch 4281: loss 0.099699\n",
      "batch 4282: loss 0.275929\n",
      "batch 4283: loss 0.117619\n",
      "batch 4284: loss 0.023528\n",
      "batch 4285: loss 0.050324\n",
      "batch 4286: loss 0.008794\n",
      "batch 4287: loss 0.034216\n",
      "batch 4288: loss 0.162089\n",
      "batch 4289: loss 0.053313\n",
      "batch 4290: loss 0.141076\n",
      "batch 4291: loss 0.076293\n",
      "batch 4292: loss 0.015122\n",
      "batch 4293: loss 0.019281\n",
      "batch 4294: loss 0.270946\n",
      "batch 4295: loss 0.042763\n",
      "batch 4296: loss 0.035793\n",
      "batch 4297: loss 0.069237\n",
      "batch 4298: loss 0.173703\n",
      "batch 4299: loss 0.079238\n",
      "batch 4300: loss 0.011973\n",
      "batch 4301: loss 0.062411\n",
      "batch 4302: loss 0.025516\n",
      "batch 4303: loss 0.025975\n",
      "batch 4304: loss 0.005393\n",
      "batch 4305: loss 0.137496\n",
      "batch 4306: loss 0.128174\n",
      "batch 4307: loss 0.048575\n",
      "batch 4308: loss 0.057077\n",
      "batch 4309: loss 0.092087\n",
      "batch 4310: loss 0.051343\n",
      "batch 4311: loss 0.035147\n",
      "batch 4312: loss 0.081096\n",
      "batch 4313: loss 0.037461\n",
      "batch 4314: loss 0.018720\n",
      "batch 4315: loss 0.007233\n",
      "batch 4316: loss 0.132210\n",
      "batch 4317: loss 0.049366\n",
      "batch 4318: loss 0.030301\n",
      "batch 4319: loss 0.180310\n",
      "batch 4320: loss 0.130253\n",
      "batch 4321: loss 0.163996\n",
      "batch 4322: loss 0.022421\n",
      "batch 4323: loss 0.039325\n",
      "batch 4324: loss 0.125727\n",
      "batch 4325: loss 0.141426\n",
      "batch 4326: loss 0.096126\n",
      "batch 4327: loss 0.130555\n",
      "batch 4328: loss 0.138891\n",
      "batch 4329: loss 0.058198\n",
      "batch 4330: loss 0.052256\n",
      "batch 4331: loss 0.044944\n",
      "batch 4332: loss 0.013343\n",
      "batch 4333: loss 0.115901\n",
      "batch 4334: loss 0.036270\n",
      "batch 4335: loss 0.159892\n",
      "batch 4336: loss 0.091055\n",
      "batch 4337: loss 0.035298\n",
      "batch 4338: loss 0.079654\n",
      "batch 4339: loss 0.006529\n",
      "batch 4340: loss 0.019262\n",
      "batch 4341: loss 0.031141\n",
      "batch 4342: loss 0.039403\n",
      "batch 4343: loss 0.017062\n",
      "batch 4344: loss 0.139703\n",
      "batch 4345: loss 0.050487\n",
      "batch 4346: loss 0.202738\n",
      "batch 4347: loss 0.044342\n",
      "batch 4348: loss 0.043145\n",
      "batch 4349: loss 0.025926\n",
      "batch 4350: loss 0.025232\n",
      "batch 4351: loss 0.057782\n",
      "batch 4352: loss 0.038866\n",
      "batch 4353: loss 0.074210\n",
      "batch 4354: loss 0.004816\n",
      "batch 4355: loss 0.024704\n",
      "batch 4356: loss 0.133350\n",
      "batch 4357: loss 0.012349\n",
      "batch 4358: loss 0.159229\n",
      "batch 4359: loss 0.032076\n",
      "batch 4360: loss 0.088077\n",
      "batch 4361: loss 0.069769\n",
      "batch 4362: loss 0.019205\n",
      "batch 4363: loss 0.146392\n",
      "batch 4364: loss 0.123237\n",
      "batch 4365: loss 0.096058\n",
      "batch 4366: loss 0.198580\n",
      "batch 4367: loss 0.023511\n",
      "batch 4368: loss 0.172763\n",
      "batch 4369: loss 0.056416\n",
      "batch 4370: loss 0.036119\n",
      "batch 4371: loss 0.023410\n",
      "batch 4372: loss 0.228527\n",
      "batch 4373: loss 0.060608\n",
      "batch 4374: loss 0.116870\n",
      "batch 4375: loss 0.187380\n",
      "batch 4376: loss 0.068625\n",
      "batch 4377: loss 0.309033\n",
      "batch 4378: loss 0.147940\n",
      "batch 4379: loss 0.039663\n",
      "batch 4380: loss 0.107465\n",
      "batch 4381: loss 0.104847\n",
      "batch 4382: loss 0.075626\n",
      "batch 4383: loss 0.040649\n",
      "batch 4384: loss 0.110176\n",
      "batch 4385: loss 0.096970\n",
      "batch 4386: loss 0.077678\n",
      "batch 4387: loss 0.136070\n",
      "batch 4388: loss 0.018744\n",
      "batch 4389: loss 0.017439\n",
      "batch 4390: loss 0.194996\n",
      "batch 4391: loss 0.062995\n",
      "batch 4392: loss 0.093936\n",
      "batch 4393: loss 0.111426\n",
      "batch 4394: loss 0.057692\n",
      "batch 4395: loss 0.021251\n",
      "batch 4396: loss 0.084099\n",
      "batch 4397: loss 0.052043\n",
      "batch 4398: loss 0.031548\n",
      "batch 4399: loss 0.131841\n",
      "batch 4400: loss 0.028032\n",
      "batch 4401: loss 0.054181\n",
      "batch 4402: loss 0.023369\n",
      "batch 4403: loss 0.017701\n",
      "batch 4404: loss 0.070462\n",
      "batch 4405: loss 0.025328\n",
      "batch 4406: loss 0.019691\n",
      "batch 4407: loss 0.079113\n",
      "batch 4408: loss 0.018889\n",
      "batch 4409: loss 0.066090\n",
      "batch 4410: loss 0.032756\n",
      "batch 4411: loss 0.007675\n",
      "batch 4412: loss 0.151549\n",
      "batch 4413: loss 0.052396\n",
      "batch 4414: loss 0.109054\n",
      "batch 4415: loss 0.164146\n",
      "batch 4416: loss 0.029827\n",
      "batch 4417: loss 0.086223\n",
      "batch 4418: loss 0.075742\n",
      "batch 4419: loss 0.025211\n",
      "batch 4420: loss 0.007397\n",
      "batch 4421: loss 0.082071\n",
      "batch 4422: loss 0.036018\n",
      "batch 4423: loss 0.028361\n",
      "batch 4424: loss 0.227742\n",
      "batch 4425: loss 0.264170\n",
      "batch 4426: loss 0.025357\n",
      "batch 4427: loss 0.009194\n",
      "batch 4428: loss 0.051914\n",
      "batch 4429: loss 0.045921\n",
      "batch 4430: loss 0.153418\n",
      "batch 4431: loss 0.116858\n",
      "batch 4432: loss 0.044076\n",
      "batch 4433: loss 0.196143\n",
      "batch 4434: loss 0.252664\n",
      "batch 4435: loss 0.048251\n",
      "batch 4436: loss 0.016281\n",
      "batch 4437: loss 0.067056\n",
      "batch 4438: loss 0.011415\n",
      "batch 4439: loss 0.051751\n",
      "batch 4440: loss 0.013468\n",
      "batch 4441: loss 0.104862\n",
      "batch 4442: loss 0.063463\n",
      "batch 4443: loss 0.185377\n",
      "batch 4444: loss 0.020437\n",
      "batch 4445: loss 0.008341\n",
      "batch 4446: loss 0.042271\n",
      "batch 4447: loss 0.061665\n",
      "batch 4448: loss 0.032592\n",
      "batch 4449: loss 0.098891\n",
      "batch 4450: loss 0.097735\n",
      "batch 4451: loss 0.048621\n",
      "batch 4452: loss 0.043856\n",
      "batch 4453: loss 0.035246\n",
      "batch 4454: loss 0.018019\n",
      "batch 4455: loss 0.018466\n",
      "batch 4456: loss 0.050778\n",
      "batch 4457: loss 0.036494\n",
      "batch 4458: loss 0.065089\n",
      "batch 4459: loss 0.035253\n",
      "batch 4460: loss 0.079909\n",
      "batch 4461: loss 0.140941\n",
      "batch 4462: loss 0.032951\n",
      "batch 4463: loss 0.109047\n",
      "batch 4464: loss 0.027019\n",
      "batch 4465: loss 0.016866\n",
      "batch 4466: loss 0.030982\n",
      "batch 4467: loss 0.032525\n",
      "batch 4468: loss 0.008518\n",
      "batch 4469: loss 0.027357\n",
      "batch 4470: loss 0.030895\n",
      "batch 4471: loss 0.080992\n",
      "batch 4472: loss 0.026038\n",
      "batch 4473: loss 0.107626\n",
      "batch 4474: loss 0.069775\n",
      "batch 4475: loss 0.051761\n",
      "batch 4476: loss 0.114742\n",
      "batch 4477: loss 0.013414\n",
      "batch 4478: loss 0.032889\n",
      "batch 4479: loss 0.012187\n",
      "batch 4480: loss 0.095134\n",
      "batch 4481: loss 0.045332\n",
      "batch 4482: loss 0.050724\n",
      "batch 4483: loss 0.041313\n",
      "batch 4484: loss 0.122441\n",
      "batch 4485: loss 0.161848\n",
      "batch 4486: loss 0.160648\n",
      "batch 4487: loss 0.059495\n",
      "batch 4488: loss 0.037570\n",
      "batch 4489: loss 0.135880\n",
      "batch 4490: loss 0.070256\n",
      "batch 4491: loss 0.097506\n",
      "batch 4492: loss 0.083772\n",
      "batch 4493: loss 0.035035\n",
      "batch 4494: loss 0.020171\n",
      "batch 4495: loss 0.123984\n",
      "batch 4496: loss 0.145310\n",
      "batch 4497: loss 0.136053\n",
      "batch 4498: loss 0.106040\n",
      "batch 4499: loss 0.090893\n",
      "batch 4500: loss 0.048214\n",
      "batch 4501: loss 0.054692\n",
      "batch 4502: loss 0.034719\n",
      "batch 4503: loss 0.033050\n",
      "batch 4504: loss 0.022143\n",
      "batch 4505: loss 0.119677\n",
      "batch 4506: loss 0.030814\n",
      "batch 4507: loss 0.114776\n",
      "batch 4508: loss 0.019109\n",
      "batch 4509: loss 0.026637\n",
      "batch 4510: loss 0.062802\n",
      "batch 4511: loss 0.058933\n",
      "batch 4512: loss 0.106676\n",
      "batch 4513: loss 0.072506\n",
      "batch 4514: loss 0.018753\n",
      "batch 4515: loss 0.051003\n",
      "batch 4516: loss 0.124990\n",
      "batch 4517: loss 0.047929\n",
      "batch 4518: loss 0.040005\n",
      "batch 4519: loss 0.044739\n",
      "batch 4520: loss 0.032071\n",
      "batch 4521: loss 0.024492\n",
      "batch 4522: loss 0.023862\n",
      "batch 4523: loss 0.025632\n",
      "batch 4524: loss 0.062853\n",
      "batch 4525: loss 0.076103\n",
      "batch 4526: loss 0.035820\n",
      "batch 4527: loss 0.018271\n",
      "batch 4528: loss 0.015970\n",
      "batch 4529: loss 0.135465\n",
      "batch 4530: loss 0.016376\n",
      "batch 4531: loss 0.055726\n",
      "batch 4532: loss 0.154442\n",
      "batch 4533: loss 0.005407\n",
      "batch 4534: loss 0.009523\n",
      "batch 4535: loss 0.042174\n",
      "batch 4536: loss 0.062018\n",
      "batch 4537: loss 0.026176\n",
      "batch 4538: loss 0.074956\n",
      "batch 4539: loss 0.050458\n",
      "batch 4540: loss 0.040942\n",
      "batch 4541: loss 0.021815\n",
      "batch 4542: loss 0.061094\n",
      "batch 4543: loss 0.048775\n",
      "batch 4544: loss 0.024315\n",
      "batch 4545: loss 0.008052\n",
      "batch 4546: loss 0.064911\n",
      "batch 4547: loss 0.113301\n",
      "batch 4548: loss 0.056020\n",
      "batch 4549: loss 0.080095\n",
      "batch 4550: loss 0.015002\n",
      "batch 4551: loss 0.029097\n",
      "batch 4552: loss 0.026081\n",
      "batch 4553: loss 0.036050\n",
      "batch 4554: loss 0.049342\n",
      "batch 4555: loss 0.052846\n",
      "batch 4556: loss 0.020149\n",
      "batch 4557: loss 0.082833\n",
      "batch 4558: loss 0.033677\n",
      "batch 4559: loss 0.077150\n",
      "batch 4560: loss 0.044900\n",
      "batch 4561: loss 0.006894\n",
      "batch 4562: loss 0.070945\n",
      "batch 4563: loss 0.056069\n",
      "batch 4564: loss 0.026100\n",
      "batch 4565: loss 0.079503\n",
      "batch 4566: loss 0.013588\n",
      "batch 4567: loss 0.015352\n",
      "batch 4568: loss 0.091694\n",
      "batch 4569: loss 0.093047\n",
      "batch 4570: loss 0.027972\n",
      "batch 4571: loss 0.087966\n",
      "batch 4572: loss 0.167643\n",
      "batch 4573: loss 0.010082\n",
      "batch 4574: loss 0.037134\n",
      "batch 4575: loss 0.023370\n",
      "batch 4576: loss 0.035053\n",
      "batch 4577: loss 0.075947\n",
      "batch 4578: loss 0.032248\n",
      "batch 4579: loss 0.025526\n",
      "batch 4580: loss 0.056726\n",
      "batch 4581: loss 0.202700\n",
      "batch 4582: loss 0.026516\n",
      "batch 4583: loss 0.053779\n",
      "batch 4584: loss 0.033764\n",
      "batch 4585: loss 0.022045\n",
      "batch 4586: loss 0.027113\n",
      "batch 4587: loss 0.061640\n",
      "batch 4588: loss 0.057204\n",
      "batch 4589: loss 0.026003\n",
      "batch 4590: loss 0.112372\n",
      "batch 4591: loss 0.062615\n",
      "batch 4592: loss 0.017177\n",
      "batch 4593: loss 0.219525\n",
      "batch 4594: loss 0.017519\n",
      "batch 4595: loss 0.025622\n",
      "batch 4596: loss 0.031883\n",
      "batch 4597: loss 0.017766\n",
      "batch 4598: loss 0.075379\n",
      "batch 4599: loss 0.029734\n",
      "batch 4600: loss 0.054046\n",
      "batch 4601: loss 0.013998\n",
      "batch 4602: loss 0.031613\n",
      "batch 4603: loss 0.012534\n",
      "batch 4604: loss 0.038127\n",
      "batch 4605: loss 0.037241\n",
      "batch 4606: loss 0.064222\n",
      "batch 4607: loss 0.023017\n",
      "batch 4608: loss 0.153853\n",
      "batch 4609: loss 0.020380\n",
      "batch 4610: loss 0.007661\n",
      "batch 4611: loss 0.023553\n",
      "batch 4612: loss 0.022296\n",
      "batch 4613: loss 0.071124\n",
      "batch 4614: loss 0.036877\n",
      "batch 4615: loss 0.086669\n",
      "batch 4616: loss 0.013600\n",
      "batch 4617: loss 0.013240\n",
      "batch 4618: loss 0.140347\n",
      "batch 4619: loss 0.020331\n",
      "batch 4620: loss 0.013863\n",
      "batch 4621: loss 0.073924\n",
      "batch 4622: loss 0.037632\n",
      "batch 4623: loss 0.076647\n",
      "batch 4624: loss 0.195289\n",
      "batch 4625: loss 0.018695\n",
      "batch 4626: loss 0.033176\n",
      "batch 4627: loss 0.051566\n",
      "batch 4628: loss 0.026792\n",
      "batch 4629: loss 0.056571\n",
      "batch 4630: loss 0.032562\n",
      "batch 4631: loss 0.038757\n",
      "batch 4632: loss 0.027620\n",
      "batch 4633: loss 0.081814\n",
      "batch 4634: loss 0.059608\n",
      "batch 4635: loss 0.050059\n",
      "batch 4636: loss 0.043359\n",
      "batch 4637: loss 0.035555\n",
      "batch 4638: loss 0.081350\n",
      "batch 4639: loss 0.055311\n",
      "batch 4640: loss 0.048353\n",
      "batch 4641: loss 0.029950\n",
      "batch 4642: loss 0.034657\n",
      "batch 4643: loss 0.045330\n",
      "batch 4644: loss 0.074604\n",
      "batch 4645: loss 0.143688\n",
      "batch 4646: loss 0.027296\n",
      "batch 4647: loss 0.073666\n",
      "batch 4648: loss 0.029017\n",
      "batch 4649: loss 0.066353\n",
      "batch 4650: loss 0.017286\n",
      "batch 4651: loss 0.052040\n",
      "batch 4652: loss 0.130857\n",
      "batch 4653: loss 0.089488\n",
      "batch 4654: loss 0.035351\n",
      "batch 4655: loss 0.031036\n",
      "batch 4656: loss 0.061093\n",
      "batch 4657: loss 0.079526\n",
      "batch 4658: loss 0.008759\n",
      "batch 4659: loss 0.022367\n",
      "batch 4660: loss 0.056878\n",
      "batch 4661: loss 0.022254\n",
      "batch 4662: loss 0.056673\n",
      "batch 4663: loss 0.067285\n",
      "batch 4664: loss 0.017159\n",
      "batch 4665: loss 0.032242\n",
      "batch 4666: loss 0.019050\n",
      "batch 4667: loss 0.024217\n",
      "batch 4668: loss 0.049143\n",
      "batch 4669: loss 0.057898\n",
      "batch 4670: loss 0.051745\n",
      "batch 4671: loss 0.007396\n",
      "batch 4672: loss 0.018064\n",
      "batch 4673: loss 0.074378\n",
      "batch 4674: loss 0.062578\n",
      "batch 4675: loss 0.052747\n",
      "batch 4676: loss 0.045646\n",
      "batch 4677: loss 0.112155\n",
      "batch 4678: loss 0.057526\n",
      "batch 4679: loss 0.051596\n",
      "batch 4680: loss 0.029746\n",
      "batch 4681: loss 0.068594\n",
      "batch 4682: loss 0.068410\n",
      "batch 4683: loss 0.087785\n",
      "batch 4684: loss 0.051219\n",
      "batch 4685: loss 0.051619\n",
      "batch 4686: loss 0.192073\n",
      "batch 4687: loss 0.010171\n",
      "batch 4688: loss 0.108687\n",
      "batch 4689: loss 0.025136\n",
      "batch 4690: loss 0.060281\n",
      "batch 4691: loss 0.090996\n",
      "batch 4692: loss 0.033029\n",
      "batch 4693: loss 0.068346\n",
      "batch 4694: loss 0.138844\n",
      "batch 4695: loss 0.167594\n",
      "batch 4696: loss 0.006362\n",
      "batch 4697: loss 0.011815\n",
      "batch 4698: loss 0.031859\n",
      "batch 4699: loss 0.010363\n",
      "batch 4700: loss 0.122272\n",
      "batch 4701: loss 0.021287\n",
      "batch 4702: loss 0.011402\n",
      "batch 4703: loss 0.017809\n",
      "batch 4704: loss 0.042128\n",
      "batch 4705: loss 0.036201\n",
      "batch 4706: loss 0.035167\n",
      "batch 4707: loss 0.041928\n",
      "batch 4708: loss 0.016449\n",
      "batch 4709: loss 0.084754\n",
      "batch 4710: loss 0.082829\n",
      "batch 4711: loss 0.056792\n",
      "batch 4712: loss 0.045751\n",
      "batch 4713: loss 0.026095\n",
      "batch 4714: loss 0.064172\n",
      "batch 4715: loss 0.042161\n",
      "batch 4716: loss 0.015972\n",
      "batch 4717: loss 0.052722\n",
      "batch 4718: loss 0.045167\n",
      "batch 4719: loss 0.088812\n",
      "batch 4720: loss 0.052535\n",
      "batch 4721: loss 0.026486\n",
      "batch 4722: loss 0.081563\n",
      "batch 4723: loss 0.016868\n",
      "batch 4724: loss 0.021399\n",
      "batch 4725: loss 0.044234\n",
      "batch 4726: loss 0.026239\n",
      "batch 4727: loss 0.058934\n",
      "batch 4728: loss 0.026114\n",
      "batch 4729: loss 0.013498\n",
      "batch 4730: loss 0.138276\n",
      "batch 4731: loss 0.040285\n",
      "batch 4732: loss 0.162582\n",
      "batch 4733: loss 0.015913\n",
      "batch 4734: loss 0.008471\n",
      "batch 4735: loss 0.019782\n",
      "batch 4736: loss 0.036203\n",
      "batch 4737: loss 0.132920\n",
      "batch 4738: loss 0.055658\n",
      "batch 4739: loss 0.013970\n",
      "batch 4740: loss 0.020621\n",
      "batch 4741: loss 0.018851\n",
      "batch 4742: loss 0.069963\n",
      "batch 4743: loss 0.074184\n",
      "batch 4744: loss 0.059519\n",
      "batch 4745: loss 0.106771\n",
      "batch 4746: loss 0.224741\n",
      "batch 4747: loss 0.060870\n",
      "batch 4748: loss 0.104176\n",
      "batch 4749: loss 0.011701\n",
      "batch 4750: loss 0.057087\n",
      "batch 4751: loss 0.038766\n",
      "batch 4752: loss 0.243789\n",
      "batch 4753: loss 0.044702\n",
      "batch 4754: loss 0.017879\n",
      "batch 4755: loss 0.043350\n",
      "batch 4756: loss 0.029717\n",
      "batch 4757: loss 0.012010\n",
      "batch 4758: loss 0.144176\n",
      "batch 4759: loss 0.179658\n",
      "batch 4760: loss 0.050951\n",
      "batch 4761: loss 0.009122\n",
      "batch 4762: loss 0.077761\n",
      "batch 4763: loss 0.008515\n",
      "batch 4764: loss 0.013712\n",
      "batch 4765: loss 0.076198\n",
      "batch 4766: loss 0.024338\n",
      "batch 4767: loss 0.218380\n",
      "batch 4768: loss 0.031287\n",
      "batch 4769: loss 0.035165\n",
      "batch 4770: loss 0.034085\n",
      "batch 4771: loss 0.058827\n",
      "batch 4772: loss 0.040047\n",
      "batch 4773: loss 0.027901\n",
      "batch 4774: loss 0.143610\n",
      "batch 4775: loss 0.033549\n",
      "batch 4776: loss 0.016128\n",
      "batch 4777: loss 0.021886\n",
      "batch 4778: loss 0.010318\n",
      "batch 4779: loss 0.045907\n",
      "batch 4780: loss 0.034887\n",
      "batch 4781: loss 0.041179\n",
      "batch 4782: loss 0.057662\n",
      "batch 4783: loss 0.143094\n",
      "batch 4784: loss 0.037601\n",
      "batch 4785: loss 0.132771\n",
      "batch 4786: loss 0.115570\n",
      "batch 4787: loss 0.044519\n",
      "batch 4788: loss 0.160044\n",
      "batch 4789: loss 0.077536\n",
      "batch 4790: loss 0.054829\n",
      "batch 4791: loss 0.108266\n",
      "batch 4792: loss 0.064549\n",
      "batch 4793: loss 0.017263\n",
      "batch 4794: loss 0.089854\n",
      "batch 4795: loss 0.043219\n",
      "batch 4796: loss 0.047466\n",
      "batch 4797: loss 0.055580\n",
      "batch 4798: loss 0.033348\n",
      "batch 4799: loss 0.009562\n",
      "batch 4800: loss 0.018153\n",
      "batch 4801: loss 0.066209\n",
      "batch 4802: loss 0.144471\n",
      "batch 4803: loss 0.085811\n",
      "batch 4804: loss 0.029882\n",
      "batch 4805: loss 0.200198\n",
      "batch 4806: loss 0.074384\n",
      "batch 4807: loss 0.111130\n",
      "batch 4808: loss 0.075942\n",
      "batch 4809: loss 0.037590\n",
      "batch 4810: loss 0.046063\n",
      "batch 4811: loss 0.045928\n",
      "batch 4812: loss 0.039354\n",
      "batch 4813: loss 0.069652\n",
      "batch 4814: loss 0.121695\n",
      "batch 4815: loss 0.026046\n",
      "batch 4816: loss 0.058569\n",
      "batch 4817: loss 0.087412\n",
      "batch 4818: loss 0.010810\n",
      "batch 4819: loss 0.046273\n",
      "batch 4820: loss 0.079539\n",
      "batch 4821: loss 0.025351\n",
      "batch 4822: loss 0.061532\n",
      "batch 4823: loss 0.049193\n",
      "batch 4824: loss 0.020705\n",
      "batch 4825: loss 0.040223\n",
      "batch 4826: loss 0.052227\n",
      "batch 4827: loss 0.076563\n",
      "batch 4828: loss 0.055921\n",
      "batch 4829: loss 0.034654\n",
      "batch 4830: loss 0.012786\n",
      "batch 4831: loss 0.042948\n",
      "batch 4832: loss 0.018900\n",
      "batch 4833: loss 0.016251\n",
      "batch 4834: loss 0.030500\n",
      "batch 4835: loss 0.019468\n",
      "batch 4836: loss 0.023010\n",
      "batch 4837: loss 0.021464\n",
      "batch 4838: loss 0.035909\n",
      "batch 4839: loss 0.033441\n",
      "batch 4840: loss 0.043473\n",
      "batch 4841: loss 0.119774\n",
      "batch 4842: loss 0.070313\n",
      "batch 4843: loss 0.071954\n",
      "batch 4844: loss 0.010543\n",
      "batch 4845: loss 0.336554\n",
      "batch 4846: loss 0.098484\n",
      "batch 4847: loss 0.026574\n",
      "batch 4848: loss 0.015308\n",
      "batch 4849: loss 0.032668\n",
      "batch 4850: loss 0.068920\n",
      "batch 4851: loss 0.010861\n",
      "batch 4852: loss 0.044050\n",
      "batch 4853: loss 0.013376\n",
      "batch 4854: loss 0.058662\n",
      "batch 4855: loss 0.037114\n",
      "batch 4856: loss 0.025075\n",
      "batch 4857: loss 0.020654\n",
      "batch 4858: loss 0.098960\n",
      "batch 4859: loss 0.062285\n",
      "batch 4860: loss 0.039594\n",
      "batch 4861: loss 0.030052\n",
      "batch 4862: loss 0.095501\n",
      "batch 4863: loss 0.063658\n",
      "batch 4864: loss 0.043329\n",
      "batch 4865: loss 0.018407\n",
      "batch 4866: loss 0.022241\n",
      "batch 4867: loss 0.014782\n",
      "batch 4868: loss 0.016833\n",
      "batch 4869: loss 0.016780\n",
      "batch 4870: loss 0.056163\n",
      "batch 4871: loss 0.103898\n",
      "batch 4872: loss 0.017802\n",
      "batch 4873: loss 0.024592\n",
      "batch 4874: loss 0.015973\n",
      "batch 4875: loss 0.064637\n",
      "batch 4876: loss 0.082745\n",
      "batch 4877: loss 0.178377\n",
      "batch 4878: loss 0.164628\n",
      "batch 4879: loss 0.035565\n",
      "batch 4880: loss 0.103282\n",
      "batch 4881: loss 0.027713\n",
      "batch 4882: loss 0.028303\n",
      "batch 4883: loss 0.079653\n",
      "batch 4884: loss 0.058308\n",
      "batch 4885: loss 0.044800\n",
      "batch 4886: loss 0.047875\n",
      "batch 4887: loss 0.037370\n",
      "batch 4888: loss 0.027533\n",
      "batch 4889: loss 0.036624\n",
      "batch 4890: loss 0.009866\n",
      "batch 4891: loss 0.046642\n",
      "batch 4892: loss 0.145044\n",
      "batch 4893: loss 0.016563\n",
      "batch 4894: loss 0.071596\n",
      "batch 4895: loss 0.063548\n",
      "batch 4896: loss 0.119007\n",
      "batch 4897: loss 0.071578\n",
      "batch 4898: loss 0.023421\n",
      "batch 4899: loss 0.154206\n",
      "batch 4900: loss 0.021911\n",
      "batch 4901: loss 0.103468\n",
      "batch 4902: loss 0.049135\n",
      "batch 4903: loss 0.058104\n",
      "batch 4904: loss 0.093563\n",
      "batch 4905: loss 0.015473\n",
      "batch 4906: loss 0.063178\n",
      "batch 4907: loss 0.107308\n",
      "batch 4908: loss 0.091024\n",
      "batch 4909: loss 0.017502\n",
      "batch 4910: loss 0.139942\n",
      "batch 4911: loss 0.027531\n",
      "batch 4912: loss 0.020385\n",
      "batch 4913: loss 0.006960\n",
      "batch 4914: loss 0.033976\n",
      "batch 4915: loss 0.181029\n",
      "batch 4916: loss 0.098080\n",
      "batch 4917: loss 0.034445\n",
      "batch 4918: loss 0.008700\n",
      "batch 4919: loss 0.133664\n",
      "batch 4920: loss 0.012420\n",
      "batch 4921: loss 0.072296\n",
      "batch 4922: loss 0.035794\n",
      "batch 4923: loss 0.034499\n",
      "batch 4924: loss 0.065724\n",
      "batch 4925: loss 0.069216\n",
      "batch 4926: loss 0.077810\n",
      "batch 4927: loss 0.067379\n",
      "batch 4928: loss 0.055217\n",
      "batch 4929: loss 0.022764\n",
      "batch 4930: loss 0.019345\n",
      "batch 4931: loss 0.030894\n",
      "batch 4932: loss 0.115339\n",
      "batch 4933: loss 0.264490\n",
      "batch 4934: loss 0.047679\n",
      "batch 4935: loss 0.029779\n",
      "batch 4936: loss 0.033867\n",
      "batch 4937: loss 0.018579\n",
      "batch 4938: loss 0.038611\n",
      "batch 4939: loss 0.049793\n",
      "batch 4940: loss 0.036282\n",
      "batch 4941: loss 0.032532\n",
      "batch 4942: loss 0.053854\n",
      "batch 4943: loss 0.023338\n",
      "batch 4944: loss 0.091927\n",
      "batch 4945: loss 0.034294\n",
      "batch 4946: loss 0.014914\n",
      "batch 4947: loss 0.046758\n",
      "batch 4948: loss 0.193167\n",
      "batch 4949: loss 0.015614\n",
      "batch 4950: loss 0.010976\n",
      "batch 4951: loss 0.025672\n",
      "batch 4952: loss 0.088056\n",
      "batch 4953: loss 0.113145\n",
      "batch 4954: loss 0.122459\n",
      "batch 4955: loss 0.047409\n",
      "batch 4956: loss 0.021053\n",
      "batch 4957: loss 0.013361\n",
      "batch 4958: loss 0.023496\n",
      "batch 4959: loss 0.050993\n",
      "batch 4960: loss 0.009891\n",
      "batch 4961: loss 0.019713\n",
      "batch 4962: loss 0.049755\n",
      "batch 4963: loss 0.108023\n",
      "batch 4964: loss 0.078990\n",
      "batch 4965: loss 0.024593\n",
      "batch 4966: loss 0.015271\n",
      "batch 4967: loss 0.041486\n",
      "batch 4968: loss 0.058107\n",
      "batch 4969: loss 0.057940\n",
      "batch 4970: loss 0.043031\n",
      "batch 4971: loss 0.097994\n",
      "batch 4972: loss 0.019678\n",
      "batch 4973: loss 0.051560\n",
      "batch 4974: loss 0.014996\n",
      "batch 4975: loss 0.011092\n",
      "batch 4976: loss 0.082754\n",
      "batch 4977: loss 0.030527\n",
      "batch 4978: loss 0.075675\n",
      "batch 4979: loss 0.008280\n",
      "batch 4980: loss 0.046244\n",
      "batch 4981: loss 0.034392\n",
      "batch 4982: loss 0.182221\n",
      "batch 4983: loss 0.078936\n",
      "batch 4984: loss 0.015959\n",
      "batch 4985: loss 0.041587\n",
      "batch 4986: loss 0.079901\n",
      "batch 4987: loss 0.061601\n",
      "batch 4988: loss 0.064441\n",
      "batch 4989: loss 0.043236\n",
      "batch 4990: loss 0.166801\n",
      "batch 4991: loss 0.112940\n",
      "batch 4992: loss 0.057645\n",
      "batch 4993: loss 0.123535\n",
      "batch 4994: loss 0.061825\n",
      "batch 4995: loss 0.223780\n",
      "batch 4996: loss 0.018376\n",
      "batch 4997: loss 0.147923\n",
      "batch 4998: loss 0.092970\n",
      "batch 4999: loss 0.057682\n",
      "batch 5000: loss 0.061684\n",
      "batch 5001: loss 0.042522\n",
      "batch 5002: loss 0.026234\n",
      "batch 5003: loss 0.089043\n",
      "batch 5004: loss 0.093843\n",
      "batch 5005: loss 0.190846\n",
      "batch 5006: loss 0.104213\n",
      "batch 5007: loss 0.020863\n",
      "batch 5008: loss 0.033785\n",
      "batch 5009: loss 0.034329\n",
      "batch 5010: loss 0.019446\n",
      "batch 5011: loss 0.032038\n",
      "batch 5012: loss 0.139676\n",
      "batch 5013: loss 0.030426\n",
      "batch 5014: loss 0.165678\n",
      "batch 5015: loss 0.183548\n",
      "batch 5016: loss 0.065407\n",
      "batch 5017: loss 0.101101\n",
      "batch 5018: loss 0.013842\n",
      "batch 5019: loss 0.020764\n",
      "batch 5020: loss 0.026550\n",
      "batch 5021: loss 0.132421\n",
      "batch 5022: loss 0.156430\n",
      "batch 5023: loss 0.163844\n",
      "batch 5024: loss 0.009216\n",
      "batch 5025: loss 0.017773\n",
      "batch 5026: loss 0.024317\n",
      "batch 5027: loss 0.046993\n",
      "batch 5028: loss 0.078377\n",
      "batch 5029: loss 0.064864\n",
      "batch 5030: loss 0.021234\n",
      "batch 5031: loss 0.113108\n",
      "batch 5032: loss 0.033065\n",
      "batch 5033: loss 0.044302\n",
      "batch 5034: loss 0.009260\n",
      "batch 5035: loss 0.152726\n",
      "batch 5036: loss 0.059173\n",
      "batch 5037: loss 0.106051\n",
      "batch 5038: loss 0.042939\n",
      "batch 5039: loss 0.052107\n",
      "batch 5040: loss 0.236490\n",
      "batch 5041: loss 0.047333\n",
      "batch 5042: loss 0.040128\n",
      "batch 5043: loss 0.011953\n",
      "batch 5044: loss 0.031515\n",
      "batch 5045: loss 0.019410\n",
      "batch 5046: loss 0.061086\n",
      "batch 5047: loss 0.090500\n",
      "batch 5048: loss 0.165481\n",
      "batch 5049: loss 0.016605\n",
      "batch 5050: loss 0.043449\n",
      "batch 5051: loss 0.116160\n",
      "batch 5052: loss 0.024064\n",
      "batch 5053: loss 0.012781\n",
      "batch 5054: loss 0.056810\n",
      "batch 5055: loss 0.048401\n",
      "batch 5056: loss 0.004163\n",
      "batch 5057: loss 0.164516\n",
      "batch 5058: loss 0.034801\n",
      "batch 5059: loss 0.026416\n",
      "batch 5060: loss 0.047153\n",
      "batch 5061: loss 0.008789\n",
      "batch 5062: loss 0.012877\n",
      "batch 5063: loss 0.034183\n",
      "batch 5064: loss 0.031431\n",
      "batch 5065: loss 0.162339\n",
      "batch 5066: loss 0.015869\n",
      "batch 5067: loss 0.006487\n",
      "batch 5068: loss 0.021317\n",
      "batch 5069: loss 0.010050\n",
      "batch 5070: loss 0.061934\n",
      "batch 5071: loss 0.034230\n",
      "batch 5072: loss 0.051105\n",
      "batch 5073: loss 0.036033\n",
      "batch 5074: loss 0.051455\n",
      "batch 5075: loss 0.106167\n",
      "batch 5076: loss 0.044391\n",
      "batch 5077: loss 0.083640\n",
      "batch 5078: loss 0.030903\n",
      "batch 5079: loss 0.079637\n",
      "batch 5080: loss 0.009950\n",
      "batch 5081: loss 0.036709\n",
      "batch 5082: loss 0.167322\n",
      "batch 5083: loss 0.020547\n",
      "batch 5084: loss 0.039058\n",
      "batch 5085: loss 0.012090\n",
      "batch 5086: loss 0.046441\n",
      "batch 5087: loss 0.068862\n",
      "batch 5088: loss 0.088275\n",
      "batch 5089: loss 0.028646\n",
      "batch 5090: loss 0.031804\n",
      "batch 5091: loss 0.061323\n",
      "batch 5092: loss 0.119763\n",
      "batch 5093: loss 0.003400\n",
      "batch 5094: loss 0.057456\n",
      "batch 5095: loss 0.082659\n",
      "batch 5096: loss 0.036500\n",
      "batch 5097: loss 0.031339\n",
      "batch 5098: loss 0.016525\n",
      "batch 5099: loss 0.057011\n",
      "batch 5100: loss 0.223591\n",
      "batch 5101: loss 0.086535\n",
      "batch 5102: loss 0.054246\n",
      "batch 5103: loss 0.016027\n",
      "batch 5104: loss 0.049054\n",
      "batch 5105: loss 0.009688\n",
      "batch 5106: loss 0.038881\n",
      "batch 5107: loss 0.029214\n",
      "batch 5108: loss 0.087843\n",
      "batch 5109: loss 0.111230\n",
      "batch 5110: loss 0.013795\n",
      "batch 5111: loss 0.054892\n",
      "batch 5112: loss 0.028893\n",
      "batch 5113: loss 0.012627\n",
      "batch 5114: loss 0.017804\n",
      "batch 5115: loss 0.034147\n",
      "batch 5116: loss 0.027826\n",
      "batch 5117: loss 0.054414\n",
      "batch 5118: loss 0.024475\n",
      "batch 5119: loss 0.089246\n",
      "batch 5120: loss 0.011171\n",
      "batch 5121: loss 0.043332\n",
      "batch 5122: loss 0.039970\n",
      "batch 5123: loss 0.063769\n",
      "batch 5124: loss 0.016087\n",
      "batch 5125: loss 0.020583\n",
      "batch 5126: loss 0.118782\n",
      "batch 5127: loss 0.086749\n",
      "batch 5128: loss 0.008575\n",
      "batch 5129: loss 0.093057\n",
      "batch 5130: loss 0.015014\n",
      "batch 5131: loss 0.037995\n",
      "batch 5132: loss 0.038330\n",
      "batch 5133: loss 0.039016\n",
      "batch 5134: loss 0.029317\n",
      "batch 5135: loss 0.019304\n",
      "batch 5136: loss 0.076510\n",
      "batch 5137: loss 0.053736\n",
      "batch 5138: loss 0.019518\n",
      "batch 5139: loss 0.032540\n",
      "batch 5140: loss 0.084795\n",
      "batch 5141: loss 0.039831\n",
      "batch 5142: loss 0.062804\n",
      "batch 5143: loss 0.007531\n",
      "batch 5144: loss 0.059897\n",
      "batch 5145: loss 0.038707\n",
      "batch 5146: loss 0.114337\n",
      "batch 5147: loss 0.019682\n",
      "batch 5148: loss 0.019189\n",
      "batch 5149: loss 0.073203\n",
      "batch 5150: loss 0.062725\n",
      "batch 5151: loss 0.013882\n",
      "batch 5152: loss 0.020440\n",
      "batch 5153: loss 0.035703\n",
      "batch 5154: loss 0.028325\n",
      "batch 5155: loss 0.016974\n",
      "batch 5156: loss 0.151065\n",
      "batch 5157: loss 0.051857\n",
      "batch 5158: loss 0.018743\n",
      "batch 5159: loss 0.009746\n",
      "batch 5160: loss 0.023066\n",
      "batch 5161: loss 0.019683\n",
      "batch 5162: loss 0.143308\n",
      "batch 5163: loss 0.009865\n",
      "batch 5164: loss 0.107275\n",
      "batch 5165: loss 0.021787\n",
      "batch 5166: loss 0.020893\n",
      "batch 5167: loss 0.103662\n",
      "batch 5168: loss 0.023895\n",
      "batch 5169: loss 0.038898\n",
      "batch 5170: loss 0.161898\n",
      "batch 5171: loss 0.128947\n",
      "batch 5172: loss 0.076647\n",
      "batch 5173: loss 0.014766\n",
      "batch 5174: loss 0.013385\n",
      "batch 5175: loss 0.047734\n",
      "batch 5176: loss 0.011070\n",
      "batch 5177: loss 0.034254\n",
      "batch 5178: loss 0.017038\n",
      "batch 5179: loss 0.034964\n",
      "batch 5180: loss 0.012187\n",
      "batch 5181: loss 0.145185\n",
      "batch 5182: loss 0.050021\n",
      "batch 5183: loss 0.272504\n",
      "batch 5184: loss 0.054046\n",
      "batch 5185: loss 0.047935\n",
      "batch 5186: loss 0.060961\n",
      "batch 5187: loss 0.029123\n",
      "batch 5188: loss 0.070094\n",
      "batch 5189: loss 0.012110\n",
      "batch 5190: loss 0.053927\n",
      "batch 5191: loss 0.040993\n",
      "batch 5192: loss 0.116104\n",
      "batch 5193: loss 0.043318\n",
      "batch 5194: loss 0.049736\n",
      "batch 5195: loss 0.026862\n",
      "batch 5196: loss 0.168140\n",
      "batch 5197: loss 0.029930\n",
      "batch 5198: loss 0.016505\n",
      "batch 5199: loss 0.017882\n",
      "batch 5200: loss 0.205246\n",
      "batch 5201: loss 0.194418\n",
      "batch 5202: loss 0.020790\n",
      "batch 5203: loss 0.064173\n",
      "batch 5204: loss 0.089824\n",
      "batch 5205: loss 0.018062\n",
      "batch 5206: loss 0.029927\n",
      "batch 5207: loss 0.074345\n",
      "batch 5208: loss 0.011720\n",
      "batch 5209: loss 0.014803\n",
      "batch 5210: loss 0.057558\n",
      "batch 5211: loss 0.027891\n",
      "batch 5212: loss 0.110346\n",
      "batch 5213: loss 0.075733\n",
      "batch 5214: loss 0.273553\n",
      "batch 5215: loss 0.008781\n",
      "batch 5216: loss 0.007297\n",
      "batch 5217: loss 0.186535\n",
      "batch 5218: loss 0.069214\n",
      "batch 5219: loss 0.023223\n",
      "batch 5220: loss 0.098693\n",
      "batch 5221: loss 0.014559\n",
      "batch 5222: loss 0.048414\n",
      "batch 5223: loss 0.051279\n",
      "batch 5224: loss 0.142612\n",
      "batch 5225: loss 0.035819\n",
      "batch 5226: loss 0.025001\n",
      "batch 5227: loss 0.051126\n",
      "batch 5228: loss 0.043140\n",
      "batch 5229: loss 0.144386\n",
      "batch 5230: loss 0.134730\n",
      "batch 5231: loss 0.033234\n",
      "batch 5232: loss 0.010728\n",
      "batch 5233: loss 0.030290\n",
      "batch 5234: loss 0.007349\n",
      "batch 5235: loss 0.166186\n",
      "batch 5236: loss 0.134985\n",
      "batch 5237: loss 0.039642\n",
      "batch 5238: loss 0.098839\n",
      "batch 5239: loss 0.175566\n",
      "batch 5240: loss 0.103270\n",
      "batch 5241: loss 0.027819\n",
      "batch 5242: loss 0.139438\n",
      "batch 5243: loss 0.088532\n",
      "batch 5244: loss 0.114239\n",
      "batch 5245: loss 0.128702\n",
      "batch 5246: loss 0.126972\n",
      "batch 5247: loss 0.032586\n",
      "batch 5248: loss 0.115886\n",
      "batch 5249: loss 0.084790\n",
      "batch 5250: loss 0.025564\n",
      "batch 5251: loss 0.048396\n",
      "batch 5252: loss 0.027202\n",
      "batch 5253: loss 0.081292\n",
      "batch 5254: loss 0.072115\n",
      "batch 5255: loss 0.084697\n",
      "batch 5256: loss 0.037482\n",
      "batch 5257: loss 0.036375\n",
      "batch 5258: loss 0.020742\n",
      "batch 5259: loss 0.083423\n",
      "batch 5260: loss 0.032560\n",
      "batch 5261: loss 0.059639\n",
      "batch 5262: loss 0.017579\n",
      "batch 5263: loss 0.051730\n",
      "batch 5264: loss 0.068535\n",
      "batch 5265: loss 0.028105\n",
      "batch 5266: loss 0.135521\n",
      "batch 5267: loss 0.106059\n",
      "batch 5268: loss 0.041141\n",
      "batch 5269: loss 0.237688\n",
      "batch 5270: loss 0.054912\n",
      "batch 5271: loss 0.023631\n",
      "batch 5272: loss 0.182449\n",
      "batch 5273: loss 0.092975\n",
      "batch 5274: loss 0.111485\n",
      "batch 5275: loss 0.079181\n",
      "batch 5276: loss 0.045309\n",
      "batch 5277: loss 0.055133\n",
      "batch 5278: loss 0.030948\n",
      "batch 5279: loss 0.050220\n",
      "batch 5280: loss 0.013990\n",
      "batch 5281: loss 0.023193\n",
      "batch 5282: loss 0.019121\n",
      "batch 5283: loss 0.026207\n",
      "batch 5284: loss 0.153377\n",
      "batch 5285: loss 0.032550\n",
      "batch 5286: loss 0.134119\n",
      "batch 5287: loss 0.184392\n",
      "batch 5288: loss 0.069239\n",
      "batch 5289: loss 0.019990\n",
      "batch 5290: loss 0.056459\n",
      "batch 5291: loss 0.058591\n",
      "batch 5292: loss 0.061795\n",
      "batch 5293: loss 0.160271\n",
      "batch 5294: loss 0.202831\n",
      "batch 5295: loss 0.145196\n",
      "batch 5296: loss 0.079602\n",
      "batch 5297: loss 0.012347\n",
      "batch 5298: loss 0.016570\n",
      "batch 5299: loss 0.220916\n",
      "batch 5300: loss 0.007946\n",
      "batch 5301: loss 0.058896\n",
      "batch 5302: loss 0.073032\n",
      "batch 5303: loss 0.103460\n",
      "batch 5304: loss 0.106032\n",
      "batch 5305: loss 0.045798\n",
      "batch 5306: loss 0.072209\n",
      "batch 5307: loss 0.143022\n",
      "batch 5308: loss 0.023882\n",
      "batch 5309: loss 0.009044\n",
      "batch 5310: loss 0.015673\n",
      "batch 5311: loss 0.019566\n",
      "batch 5312: loss 0.039807\n",
      "batch 5313: loss 0.050441\n",
      "batch 5314: loss 0.107584\n",
      "batch 5315: loss 0.069548\n",
      "batch 5316: loss 0.107089\n",
      "batch 5317: loss 0.008398\n",
      "batch 5318: loss 0.044479\n",
      "batch 5319: loss 0.016163\n",
      "batch 5320: loss 0.040337\n",
      "batch 5321: loss 0.068232\n",
      "batch 5322: loss 0.070307\n",
      "batch 5323: loss 0.019331\n",
      "batch 5324: loss 0.217531\n",
      "batch 5325: loss 0.035612\n",
      "batch 5326: loss 0.060671\n",
      "batch 5327: loss 0.044009\n",
      "batch 5328: loss 0.091100\n",
      "batch 5329: loss 0.185521\n",
      "batch 5330: loss 0.019575\n",
      "batch 5331: loss 0.070239\n",
      "batch 5332: loss 0.014378\n",
      "batch 5333: loss 0.014913\n",
      "batch 5334: loss 0.017593\n",
      "batch 5335: loss 0.037283\n",
      "batch 5336: loss 0.055028\n",
      "batch 5337: loss 0.029566\n",
      "batch 5338: loss 0.040860\n",
      "batch 5339: loss 0.153015\n",
      "batch 5340: loss 0.011843\n",
      "batch 5341: loss 0.049464\n",
      "batch 5342: loss 0.066943\n",
      "batch 5343: loss 0.059442\n",
      "batch 5344: loss 0.009446\n",
      "batch 5345: loss 0.032016\n",
      "batch 5346: loss 0.050857\n",
      "batch 5347: loss 0.127795\n",
      "batch 5348: loss 0.061481\n",
      "batch 5349: loss 0.030263\n",
      "batch 5350: loss 0.066602\n",
      "batch 5351: loss 0.107001\n",
      "batch 5352: loss 0.020432\n",
      "batch 5353: loss 0.025560\n",
      "batch 5354: loss 0.052836\n",
      "batch 5355: loss 0.065058\n",
      "batch 5356: loss 0.046064\n",
      "batch 5357: loss 0.082871\n",
      "batch 5358: loss 0.017316\n",
      "batch 5359: loss 0.034266\n",
      "batch 5360: loss 0.035435\n",
      "batch 5361: loss 0.046135\n",
      "batch 5362: loss 0.017058\n",
      "batch 5363: loss 0.124132\n",
      "batch 5364: loss 0.018373\n",
      "batch 5365: loss 0.059593\n",
      "batch 5366: loss 0.050580\n",
      "batch 5367: loss 0.017120\n",
      "batch 5368: loss 0.110799\n",
      "batch 5369: loss 0.056835\n",
      "batch 5370: loss 0.075159\n",
      "batch 5371: loss 0.015755\n",
      "batch 5372: loss 0.048839\n",
      "batch 5373: loss 0.055864\n",
      "batch 5374: loss 0.015429\n",
      "batch 5375: loss 0.010313\n",
      "batch 5376: loss 0.013526\n",
      "batch 5377: loss 0.021428\n",
      "batch 5378: loss 0.062191\n",
      "batch 5379: loss 0.031288\n",
      "batch 5380: loss 0.042628\n",
      "batch 5381: loss 0.034989\n",
      "batch 5382: loss 0.046204\n",
      "batch 5383: loss 0.023693\n",
      "batch 5384: loss 0.007640\n",
      "batch 5385: loss 0.076128\n",
      "batch 5386: loss 0.093210\n",
      "batch 5387: loss 0.060088\n",
      "batch 5388: loss 0.011123\n",
      "batch 5389: loss 0.071849\n",
      "batch 5390: loss 0.022864\n",
      "batch 5391: loss 0.045894\n",
      "batch 5392: loss 0.005086\n",
      "batch 5393: loss 0.022758\n",
      "batch 5394: loss 0.063781\n",
      "batch 5395: loss 0.188755\n",
      "batch 5396: loss 0.027789\n",
      "batch 5397: loss 0.031872\n",
      "batch 5398: loss 0.055489\n",
      "batch 5399: loss 0.037117\n",
      "batch 5400: loss 0.135512\n",
      "batch 5401: loss 0.048970\n",
      "batch 5402: loss 0.014812\n",
      "batch 5403: loss 0.019708\n",
      "batch 5404: loss 0.043673\n",
      "batch 5405: loss 0.170470\n",
      "batch 5406: loss 0.022038\n",
      "batch 5407: loss 0.040343\n",
      "batch 5408: loss 0.032566\n",
      "batch 5409: loss 0.217499\n",
      "batch 5410: loss 0.064224\n",
      "batch 5411: loss 0.068954\n",
      "batch 5412: loss 0.022525\n",
      "batch 5413: loss 0.028753\n",
      "batch 5414: loss 0.043118\n",
      "batch 5415: loss 0.013435\n",
      "batch 5416: loss 0.071612\n",
      "batch 5417: loss 0.062130\n",
      "batch 5418: loss 0.009247\n",
      "batch 5419: loss 0.018513\n",
      "batch 5420: loss 0.051715\n",
      "batch 5421: loss 0.007561\n",
      "batch 5422: loss 0.044346\n",
      "batch 5423: loss 0.123893\n",
      "batch 5424: loss 0.061755\n",
      "batch 5425: loss 0.108107\n",
      "batch 5426: loss 0.011691\n",
      "batch 5427: loss 0.035534\n",
      "batch 5428: loss 0.038810\n",
      "batch 5429: loss 0.052440\n",
      "batch 5430: loss 0.010375\n",
      "batch 5431: loss 0.049514\n",
      "batch 5432: loss 0.026952\n",
      "batch 5433: loss 0.182488\n",
      "batch 5434: loss 0.020123\n",
      "batch 5435: loss 0.023523\n",
      "batch 5436: loss 0.061633\n",
      "batch 5437: loss 0.182382\n",
      "batch 5438: loss 0.093879\n",
      "batch 5439: loss 0.010118\n",
      "batch 5440: loss 0.130017\n",
      "batch 5441: loss 0.051631\n",
      "batch 5442: loss 0.015432\n",
      "batch 5443: loss 0.054597\n",
      "batch 5444: loss 0.011523\n",
      "batch 5445: loss 0.012683\n",
      "batch 5446: loss 0.118416\n",
      "batch 5447: loss 0.007688\n",
      "batch 5448: loss 0.067406\n",
      "batch 5449: loss 0.015877\n",
      "batch 5450: loss 0.037779\n",
      "batch 5451: loss 0.069002\n",
      "batch 5452: loss 0.024702\n",
      "batch 5453: loss 0.097944\n",
      "batch 5454: loss 0.028058\n",
      "batch 5455: loss 0.055793\n",
      "batch 5456: loss 0.132375\n",
      "batch 5457: loss 0.029028\n",
      "batch 5458: loss 0.018412\n",
      "batch 5459: loss 0.017425\n",
      "batch 5460: loss 0.103607\n",
      "batch 5461: loss 0.040268\n",
      "batch 5462: loss 0.048344\n",
      "batch 5463: loss 0.102252\n",
      "batch 5464: loss 0.069283\n",
      "batch 5465: loss 0.047558\n",
      "batch 5466: loss 0.018965\n",
      "batch 5467: loss 0.180555\n",
      "batch 5468: loss 0.073344\n",
      "batch 5469: loss 0.032148\n",
      "batch 5470: loss 0.030366\n",
      "batch 5471: loss 0.058519\n",
      "batch 5472: loss 0.006634\n",
      "batch 5473: loss 0.057192\n",
      "batch 5474: loss 0.122938\n",
      "batch 5475: loss 0.082336\n",
      "batch 5476: loss 0.020396\n",
      "batch 5477: loss 0.009446\n",
      "batch 5478: loss 0.027823\n",
      "batch 5479: loss 0.059006\n",
      "batch 5480: loss 0.023683\n",
      "batch 5481: loss 0.022759\n",
      "batch 5482: loss 0.036783\n",
      "batch 5483: loss 0.110393\n",
      "batch 5484: loss 0.030774\n",
      "batch 5485: loss 0.023386\n",
      "batch 5486: loss 0.009949\n",
      "batch 5487: loss 0.017028\n",
      "batch 5488: loss 0.082157\n",
      "batch 5489: loss 0.012270\n",
      "batch 5490: loss 0.047342\n",
      "batch 5491: loss 0.022658\n",
      "batch 5492: loss 0.023603\n",
      "batch 5493: loss 0.026515\n",
      "batch 5494: loss 0.023513\n",
      "batch 5495: loss 0.073470\n",
      "batch 5496: loss 0.044092\n",
      "batch 5497: loss 0.034895\n",
      "batch 5498: loss 0.024557\n",
      "batch 5499: loss 0.022057\n",
      "batch 5500: loss 0.021621\n",
      "batch 5501: loss 0.019152\n",
      "batch 5502: loss 0.019692\n",
      "batch 5503: loss 0.027361\n",
      "batch 5504: loss 0.098133\n",
      "batch 5505: loss 0.014396\n",
      "batch 5506: loss 0.007114\n",
      "batch 5507: loss 0.010816\n",
      "batch 5508: loss 0.034817\n",
      "batch 5509: loss 0.151084\n",
      "batch 5510: loss 0.046528\n",
      "batch 5511: loss 0.011255\n",
      "batch 5512: loss 0.153991\n",
      "batch 5513: loss 0.063485\n",
      "batch 5514: loss 0.006658\n",
      "batch 5515: loss 0.009401\n",
      "batch 5516: loss 0.029047\n",
      "batch 5517: loss 0.089923\n",
      "batch 5518: loss 0.006550\n",
      "batch 5519: loss 0.067778\n",
      "batch 5520: loss 0.013643\n",
      "batch 5521: loss 0.020683\n",
      "batch 5522: loss 0.103302\n",
      "batch 5523: loss 0.098564\n",
      "batch 5524: loss 0.094665\n",
      "batch 5525: loss 0.013194\n",
      "batch 5526: loss 0.054476\n",
      "batch 5527: loss 0.012947\n",
      "batch 5528: loss 0.077601\n",
      "batch 5529: loss 0.033143\n",
      "batch 5530: loss 0.035289\n",
      "batch 5531: loss 0.059713\n",
      "batch 5532: loss 0.146903\n",
      "batch 5533: loss 0.103445\n",
      "batch 5534: loss 0.052210\n",
      "batch 5535: loss 0.034480\n",
      "batch 5536: loss 0.076153\n",
      "batch 5537: loss 0.035522\n",
      "batch 5538: loss 0.071626\n",
      "batch 5539: loss 0.105765\n",
      "batch 5540: loss 0.189076\n",
      "batch 5541: loss 0.018412\n",
      "batch 5542: loss 0.051544\n",
      "batch 5543: loss 0.110166\n",
      "batch 5544: loss 0.012814\n",
      "batch 5545: loss 0.070888\n",
      "batch 5546: loss 0.127290\n",
      "batch 5547: loss 0.018925\n",
      "batch 5548: loss 0.143223\n",
      "batch 5549: loss 0.151957\n",
      "batch 5550: loss 0.039215\n",
      "batch 5551: loss 0.179194\n",
      "batch 5552: loss 0.120432\n",
      "batch 5553: loss 0.081426\n",
      "batch 5554: loss 0.071233\n",
      "batch 5555: loss 0.017272\n",
      "batch 5556: loss 0.030800\n",
      "batch 5557: loss 0.132210\n",
      "batch 5558: loss 0.034326\n",
      "batch 5559: loss 0.008601\n",
      "batch 5560: loss 0.257521\n",
      "batch 5561: loss 0.073937\n",
      "batch 5562: loss 0.086035\n",
      "batch 5563: loss 0.066356\n",
      "batch 5564: loss 0.059435\n",
      "batch 5565: loss 0.049285\n",
      "batch 5566: loss 0.096792\n",
      "batch 5567: loss 0.031230\n",
      "batch 5568: loss 0.089057\n",
      "batch 5569: loss 0.090102\n",
      "batch 5570: loss 0.033068\n",
      "batch 5571: loss 0.091885\n",
      "batch 5572: loss 0.062008\n",
      "batch 5573: loss 0.098074\n",
      "batch 5574: loss 0.034235\n",
      "batch 5575: loss 0.016382\n",
      "batch 5576: loss 0.116522\n",
      "batch 5577: loss 0.101937\n",
      "batch 5578: loss 0.013124\n",
      "batch 5579: loss 0.096478\n",
      "batch 5580: loss 0.035847\n",
      "batch 5581: loss 0.055940\n",
      "batch 5582: loss 0.145166\n",
      "batch 5583: loss 0.023645\n",
      "batch 5584: loss 0.102507\n",
      "batch 5585: loss 0.064286\n",
      "batch 5586: loss 0.121074\n",
      "batch 5587: loss 0.047768\n",
      "batch 5588: loss 0.017910\n",
      "batch 5589: loss 0.013658\n",
      "batch 5590: loss 0.087189\n",
      "batch 5591: loss 0.022383\n",
      "batch 5592: loss 0.050007\n",
      "batch 5593: loss 0.014040\n",
      "batch 5594: loss 0.079683\n",
      "batch 5595: loss 0.051294\n",
      "batch 5596: loss 0.077860\n",
      "batch 5597: loss 0.019336\n",
      "batch 5598: loss 0.069932\n",
      "batch 5599: loss 0.051867\n",
      "batch 5600: loss 0.036809\n",
      "batch 5601: loss 0.057663\n",
      "batch 5602: loss 0.053435\n",
      "batch 5603: loss 0.033596\n",
      "batch 5604: loss 0.003895\n",
      "batch 5605: loss 0.015105\n",
      "batch 5606: loss 0.011036\n",
      "batch 5607: loss 0.033121\n",
      "batch 5608: loss 0.008787\n",
      "batch 5609: loss 0.039657\n",
      "batch 5610: loss 0.009645\n",
      "batch 5611: loss 0.028735\n",
      "batch 5612: loss 0.008441\n",
      "batch 5613: loss 0.022540\n",
      "batch 5614: loss 0.011821\n",
      "batch 5615: loss 0.035027\n",
      "batch 5616: loss 0.028536\n",
      "batch 5617: loss 0.097597\n",
      "batch 5618: loss 0.042061\n",
      "batch 5619: loss 0.018473\n",
      "batch 5620: loss 0.013067\n",
      "batch 5621: loss 0.173690\n",
      "batch 5622: loss 0.011341\n",
      "batch 5623: loss 0.048415\n",
      "batch 5624: loss 0.025353\n",
      "batch 5625: loss 0.060803\n",
      "batch 5626: loss 0.008709\n",
      "batch 5627: loss 0.022014\n",
      "batch 5628: loss 0.020805\n",
      "batch 5629: loss 0.064175\n",
      "batch 5630: loss 0.124643\n",
      "batch 5631: loss 0.046586\n",
      "batch 5632: loss 0.006075\n",
      "batch 5633: loss 0.030560\n",
      "batch 5634: loss 0.024427\n",
      "batch 5635: loss 0.106903\n",
      "batch 5636: loss 0.037051\n",
      "batch 5637: loss 0.009493\n",
      "batch 5638: loss 0.166071\n",
      "batch 5639: loss 0.080274\n",
      "batch 5640: loss 0.023269\n",
      "batch 5641: loss 0.021499\n",
      "batch 5642: loss 0.101751\n",
      "batch 5643: loss 0.028905\n",
      "batch 5644: loss 0.045890\n",
      "batch 5645: loss 0.082128\n",
      "batch 5646: loss 0.045467\n",
      "batch 5647: loss 0.008976\n",
      "batch 5648: loss 0.059674\n",
      "batch 5649: loss 0.028860\n",
      "batch 5650: loss 0.018299\n",
      "batch 5651: loss 0.063689\n",
      "batch 5652: loss 0.091156\n",
      "batch 5653: loss 0.008603\n",
      "batch 5654: loss 0.088647\n",
      "batch 5655: loss 0.048003\n",
      "batch 5656: loss 0.015407\n",
      "batch 5657: loss 0.004855\n",
      "batch 5658: loss 0.060297\n",
      "batch 5659: loss 0.048696\n",
      "batch 5660: loss 0.075957\n",
      "batch 5661: loss 0.021377\n",
      "batch 5662: loss 0.015966\n",
      "batch 5663: loss 0.055230\n",
      "batch 5664: loss 0.004637\n",
      "batch 5665: loss 0.008979\n",
      "batch 5666: loss 0.139857\n",
      "batch 5667: loss 0.019238\n",
      "batch 5668: loss 0.015224\n",
      "batch 5669: loss 0.031825\n",
      "batch 5670: loss 0.011160\n",
      "batch 5671: loss 0.153359\n",
      "batch 5672: loss 0.049255\n",
      "batch 5673: loss 0.083329\n",
      "batch 5674: loss 0.006082\n",
      "batch 5675: loss 0.059679\n",
      "batch 5676: loss 0.177593\n",
      "batch 5677: loss 0.102298\n",
      "batch 5678: loss 0.014199\n",
      "batch 5679: loss 0.185886\n",
      "batch 5680: loss 0.031647\n",
      "batch 5681: loss 0.005132\n",
      "batch 5682: loss 0.016986\n",
      "batch 5683: loss 0.143397\n",
      "batch 5684: loss 0.032730\n",
      "batch 5685: loss 0.076139\n",
      "batch 5686: loss 0.080578\n",
      "batch 5687: loss 0.027698\n",
      "batch 5688: loss 0.101316\n",
      "batch 5689: loss 0.013927\n",
      "batch 5690: loss 0.094461\n",
      "batch 5691: loss 0.092135\n",
      "batch 5692: loss 0.042515\n",
      "batch 5693: loss 0.073695\n",
      "batch 5694: loss 0.023150\n",
      "batch 5695: loss 0.014519\n",
      "batch 5696: loss 0.020510\n",
      "batch 5697: loss 0.036801\n",
      "batch 5698: loss 0.032556\n",
      "batch 5699: loss 0.006089\n",
      "batch 5700: loss 0.056069\n",
      "batch 5701: loss 0.008577\n",
      "batch 5702: loss 0.021839\n",
      "batch 5703: loss 0.019129\n",
      "batch 5704: loss 0.019319\n",
      "batch 5705: loss 0.054660\n",
      "batch 5706: loss 0.036962\n",
      "batch 5707: loss 0.080800\n",
      "batch 5708: loss 0.175007\n",
      "batch 5709: loss 0.164944\n",
      "batch 5710: loss 0.156769\n",
      "batch 5711: loss 0.017404\n",
      "batch 5712: loss 0.041540\n",
      "batch 5713: loss 0.031114\n",
      "batch 5714: loss 0.035608\n",
      "batch 5715: loss 0.009516\n",
      "batch 5716: loss 0.048721\n",
      "batch 5717: loss 0.008999\n",
      "batch 5718: loss 0.053214\n",
      "batch 5719: loss 0.014932\n",
      "batch 5720: loss 0.041500\n",
      "batch 5721: loss 0.019596\n",
      "batch 5722: loss 0.091779\n",
      "batch 5723: loss 0.059057\n",
      "batch 5724: loss 0.054365\n",
      "batch 5725: loss 0.014299\n",
      "batch 5726: loss 0.055468\n",
      "batch 5727: loss 0.101944\n",
      "batch 5728: loss 0.016365\n",
      "batch 5729: loss 0.049897\n",
      "batch 5730: loss 0.029944\n",
      "batch 5731: loss 0.004265\n",
      "batch 5732: loss 0.067522\n",
      "batch 5733: loss 0.080052\n",
      "batch 5734: loss 0.046850\n",
      "batch 5735: loss 0.043317\n",
      "batch 5736: loss 0.026556\n",
      "batch 5737: loss 0.020982\n",
      "batch 5738: loss 0.009018\n",
      "batch 5739: loss 0.020428\n",
      "batch 5740: loss 0.124657\n",
      "batch 5741: loss 0.012760\n",
      "batch 5742: loss 0.014941\n",
      "batch 5743: loss 0.024482\n",
      "batch 5744: loss 0.015000\n",
      "batch 5745: loss 0.027127\n",
      "batch 5746: loss 0.048999\n",
      "batch 5747: loss 0.034839\n",
      "batch 5748: loss 0.061841\n",
      "batch 5749: loss 0.278843\n",
      "batch 5750: loss 0.036771\n",
      "batch 5751: loss 0.034002\n",
      "batch 5752: loss 0.014497\n",
      "batch 5753: loss 0.040622\n",
      "batch 5754: loss 0.037419\n",
      "batch 5755: loss 0.012322\n",
      "batch 5756: loss 0.175090\n",
      "batch 5757: loss 0.092647\n",
      "batch 5758: loss 0.027696\n",
      "batch 5759: loss 0.004250\n",
      "batch 5760: loss 0.017799\n",
      "batch 5761: loss 0.050376\n",
      "batch 5762: loss 0.065364\n",
      "batch 5763: loss 0.005311\n",
      "batch 5764: loss 0.048068\n",
      "batch 5765: loss 0.173534\n",
      "batch 5766: loss 0.031117\n",
      "batch 5767: loss 0.024752\n",
      "batch 5768: loss 0.078550\n",
      "batch 5769: loss 0.036958\n",
      "batch 5770: loss 0.018154\n",
      "batch 5771: loss 0.021276\n",
      "batch 5772: loss 0.018878\n",
      "batch 5773: loss 0.052917\n",
      "batch 5774: loss 0.009593\n",
      "batch 5775: loss 0.036190\n",
      "batch 5776: loss 0.029774\n",
      "batch 5777: loss 0.148030\n",
      "batch 5778: loss 0.119118\n",
      "batch 5779: loss 0.023207\n",
      "batch 5780: loss 0.238756\n",
      "batch 5781: loss 0.037888\n",
      "batch 5782: loss 0.020130\n",
      "batch 5783: loss 0.026400\n",
      "batch 5784: loss 0.025140\n",
      "batch 5785: loss 0.015425\n",
      "batch 5786: loss 0.019767\n",
      "batch 5787: loss 0.010856\n",
      "batch 5788: loss 0.060730\n",
      "batch 5789: loss 0.016556\n",
      "batch 5790: loss 0.064312\n",
      "batch 5791: loss 0.005587\n",
      "batch 5792: loss 0.013829\n",
      "batch 5793: loss 0.023090\n",
      "batch 5794: loss 0.007120\n",
      "batch 5795: loss 0.168785\n",
      "batch 5796: loss 0.030482\n",
      "batch 5797: loss 0.048760\n",
      "batch 5798: loss 0.019869\n",
      "batch 5799: loss 0.036528\n",
      "batch 5800: loss 0.023242\n",
      "batch 5801: loss 0.086643\n",
      "batch 5802: loss 0.050582\n",
      "batch 5803: loss 0.035634\n",
      "batch 5804: loss 0.035792\n",
      "batch 5805: loss 0.076419\n",
      "batch 5806: loss 0.012902\n",
      "batch 5807: loss 0.034367\n",
      "batch 5808: loss 0.049588\n",
      "batch 5809: loss 0.030024\n",
      "batch 5810: loss 0.015575\n",
      "batch 5811: loss 0.064439\n",
      "batch 5812: loss 0.011418\n",
      "batch 5813: loss 0.083026\n",
      "batch 5814: loss 0.064540\n",
      "batch 5815: loss 0.042254\n",
      "batch 5816: loss 0.077027\n",
      "batch 5817: loss 0.015345\n",
      "batch 5818: loss 0.014196\n",
      "batch 5819: loss 0.003400\n",
      "batch 5820: loss 0.091521\n",
      "batch 5821: loss 0.031637\n",
      "batch 5822: loss 0.046085\n",
      "batch 5823: loss 0.060117\n",
      "batch 5824: loss 0.056231\n",
      "batch 5825: loss 0.015009\n",
      "batch 5826: loss 0.017999\n",
      "batch 5827: loss 0.014887\n",
      "batch 5828: loss 0.023441\n",
      "batch 5829: loss 0.031670\n",
      "batch 5830: loss 0.052849\n",
      "batch 5831: loss 0.010782\n",
      "batch 5832: loss 0.031061\n",
      "batch 5833: loss 0.005593\n",
      "batch 5834: loss 0.078669\n",
      "batch 5835: loss 0.086631\n",
      "batch 5836: loss 0.072014\n",
      "batch 5837: loss 0.087313\n",
      "batch 5838: loss 0.034261\n",
      "batch 5839: loss 0.016856\n",
      "batch 5840: loss 0.047494\n",
      "batch 5841: loss 0.100675\n",
      "batch 5842: loss 0.014054\n",
      "batch 5843: loss 0.139892\n",
      "batch 5844: loss 0.061424\n",
      "batch 5845: loss 0.036827\n",
      "batch 5846: loss 0.012696\n",
      "batch 5847: loss 0.017878\n",
      "batch 5848: loss 0.073627\n",
      "batch 5849: loss 0.170364\n",
      "batch 5850: loss 0.037918\n",
      "batch 5851: loss 0.008151\n",
      "batch 5852: loss 0.052670\n",
      "batch 5853: loss 0.074578\n",
      "batch 5854: loss 0.066686\n",
      "batch 5855: loss 0.008905\n",
      "batch 5856: loss 0.140950\n",
      "batch 5857: loss 0.080489\n",
      "batch 5858: loss 0.023072\n",
      "batch 5859: loss 0.054700\n",
      "batch 5860: loss 0.101818\n",
      "batch 5861: loss 0.031345\n",
      "batch 5862: loss 0.129131\n",
      "batch 5863: loss 0.070316\n",
      "batch 5864: loss 0.090159\n",
      "batch 5865: loss 0.021733\n",
      "batch 5866: loss 0.044673\n",
      "batch 5867: loss 0.026165\n",
      "batch 5868: loss 0.040258\n",
      "batch 5869: loss 0.077908\n",
      "batch 5870: loss 0.108752\n",
      "batch 5871: loss 0.018419\n",
      "batch 5872: loss 0.187909\n",
      "batch 5873: loss 0.019835\n",
      "batch 5874: loss 0.118376\n",
      "batch 5875: loss 0.095489\n",
      "batch 5876: loss 0.015769\n",
      "batch 5877: loss 0.097718\n",
      "batch 5878: loss 0.015724\n",
      "batch 5879: loss 0.011782\n",
      "batch 5880: loss 0.014970\n",
      "batch 5881: loss 0.030014\n",
      "batch 5882: loss 0.140350\n",
      "batch 5883: loss 0.102935\n",
      "batch 5884: loss 0.012773\n",
      "batch 5885: loss 0.031307\n",
      "batch 5886: loss 0.089458\n",
      "batch 5887: loss 0.043702\n",
      "batch 5888: loss 0.041918\n",
      "batch 5889: loss 0.028871\n",
      "batch 5890: loss 0.036501\n",
      "batch 5891: loss 0.054369\n",
      "batch 5892: loss 0.022151\n",
      "batch 5893: loss 0.065676\n",
      "batch 5894: loss 0.033398\n",
      "batch 5895: loss 0.189961\n",
      "batch 5896: loss 0.072195\n",
      "batch 5897: loss 0.042229\n",
      "batch 5898: loss 0.048320\n",
      "batch 5899: loss 0.031479\n",
      "batch 5900: loss 0.038709\n",
      "batch 5901: loss 0.082290\n",
      "batch 5902: loss 0.160612\n",
      "batch 5903: loss 0.045864\n",
      "batch 5904: loss 0.037156\n",
      "batch 5905: loss 0.037744\n",
      "batch 5906: loss 0.096311\n",
      "batch 5907: loss 0.062073\n",
      "batch 5908: loss 0.025112\n",
      "batch 5909: loss 0.034482\n",
      "batch 5910: loss 0.100672\n",
      "batch 5911: loss 0.054430\n",
      "batch 5912: loss 0.056481\n",
      "batch 5913: loss 0.098856\n",
      "batch 5914: loss 0.014034\n",
      "batch 5915: loss 0.070105\n",
      "batch 5916: loss 0.136461\n",
      "batch 5917: loss 0.071868\n",
      "batch 5918: loss 0.088205\n",
      "batch 5919: loss 0.029287\n",
      "batch 5920: loss 0.023307\n",
      "batch 5921: loss 0.014486\n",
      "batch 5922: loss 0.048775\n",
      "batch 5923: loss 0.017241\n",
      "batch 5924: loss 0.041474\n",
      "batch 5925: loss 0.023282\n",
      "batch 5926: loss 0.044153\n",
      "batch 5927: loss 0.036160\n",
      "batch 5928: loss 0.037370\n",
      "batch 5929: loss 0.167254\n",
      "batch 5930: loss 0.095465\n",
      "batch 5931: loss 0.010622\n",
      "batch 5932: loss 0.067991\n",
      "batch 5933: loss 0.028148\n",
      "batch 5934: loss 0.035595\n",
      "batch 5935: loss 0.047282\n",
      "batch 5936: loss 0.046892\n",
      "batch 5937: loss 0.042912\n",
      "batch 5938: loss 0.051351\n",
      "batch 5939: loss 0.079793\n",
      "batch 5940: loss 0.051165\n",
      "batch 5941: loss 0.015514\n",
      "batch 5942: loss 0.005416\n",
      "batch 5943: loss 0.024235\n",
      "batch 5944: loss 0.023507\n",
      "batch 5945: loss 0.058886\n",
      "batch 5946: loss 0.054778\n",
      "batch 5947: loss 0.044340\n",
      "batch 5948: loss 0.020231\n",
      "batch 5949: loss 0.018852\n",
      "batch 5950: loss 0.015576\n",
      "batch 5951: loss 0.019549\n",
      "batch 5952: loss 0.015500\n",
      "batch 5953: loss 0.024223\n",
      "batch 5954: loss 0.108171\n",
      "batch 5955: loss 0.059273\n",
      "batch 5956: loss 0.091718\n",
      "batch 5957: loss 0.038924\n",
      "batch 5958: loss 0.011796\n",
      "batch 5959: loss 0.008715\n",
      "batch 5960: loss 0.028381\n",
      "batch 5961: loss 0.052480\n",
      "batch 5962: loss 0.072067\n",
      "batch 5963: loss 0.068049\n",
      "batch 5964: loss 0.100548\n",
      "batch 5965: loss 0.013202\n",
      "batch 5966: loss 0.015480\n",
      "batch 5967: loss 0.030685\n",
      "batch 5968: loss 0.033316\n",
      "batch 5969: loss 0.008418\n",
      "batch 5970: loss 0.012149\n",
      "batch 5971: loss 0.014156\n",
      "batch 5972: loss 0.033131\n",
      "batch 5973: loss 0.021939\n",
      "batch 5974: loss 0.044054\n",
      "batch 5975: loss 0.036889\n",
      "batch 5976: loss 0.116259\n",
      "batch 5977: loss 0.027652\n",
      "batch 5978: loss 0.164142\n",
      "batch 5979: loss 0.023756\n",
      "batch 5980: loss 0.007214\n",
      "batch 5981: loss 0.089046\n",
      "batch 5982: loss 0.011517\n",
      "batch 5983: loss 0.064155\n",
      "batch 5984: loss 0.017856\n",
      "batch 5985: loss 0.005980\n",
      "batch 5986: loss 0.076563\n",
      "batch 5987: loss 0.051342\n",
      "batch 5988: loss 0.084376\n",
      "batch 5989: loss 0.063226\n",
      "batch 5990: loss 0.028734\n",
      "batch 5991: loss 0.015559\n",
      "batch 5992: loss 0.059901\n",
      "batch 5993: loss 0.014650\n",
      "batch 5994: loss 0.107461\n",
      "batch 5995: loss 0.027546\n",
      "batch 5996: loss 0.107657\n",
      "batch 5997: loss 0.020065\n",
      "batch 5998: loss 0.059593\n",
      "batch 5999: loss 0.024204\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred (50, 10)\n",
      "[[0.06355229 0.07759789 0.08952925 0.04277666 0.12865819 0.25102627\n",
      "  0.0921306  0.09576768 0.07745793 0.08150328]\n",
      " [0.06656542 0.07688951 0.08774498 0.04327249 0.1337415  0.24226162\n",
      "  0.08924366 0.09934657 0.07955554 0.0813788 ]\n",
      " [0.06426431 0.07097084 0.08789078 0.04353301 0.1279626  0.24340905\n",
      "  0.08482601 0.11056635 0.07917313 0.08740396]\n",
      " [0.06146048 0.07572088 0.08989947 0.04155692 0.13308713 0.24662761\n",
      "  0.08957034 0.09990643 0.07683126 0.08533949]\n",
      " [0.06174802 0.07170568 0.08872624 0.0420539  0.13461688 0.24541834\n",
      "  0.08631083 0.10393292 0.07812414 0.08736301]\n",
      " [0.06195017 0.07650089 0.08695192 0.0424284  0.1349927  0.24428841\n",
      "  0.08661191 0.09986589 0.0792086  0.08720113]\n",
      " [0.06841341 0.07671553 0.09508575 0.04095772 0.13505627 0.23277201\n",
      "  0.09123392 0.09914985 0.08024698 0.08036851]\n",
      " [0.06132089 0.0744668  0.08917738 0.04120601 0.13532768 0.2463861\n",
      "  0.08735491 0.10158261 0.07519355 0.08798406]\n",
      " [0.05867406 0.07231099 0.09009182 0.04185293 0.14322771 0.24621\n",
      "  0.08699808 0.09822072 0.07796635 0.08444729]\n",
      " [0.06299316 0.07937236 0.09110465 0.04081368 0.12903765 0.25170776\n",
      "  0.09064078 0.09458999 0.07694211 0.08279785]\n",
      " [0.06234537 0.07260376 0.09701566 0.04167567 0.13725765 0.23637272\n",
      "  0.08888452 0.10221078 0.07948457 0.08214929]\n",
      " [0.06069632 0.07517218 0.08995772 0.04138863 0.13537748 0.2489565\n",
      "  0.08880887 0.09819841 0.07663982 0.08480404]\n",
      " [0.06263647 0.07558981 0.08958831 0.04062448 0.13402447 0.25081417\n",
      "  0.0934009  0.09197071 0.07929154 0.08205911]\n",
      " [0.06699418 0.07418989 0.08838004 0.04284187 0.13162644 0.24727151\n",
      "  0.08831603 0.09763824 0.07890248 0.08383927]\n",
      " [0.06524573 0.07413151 0.09272467 0.04134695 0.13337423 0.24009547\n",
      "  0.09140021 0.10060989 0.0804117  0.08065965]\n",
      " [0.06300698 0.07514698 0.09106454 0.04147265 0.12964228 0.25460207\n",
      "  0.08839462 0.09598597 0.08017141 0.08051252]\n",
      " [0.06410781 0.0742597  0.08975408 0.04144531 0.1303963  0.2509976\n",
      "  0.09482682 0.09443213 0.0786662  0.08111409]\n",
      " [0.06252779 0.07847165 0.09163759 0.04102227 0.12922263 0.24920487\n",
      "  0.09036241 0.09565786 0.07690767 0.08498516]\n",
      " [0.06461333 0.07891718 0.09618414 0.04176798 0.1310224  0.24059176\n",
      "  0.09056915 0.09697542 0.08066722 0.07869142]\n",
      " [0.06729437 0.07410073 0.08810242 0.04236717 0.13094637 0.24502471\n",
      "  0.09150066 0.0979018  0.08038131 0.08238044]\n",
      " [0.06338403 0.08001098 0.09128118 0.04104606 0.12901011 0.24516268\n",
      "  0.09421699 0.09393886 0.07862329 0.0833258 ]\n",
      " [0.06434259 0.07538876 0.0910738  0.04198312 0.13072212 0.2428108\n",
      "  0.08775916 0.10535872 0.07625014 0.08431087]\n",
      " [0.06268109 0.07908109 0.09187794 0.04061051 0.12953429 0.24709181\n",
      "  0.09155948 0.09747023 0.07606805 0.08402545]\n",
      " [0.06406439 0.07646968 0.09314071 0.0407034  0.13171138 0.24311417\n",
      "  0.09524796 0.09569365 0.07839547 0.08145912]\n",
      " [0.06236862 0.07454772 0.08694414 0.0424262  0.13013159 0.25172284\n",
      "  0.09016696 0.09822976 0.07796165 0.0855005 ]\n",
      " [0.066342   0.07569826 0.0861836  0.04317535 0.12904432 0.25015873\n",
      "  0.09063681 0.09850848 0.0785905  0.08166192]\n",
      " [0.06388008 0.07978857 0.09428941 0.04136124 0.13008097 0.243108\n",
      "  0.09132934 0.09365459 0.07786639 0.08464132]\n",
      " [0.06336328 0.07946144 0.09114017 0.04098587 0.12843661 0.24974096\n",
      "  0.09009739 0.0950285  0.0765757  0.08517013]\n",
      " [0.06233703 0.08005677 0.09237349 0.04039535 0.13209775 0.24452603\n",
      "  0.0908199  0.09729795 0.07618055 0.0839153 ]\n",
      " [0.06374478 0.07633796 0.09155422 0.04221749 0.1316809  0.24187726\n",
      "  0.08691909 0.10462777 0.07601291 0.08502763]\n",
      " [0.06484912 0.07733244 0.0921341  0.04243377 0.13064721 0.24285494\n",
      "  0.09139856 0.09773082 0.07919835 0.08142069]\n",
      " [0.06628641 0.07467802 0.08550794 0.04277121 0.12986003 0.24954663\n",
      "  0.09262735 0.09784473 0.08018114 0.08069663]\n",
      " [0.06385922 0.07742287 0.09450147 0.04233264 0.13294926 0.23954462\n",
      "  0.09761164 0.09057751 0.08252682 0.07867387]\n",
      " [0.06418035 0.07718616 0.09372541 0.04029309 0.13351212 0.241989\n",
      "  0.09465408 0.09718429 0.07780667 0.07946875]\n",
      " [0.06305046 0.07420377 0.08927309 0.04286144 0.13013059 0.24254432\n",
      "  0.0875257  0.10668715 0.07645795 0.0872655 ]\n",
      " [0.0628808  0.07703391 0.08804867 0.04258339 0.13031486 0.25115892\n",
      "  0.08891058 0.09659587 0.07753058 0.08494231]\n",
      " [0.06300946 0.07586662 0.09336055 0.04374588 0.13407078 0.23540021\n",
      "  0.0885896  0.10284917 0.07667269 0.08643503]\n",
      " [0.0620503  0.07276741 0.08882108 0.04136408 0.13607816 0.24415405\n",
      "  0.08660501 0.10277271 0.07585523 0.08953197]\n",
      " [0.0662332  0.07761204 0.0942492  0.04147776 0.13528872 0.23538361\n",
      "  0.09179708 0.09847187 0.07925352 0.08023304]\n",
      " [0.06228125 0.07222893 0.08966773 0.04120893 0.13921182 0.24515268\n",
      "  0.08854711 0.10156806 0.07726753 0.08286595]\n",
      " [0.06398048 0.07533168 0.09228043 0.04046078 0.13354522 0.24523383\n",
      "  0.09549624 0.09520971 0.07872057 0.07974107]\n",
      " [0.06406566 0.07291113 0.08906405 0.04316135 0.13003667 0.24506545\n",
      "  0.08627708 0.10681938 0.0778055  0.08479374]\n",
      " [0.06067214 0.07490154 0.09240514 0.04162064 0.13656326 0.24662764\n",
      "  0.08771218 0.09921435 0.07547626 0.08480693]\n",
      " [0.0635704  0.07638551 0.09068159 0.04083803 0.13332133 0.24548016\n",
      "  0.09448974 0.09574743 0.07876933 0.08071659]\n",
      " [0.06203647 0.07768547 0.0894829  0.04099575 0.13467139 0.24386816\n",
      "  0.09077398 0.09708729 0.07906917 0.08432943]\n",
      " [0.06728744 0.07506763 0.08778521 0.04224274 0.13008755 0.24679086\n",
      "  0.09346001 0.09855518 0.07898089 0.07974245]\n",
      " [0.06261773 0.07711318 0.08969162 0.04152611 0.13005301 0.24484694\n",
      "  0.0868948  0.10462513 0.0752358  0.0873956 ]\n",
      " [0.06256856 0.07680909 0.08816035 0.0414022  0.13275419 0.24850485\n",
      "  0.08981387 0.09629191 0.07875137 0.08494364]\n",
      " [0.06188709 0.07543807 0.08737111 0.0424825  0.13049583 0.25264725\n",
      "  0.08961134 0.09697486 0.07752158 0.08557036]\n",
      " [0.06228547 0.07958297 0.09222984 0.04074753 0.13144983 0.24568601\n",
      "  0.09037152 0.09884547 0.07514557 0.08365575]]\n",
      "y_true (50,)\n",
      "[6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2 9 7 7 6 2 7 8 4 7\n",
      " 3 6 1 3 6 9 3 1 4 1 7 6 9]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [0] vs. [50] [Op:Equal]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e321ee460d57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mend_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#y_true = y_true.reshape(-1,1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msparse_categorical_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y_pred, y_true)\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     return super(MeanMetricWrapper, self).update_state(\n\u001b[0;32m    583\u001b[0m         matches, sample_weight=sample_weight)\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36msparse_categorical_accuracy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   2784\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2786\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mof\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m   \"\"\"\n\u001b[1;32m-> 1306\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[1;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[0;32m   3617\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3618\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3619\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3620\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3621\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mincompatible_shape_error\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [0] vs. [50] [Op:Equal]"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    y_true = data_loader.test_label[start_index: end_index]\n",
    "    #y_true = y_true.reshape(-1,1)\n",
    "    sparse_categorical_accuracy.update_state(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if batch_index == 1:\n",
    "        print(\"y_pred\", y_pred.shape)\n",
    "        print(y_pred)\n",
    "        print(\"y_true\", y_true.shape)\n",
    "        print(y_true)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output= tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    y_true = data_loader.test_label[start_index: end_index]\n",
    "    #y_true = y_true.reshape(-1,1)\n",
    "    sparse_categorical_accuracy.update_state(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if batch_index == 1:\n",
    "        print(\"y_pred\", y_pred.shape)\n",
    "        print(y_pred)\n",
    "        print(\"y_true\", y_true.shape)\n",
    "        print(y_true)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
